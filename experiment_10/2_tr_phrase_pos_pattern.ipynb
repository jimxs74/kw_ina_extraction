{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec + textrank\n",
    "POS dengan fiture phrase detection berbasis pola POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. rutin2 membuat syspath ke root utk aktifkan __init__.py\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. rutin3 Load the dataset\n",
    "#dataset_path = os.path.join(repo_root, \"notebooks/postager_nlp-id/dataset_ekstraksi_r29_pos_sm.xlsx\")\n",
    "dataset_path = os.path.join(repo_root, \"data/dataset_ekstraksi_r29_lg.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"judul\"] +\". \"+ df[\"isi\"]\n",
    "#df_pos = df['pos_sentence_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "'''\n",
    "stopwords tidak masuk dalam preprocessing\n",
    "'''\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df['text'].apply(preprocess)\n",
    "df[\"judul\"] = df[\"judul\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(words, n=2):\n",
    "    \"\"\"Generate ngrams from a list of words.\"\"\"\n",
    "    return [\" \".join(gram) for gram in ngrams(words, n)]\n",
    "\n",
    "def get_phrase_embedding(phrase, w2v_model):\n",
    "    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from collections import Counter\n",
    "from nlp_id_local.tokenizer import PhraseTokenizer \n",
    "from nlp_id_local.postag import PosTag\n",
    "\n",
    "\n",
    "model_path = os.path.join(repo_root, \"notebooks/nlp-id_retraining/train_tuned.pkl\") #add_8\n",
    "#model_path = os.path.join(repo_root, \"src/adj_textrank/nlp_id_local/data/postagger_v9.pkl\") #add_8\n",
    "\n",
    "def detect_bigram(text, available_tokens,):\n",
    "    \n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only bigrams whose individual words are in available_tokens\n",
    "    bigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 1 and all(word in available_tokens for word in phrase.split())]\n",
    "\n",
    "    return bigrams_only\n",
    "\n",
    "def detect_trigram(text, available_tokens):\n",
    "\n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only trigrams whose individual words are in available_tokens\n",
    "    trigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 2 and all(word in available_tokens for word in phrase.split())]\n",
    "\n",
    "    return trigrams_only\n",
    "\n",
    "def get_unique_tokens_pos(all_tokens, model_path):\n",
    "    \"\"\"\n",
    "    Get unique POS tags for tokens.\n",
    "    \"\"\"\n",
    "    postagger = PosTag(model_path)\n",
    "    pos_tokens = []\n",
    "    seen_tokens = set()\n",
    "    \n",
    "    for token in all_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            tokens_pos = postagger.get_phrase_tag(token)\n",
    "            pos_tokens.append(tokens_pos)\n",
    "    return pos_tokens\n",
    "\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists into a single list.\n",
    "    \"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "\n",
    "def filter_tokens_by_pos(flat_tokens, pos_filters):\n",
    "    \"\"\"\n",
    "    Filter tokens based on their POS tags and ensure they're unique.\n",
    "    \"\"\"\n",
    "    seen_tokens = set()\n",
    "    return [token[0] for token in flat_tokens if token[1] in pos_filters and not (token[0] in seen_tokens or seen_tokens.add(token[0]))]\n",
    "\n",
    "def extract_keyphrases_with_ngrams_graph(text, w2v_model, judul, available_tokens, n=10):\n",
    "    # Read stopwords from the file\n",
    "    #stopwords_path = os.path.join(repo_root, \"data/all_stop_words.txt\") \n",
    "    stopwords_path = os.path.join(repo_root, \"notebooks/stopwords_tuning/all_stop_words.txt\")\n",
    "    with open(stopwords_path, 'r') as file:\n",
    "        stopwords = set(file.read().strip().splitlines())\n",
    "\n",
    "    # Tokenize the text into unigrams\n",
    "    #unigrams = [word for word in text.split() if word not in stopwords]\n",
    "\n",
    "    # Tokenize the text into unigrams that are in available_tokens\n",
    "    unigrams = [word for word in text.split() if word not in stopwords and word in available_tokens]\n",
    "\n",
    "    # Generate bigrams and trigrams using nlp-id\n",
    "    bigrams = detect_bigram(text, available_tokens)\n",
    "    trigrams = detect_trigram(text, available_tokens)\n",
    "    \n",
    "    # Combine unigrams, filtered bigrams, and filtered trigrams\n",
    "    all_tokens = unigrams + bigrams + trigrams\n",
    "\n",
    "    # Filter tokens only for selected POS\n",
    "#    pos_tokens = get_unique_tokens_pos(all_tokens, model_path)\n",
    "#    flat_pos_tokens = flatten_list_of_lists(pos_tokens)\n",
    "#    selected_pos = {'NN', 'NNP', 'VB', 'NP', 'VP'} # FW di exclude\n",
    "#    filtered_tokens = filter_tokens_by_pos(flat_pos_tokens, selected_pos)\n",
    "\n",
    "    # Get embeddings for each token (averaging word embeddings for bigrams/trigrams)\n",
    "#    token_embeddings = [get_phrase_embedding(token, w2v_model) for token in filtered_tokens]\n",
    "    token_embeddings = [get_phrase_embedding(token, w2v_model) for token in all_tokens]\n",
    "    \n",
    "    # Filter out tokens that don't have embeddings\n",
    "#    tokens, embeddings = zip(*[(token, emb) for token, emb in zip(filtered_tokens, token_embeddings) if emb is not None])\n",
    "    tokens, embeddings = zip(*[(token, emb) for token, emb in zip(all_tokens, token_embeddings) if emb is not None])\n",
    "    # todo : masih ada token bahasa asing atau token aneh yg lolos. \n",
    "\n",
    "    # Compute the cosine similarity between token embeddings\n",
    "    cosine_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Create a graph and connect tokens with high similarity\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            if cosine_matrix[i][j] > 0.5:  # This threshold can be adjusted\n",
    "                G.add_edge(tokens[i], tokens[j], weight=cosine_matrix[i][j])\n",
    "    \n",
    "    # Create labels dictionary using the tokens\n",
    "    labels = {token: token for token in tokens}\n",
    "\n",
    "    # Compute the PageRank scores to rank the tokens\n",
    "    scores = nx.pagerank(G)\n",
    "\n",
    "    # Modify scores if token is in title letter\n",
    "#    for token in scores:\n",
    "#        if any(token in title for title in judul):\n",
    "#            scores[token] *= 2\n",
    "\n",
    "    # Extract top N keyphrases along with their scores\n",
    "    ranked_tokens = sorted(((scores[token], token) for token in tokens if token in scores), reverse=True)\n",
    "    \n",
    "    keyphrases_with_scores = []\n",
    "    seen_tokens = set()  # Set to keep track of tokens that have already been added\n",
    "\n",
    "    for score, token in ranked_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            keyphrases_with_scores.append((token, score))\n",
    "            seen_tokens.add(token)  # Mark the token as seen\n",
    "            if len(keyphrases_with_scores) >= n:\n",
    "                break  # Stop when the desired number of keyphrases is reached\n",
    "\n",
    "    return keyphrases_with_scores, G, labels\n",
    "\n",
    "\n",
    "def visualize_graph(G, labels):\n",
    "\n",
    "    # Remove self-loops (edges that connect a node to itself)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\")\n",
    "    nx.draw_networkx_labels(G, pos, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = os.path.join(repo_root, \"models/w2v_200/idwiki_word2vec_200_new_lower.model\")\n",
    "w2v_model = Word2Vec.load(w2v_path)\n",
    "\n",
    "# Get available tokens from the Word2Vec model\n",
    "available_tokens = set(w2v_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 490...! Done\n",
      "Processing index 491...! Done\n",
      "Processing index 492...! Done\n",
      "Processing index 493...! Done\n",
      "Processing index 494...! Done\n",
      "Processing index 495...! Done\n",
      "Processing index 496...! Done\n",
      "Processing index 497...! Done\n",
      "Processing index 498...! Done\n",
      "Processing index 499...! Done\n",
      "Processing index 500...! Done\n",
      "Processing index 501...! Done\n",
      "Processing index 502...! Done\n",
      "Processing index 503...! Done\n",
      "Processing index 504...! Done\n",
      "Processing index 505...! Done\n",
      "Processing index 506...! Done\n",
      "Processing index 507...! Done\n",
      "Processing index 508...! Done\n",
      "Processing index 509...! Done\n",
      "Processing index 510...! Done\n",
      "Processing index 511...! Done\n",
      "Processing index 512...! Done\n",
      "Processing index 513...! Done\n",
      "Processing index 514...! Done\n",
      "Processing index 515...! Done\n",
      "Processing index 516...! Done\n",
      "Processing index 517...! Done\n",
      "Processing index 518...! Done\n",
      "Processing index 519...! Done\n",
      "Processing index 520...! Done\n",
      "Processing index 521...! Done\n",
      "Processing index 522...! "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#text = df_tr[i] # setelah di preprocess\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ls_judul \u001b[39m=\u001b[39m preprocess(df[\u001b[39m\"\u001b[39m\u001b[39mjudul\u001b[39m\u001b[39m\"\u001b[39m][i])\u001b[39m.\u001b[39msplit()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m keyphrases,_,_ \u001b[39m=\u001b[39m extract_keyphrases_with_ngrams_graph(text, w2v_model, ls_judul, available_tokens, \u001b[39m3\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df_keyphrases \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(keyphrases, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mKeyword\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m a \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(df_keyphrases\u001b[39m.\u001b[39mKeyword)\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m unigrams \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords \u001b[39mand\u001b[39;00m word \u001b[39min\u001b[39;00m available_tokens]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# Generate bigrams and trigrams using nlp-id\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m bigrams \u001b[39m=\u001b[39m detect_bigram(text, available_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m trigrams \u001b[39m=\u001b[39m detect_trigram(text, available_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# Combine unigrams, filtered bigrams, and filtered trigrams\u001b[39;00m\n",
      "\u001b[1;32m/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect_bigram\u001b[39m(text, available_tokens,):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m PhraseTokenizer()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     phrases \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment_10/2_tr_phrase_pos_pattern.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Include only bigrams whose individual words are in available_tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/kw_ina_extraction/experiment_10/nlp_id_local/tokenizer.py:246\u001b[0m, in \u001b[0;36mPhraseTokenizer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostagger \u001b[39m=\u001b[39m postag\u001b[39m.\u001b[39;49mPosTag()\n",
      "File \u001b[0;32m~/anaconda3/envs/kw_ina/lib/python3.11/site-packages/nlp_id/postag.py:45\u001b[0m, in \u001b[0;36mPosTag.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m         wget\u001b[39m.\u001b[39mdownload(url, model_path)\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_model(model_path)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mTokenizer()\n",
      "File \u001b[0;32m~/anaconda3/envs/kw_ina/lib/python3.11/site-packages/nlp_id/tokenizer.py:14\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutside_punct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minside_punct \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrealpath(\u001b[39m__file__\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlemmatizer \u001b[39m=\u001b[39m Lemmatizer()\n\u001b[1;32m     15\u001b[0m CliticsFile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_dir, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnon_clitics.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(CliticsFile) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/kw_ina/lib/python3.11/site-packages/nlp_id/lemmatizer.py:12\u001b[0m, in \u001b[0;36mLemmatizer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m DictionaryFile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(current_dir, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlemma_dict.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(RootWordFile) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_word \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(f\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39msplitlines())\n\u001b[1;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(DictionaryFile) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlemma_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict_textrank = pd.DataFrame()\n",
    "for i in df.index:\n",
    "#for i in df.loc[490:].index:\n",
    "    print('Processing index', i, end='...! ')\n",
    "    text = df[\"text\"][i] # sblm di preprocess\n",
    "    #text = df_tr[i] # setelah di preprocess\n",
    "    ls_judul = preprocess(df[\"judul\"][i]).split()\n",
    "    keyphrases,_,_ = extract_keyphrases_with_ngrams_graph(text, w2v_model, ls_judul, available_tokens, 10)\n",
    "    df_keyphrases = pd.DataFrame(keyphrases, columns=['Keyword', 'Score'])\n",
    "    a = pd.DataFrame(df_keyphrases.Keyword).T.reset_index(drop=True)\n",
    "    b = pd.DataFrame(df_keyphrases.Score).round(3).T.reset_index(drop=True)\n",
    "    df_keyphrases = pd.concat([a, b], axis=1)\n",
    "\n",
    "    # Check if there are missing columns and add them with zero values\n",
    "    missing_columns = 20 - df_keyphrases.shape[1]\n",
    "    for _ in range(missing_columns):\n",
    "        df_keyphrases[df_keyphrases.shape[1]] = 0\n",
    "\n",
    "    #df_keyphrases.columns = ['key_1', 'key_2','key_3','score_1', 'score_2','score_3']\n",
    "    df_keyphrases.columns = ['key_1', 'key_2','key_3', 'key_4', 'key_5','key_6', 'key_7', 'key_8','key_9','key_10','score_1', 'score_2','score_3','score_4', 'score_5','score_6','score_7', 'score_8','score_9','score_10'] \n",
    "    predict_textrank = pd.concat([predict_textrank, df_keyphrases], ignore_index=True)\n",
    "    print('Done')\n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eval\n",
    "\n",
    "targets = df[[\"k1\", \"k2\", \"k3\",\"k4\", \"k5\", \"k6\",\"k7\"]].values.tolist()\n",
    "df_targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation TextRank top 10\n",
    "predict_textrank_list_10 = predict_textrank[['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10']].values.tolist()\n",
    "eval_textrank_10 = eval(predict_textrank_list_10, targets, True).round(3)\n",
    "eval_textrank_10.columns = ['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_10 = eval_textrank_10[['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "eval_textrank_10.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_10 = eval_textrank_10['flex_recall'].mean()\n",
    "textrank_prec_10 = eval_textrank_10['flex_prec'].mean()\n",
    "textrank_f1_10 = 2 * (textrank_prec_10 * textrank_recall_10) / (textrank_prec_10 + textrank_recall_10)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_10 = pd.DataFrame({'textrank': [textrank_recall_10, textrank_prec_10, textrank_f1_10]}, index=['recall', 'precision', 'F1'])\n",
    "summary_10 = summary_10.round(3)\n",
    "summary_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation TextRank top 5\n",
    "predict_textrank_list_5 = predict_textrank[['key_1','key_2','key_3', 'key_4','key_5']].values.tolist()\n",
    "eval_textrank_5 = eval(predict_textrank_list_5, targets, True).round(3)\n",
    "eval_textrank_5.columns = ['key_1','key_2','key_3', 'key_4','key_5','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_5 = eval_textrank_5[['key_1','key_2','key_3', 'key_4','key_5', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "eval_textrank_5.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_5 = eval_textrank_5['flex_recall'].mean()\n",
    "textrank_prec_5 = eval_textrank_5['flex_prec'].mean()\n",
    "textrank_f1_5 = 2 * (textrank_prec_5 * textrank_recall_5) / (textrank_prec_5 + textrank_recall_5)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_5 = pd.DataFrame({'textrank': [textrank_recall_5, textrank_prec_5, textrank_f1_5]}, index=['recall', 'precision', 'F1'])\n",
    "summary_5 = summary_5.round(3)\n",
    "summary_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation TextRank top 3\n",
    "predict_textrank_list_3 = predict_textrank[['key_1','key_2','key_3']].values.tolist()\n",
    "eval_textrank_3 = eval(predict_textrank_list_3, targets, True).round(3)\n",
    "eval_textrank_3.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_3 = eval_textrank_3[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "eval_textrank_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_3 = eval_textrank_3['flex_recall'].mean()\n",
    "textrank_prec_3 = eval_textrank_3['flex_prec'].mean()\n",
    "textrank_f1_3 = 2 * (textrank_prec_3 * textrank_recall_3) / (textrank_prec_3 + textrank_recall_3)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_3 = pd.DataFrame({'textrank': [textrank_recall_3, textrank_prec_3, textrank_f1_3]}, index=['recall', 'precision', 'F1'])\n",
    "summary_3 = summary_3.round(3)\n",
    "summary_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_10 = pd.concat([predict_textrank, df_targets, eval_textrank_10], axis=1)\n",
    "predict_textrank_10.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_5 = pd.concat([predict_textrank, df_targets, eval_textrank_5], axis=1)\n",
    "predict_textrank_5.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_3 = pd.concat([predict_textrank, df_targets, eval_textrank_3], axis=1)\n",
    "predict_textrank_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to excel file\n",
    "from utils import write_excel\n",
    "\n",
    "sheet_name_10 = '2_tr_phrase_pos_pattern_10'\n",
    "sheet_name_5 = '2_tr_phrase_pos_pattern_5'\n",
    "sheet_name_3 = '2_tr_phrase_pos_pattern_3'\n",
    "\n",
    "output_file = '2_tr_phrase_pos_pattern.xlsx'\n",
    "write_excel(predict_textrank_10, sheet_name_10, output_file)\n",
    "write_excel(predict_textrank_5, sheet_name_5, output_file)\n",
    "write_excel(predict_textrank_3, sheet_name_3, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_ina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
