{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 0it [00:00, ?it/s]\n",
      "Processing files: 100%|██████████| 35/35 [00:27<00:00,  1.29it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:22<00:00,  1.22it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:05<00:00,  1.52it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:16<00:00,  1.31it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:19<00:00,  1.25it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      "Processing files: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# PreProcess data\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing and removing special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def process_wiki_files(input_dir, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for root, dirs, files in os.walk(input_dir):\n",
    "            for file in tqdm(files, desc=\"Processing files\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "                    for line in tqdm(f_in, desc=f\"Processing lines in {file}\", leave=False):\n",
    "                        article = json.loads(line)\n",
    "                        text = preprocess_text(article['text'])\n",
    "                        f_out.write(text + '\\n')\n",
    "\n",
    "# Assuming preprocess_text is a function you have defined\n",
    "process_wiki_files('extracted_text', 'processed_wiki_id.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 87M wordsess:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   10828 lr:  0.000000 avg.loss:  1.220157 ETA:   0h 0m 0s  2.2% words/sec/thread:   10039 lr:  0.048904 avg.loss:  1.394719 ETA:   0h20m13s 18.3% words/sec/thread:   10216 lr:  0.040832 avg.loss:  1.364442 ETA:   0h16m35s 36.0% words/sec/thread:   10313 lr:  0.031987 avg.loss:  1.331006 ETA:   0h12m52s 1.302212 ETA:   0h 9m19s  10591 lr:  0.006877 avg.loss:  1.216736 ETA:   0h 2m41s\n",
      "Read 87M wordsess:  20%|██        | 1/5 [19:34<1:18:18, 1174.73s/it]\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   12194 lr:  0.000000 avg.loss:  0.803152 ETA:   0h 0m 0s 0.048667 avg.loss:  1.360469 ETA:   0h35m42s 0.009289 avg.loss:  0.910470 ETA:   0h 5m58s 84.6% words/sec/thread:   12756 lr:  0.007689 avg.loss:  0.888377 ETA:   0h 5m 0s 92.0% words/sec/thread:   12463 lr:  0.004004 avg.loss:  0.843869 ETA:   0h 2m40sm 0s\n",
      "Read 87M wordsess:  40%|████      | 2/5 [53:59<1:24:54, 1698.17s/it]\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   10659 lr:  0.000000 avg.loss:  0.619720 ETA:   0h 0m 0s 20.4% words/sec/thread:   10550 lr:  0.039787 avg.loss:  1.249303 ETA:   0h46m58s 21.7% words/sec/thread:   10513 lr:  0.039155 avg.loss:  1.232305 ETA:   0h46m23s 0.037893 avg.loss:  1.206694 ETA:   0h45m 8s 34.3% words/sec/thread:   10270 lr:  0.032826 avg.loss:  1.141990 ETA:   0h39m48s 34.9% words/sec/thread:   10272 lr:  0.032574 avg.loss:  1.139288 ETA:   0h39m29s 35.8% words/sec/thread:   10279 lr:  0.032081 avg.loss:  1.133109 ETA:   0h38m52s 36.5% words/sec/thread:   10294 lr:  0.031745 avg.loss:  1.131015 ETA:   0h38m24s 37.0% words/sec/thread:   10277 lr:  0.031495 avg.loss:  1.128944 ETA:   0h38m 9s 41.9% words/sec/thread:   10406 lr:  0.029063 avg.loss:  1.060820 ETA:   0h34m46s 53.4% words/sec/thread:   10622 lr:  0.023316 avg.loss:  0.904446 ETA:   0h27m20s 57.3% words/sec/thread:   10670 lr:  0.021363 avg.loss:  0.864143 ETA:   0h24m56s 64.8% words/sec/thread:   10626 lr:  0.017579 avg.loss:  0.798718 ETA:   0h20m36s 0.013105 avg.loss:  0.738159 ETA:   0h15m24s 75.5% words/sec/thread:   10584 lr:  0.012253 avg.loss:  0.728093 ETA:   0h14m25s 0.008626 avg.loss:  0.690427 ETA:   0h10m 8s 94.4% words/sec/thread:   10623 lr:  0.002808 avg.loss:  0.638228 ETA:   0h 3m17s% words/sec/thread:   10643 lr:  0.001249 avg.loss:  0.627051 ETA:   0h 1m27s\n",
      "Read 87M wordsess:  60%|██████    | 3/5 [1:52:49<1:24:29, 2534.76s/it]\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Train the FastText Model\n",
    "\n",
    "def train_fasttext_with_phrases(input_file, epochs=5, lr=0.05):\n",
    "    # Read the file and build sentences\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentences.append(line.split())\n",
    "\n",
    "    # Detect and create bigrams/trigrams\n",
    "    bigram = Phraser(Phrases(sentences, min_count=5, threshold=10))\n",
    "    trigram = Phraser(Phrases(bigram[sentences], min_count=5, threshold=10))\n",
    "\n",
    "    # Apply the bigram/trigram models to the sentences\n",
    "    sentences_with_phrases = [trigram[bigram[sentence]] for sentence in sentences]\n",
    "\n",
    "    # Write the sentences with phrases to a temporary file\n",
    "    temp_file = 'temp_sentences_with_phrases.txt'\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences_with_phrases:\n",
    "            f.write(' '.join(sentence) + '\\n')\n",
    "\n",
    "    # Train FastText model with a progress bar\n",
    "    model = None\n",
    "    for epoch in tqdm(range(1, epochs + 1), desc='Training Progress'):\n",
    "        model = fasttext.train_unsupervised(temp_file, model='skipgram', lr=lr, epoch=epoch, dim=200)\n",
    "\n",
    "    # Clean up the temporary file\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train the model with the preprocessed Wikipedia text\n",
    "fasttext_model = train_fasttext_with_phrases('processed_wiki_id.txt')\n",
    "\n",
    "# Save the model\n",
    "fasttext_model.save_model(\"fasttext_ina_200_with_phrases.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5710175   0.30058366  0.34796658  0.88001174  0.52425134 -0.02964741\n",
      " -0.17222954 -0.5002736   0.762855   -0.7790979   0.08115216  0.14185356\n",
      " -0.6173891  -0.0973238   0.3765095  -0.18282877 -0.02402508  0.19270276\n",
      " -0.63856393 -0.22311787  0.02718417  0.09362951 -0.13754086  0.12484796\n",
      "  0.29579833 -1.0931784  -0.21157692  0.85819167  0.21909578 -0.35906434\n",
      "  0.33472145 -0.01592343 -0.26751727 -0.82759494 -0.17570087 -0.4351454\n",
      " -0.0893899  -0.73693675 -0.04124119 -0.35097325 -0.89134544 -0.8440361\n",
      " -0.2711332   0.20650207  0.37705436  0.02467851 -0.34632164  0.0023733\n",
      "  0.334291    0.01247291 -0.6152394  -0.4529973  -0.43055084 -0.08579167\n",
      "  0.6661112  -0.42551488  0.7379743   0.06338852 -0.23205082 -0.49874333\n",
      " -0.94483197 -0.38910538  0.19761562 -0.48624828 -0.50785965 -0.03037283\n",
      "  0.05746352  0.21924978 -0.0416922  -0.31730434  0.40931764 -0.69406706\n",
      "  0.2582438   0.7814642  -1.0616256  -0.19469967 -0.15268809  0.24732283\n",
      "  0.3922155  -0.57278365 -0.31278664  0.31659085  0.18266565 -0.17567167\n",
      "  0.00166491  0.9856425   0.24358395 -0.22927389  0.488922    0.40644836\n",
      " -0.2326628   0.09825194  0.31876692 -0.58408046 -0.3358311  -0.56020945\n",
      "  0.1358487   0.05317448 -0.2999138  -0.1951633 ]\n",
      "[(0.9499713778495789, 'harimau_harimau'), (0.9327273368835449, 'macan_harimau'), (0.9088782072067261, 'singa_harimau'), (0.9017631411552429, 'harimaunya'), (0.8993623852729797, 'harimau_singa'), (0.8944528698921204, 'harimau_macan'), (0.8920230865478516, 'ekor_harimau'), (0.8858714699745178, 'harimau_putih'), (0.8777719736099243, 'seekor_harimau'), (0.8601592779159546, 'sang_harimau')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Using the Model\n",
    "# Load model\n",
    "model = fasttext.load_model(\"fasttext_ina_200_with_phrases.bin\")\n",
    "\n",
    "# Get word vector for a word in Bahasa Indonesia\n",
    "word_vector = model.get_word_vector(\"singa\")\n",
    "print(word_vector)\n",
    "# Find similar words\n",
    "similar_words = model.get_nearest_neighbors(\"harimau\")\n",
    "print(similar_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
