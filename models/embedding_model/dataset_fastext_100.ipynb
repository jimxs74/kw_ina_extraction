{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreProcess data\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing and removing special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def process_wiki_files(input_dir, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for root, dirs, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "                    for line in f_in:\n",
    "                        article = json.loads(line)\n",
    "                        text = preprocess_text(article['text'])\n",
    "                        f_out.write(text + '\\n')\n",
    "\n",
    "# Process the extracted Wikipedia text\n",
    "process_wiki_files('extracted_text', 'processed_wiki_id.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 87M wordsess:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n",
      "Training Progress:  20%|██        | 1/5 [13:40<54:43, 820.91s/it]:  1.233117 ETA:   0h 0m 0s 15.0% words/sec/thread:   15476 lr:  0.042499 avg.loss:  1.390807 ETA:   0h11m24s  0h11m16s 33.3% words/sec/thread:   15506 lr:  0.033363 avg.loss:  1.352089 ETA:   0h 8m55s 0.017310 avg.loss:  1.265731 ETA:   0h 4m35s 73.8% words/sec/thread:   15658 lr:  0.013119 avg.loss:  1.244987 ETA:   0h 3m28s 81.3% words/sec/thread:   15666 lr:  0.009355 avg.loss:  1.233814 ETA:   0h 2m28s 86.6% words/sec/thread:   15661 lr:  0.006699 avg.loss:  1.229112 ETA:   0h 1m46s\n",
      "Read 87M words\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   16701 lr:  0.000000 avg.loss:  0.789356 ETA:   0h 0m 0s 30.6% words/sec/thread:   15518 lr:  0.034687 avg.loss:  1.250594 ETA:   0h18m33s 41.4% words/sec/thread:   16000 lr:  0.029314 avg.loss:  1.187270 ETA:   0h15m12s 71.0% words/sec/thread:   16523 lr:  0.014519 avg.loss:  0.980905 ETA:   0h 7m17s 87.1% words/sec/thread:   16659 lr:  0.006440 avg.loss:  0.856986 ETA:   0h 3m12s 89.9% words/sec/thread:   16668 lr:  0.005064 avg.loss:  0.841040 ETA:   0h 2m31s\n",
      "Read 87M wordsess:  40%|████      | 2/5 [39:00<1:01:34, 1231.64s/it]\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   18453 lr:  0.000000 avg.loss:  0.629687 ETA:   0h 0m 0s  1.6% words/sec/thread:   17925 lr:  0.049185 avg.loss:  1.371985 ETA:   0h34m10s  5.2% words/sec/thread:   18085 lr:  0.047387 avg.loss:  1.348402 ETA:   0h32m38s 20.0% words/sec/thread:   18362 lr:  0.039997 avg.loss:  1.246370 ETA:   0h27m 7s 26.2% words/sec/thread:   18424 lr:  0.036917 avg.loss:  1.184773 ETA:   0h24m57s avg.loss:  1.178084 ETA:   0h24m36s  18431 lr:  0.035764 avg.loss:  1.169994 ETA:   0h24m10s 28.5% words/sec/thread:   18431 lr:  0.035759 avg.loss:  1.169926 ETA:   0h24m 9s 29.0% words/sec/thread:   18435 lr:  0.035523 avg.loss:  1.167321 ETA:   0h23m59s 33.6% words/sec/thread:   18401 lr:  0.033224 avg.loss:  1.144960 ETA:   0h22m29s 35.7% words/sec/thread:   18392 lr:  0.032151 avg.loss:  1.133232 ETA:   0h21m46s 40.0% words/sec/thread:   18361 lr:  0.030021 avg.loss:  1.094322 ETA:   0h20m21s 1.043373 ETA:   0h19m22s 1.035535 ETA:   0h19m11s  18391 lr:  0.026510 avg.loss:  0.984941 ETA:   0h17m57s29s 49.8% words/sec/thread:   18408 lr:  0.025081 avg.loss:  0.948947 ETA:   0h16m58s 51.1% words/sec/thread:   18413 lr:  0.024472 avg.loss:  0.934285 ETA:   0h16m33s  18434 lr:  0.022770 avg.loss:  0.897481 ETA:   0h15m23s14m35s 0.850712 ETA:   0h13m44s 64.3% words/sec/thread:   18452 lr:  0.017872 avg.loss:  0.809384 ETA:   0h12m 3s 67.8% words/sec/thread:   18437 lr:  0.016103 avg.loss:  0.782604 ETA:   0h10m52s 67.9% words/sec/thread:   18437 lr:  0.016050 avg.loss:  0.781894 ETA:   0h10m50s 73.2% words/sec/thread:   18429 lr:  0.013386 avg.loss:  0.746380 ETA:   0h 9m 2s lr:  0.012447 avg.loss:  0.735112 ETA:   0h 8m24s 77.1% words/sec/thread:   18435 lr:  0.011426 avg.loss:  0.723219 ETA:   0h 7m43s 86.4% words/sec/thread:   18458 lr:  0.006811 avg.loss:  0.678354 ETA:   0h 4m35s 88.6% words/sec/thread:   18464 lr:  0.005683 avg.loss:  0.668207 ETA:   0h 3m50s 93.6% words/sec/thread:   18467 lr:  0.003181 avg.loss:  0.649883 ETA:   0h 2m 8s\n",
      "Read 87M wordsess:  60%|██████    | 3/5 [1:13:08<53:28, 1604.41s/it]\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   18329 lr:  0.000000 avg.loss:  0.534336 ETA:   0h 0m 0s  17863 lr:  0.047210 avg.loss:  1.352490 ETA:   0h43m53s 13.3% words/sec/thread:   18229 lr:  0.043345 avg.loss:  1.278488 ETA:   0h39m29s 15.9% words/sec/thread:   18291 lr:  0.042025 avg.loss:  1.230414 ETA:   0h38m 9s 16.7% words/sec/thread:   18306 lr:  0.041632 avg.loss:  1.219189 ETA:   0h37m45s 20.1% words/sec/thread:   18318 lr:  0.039973 avg.loss:  1.180918 ETA:   0h36m14s35m54s 23.8% words/sec/thread:   18319 lr:  0.038116 avg.loss:  1.151012 ETA:   0h34m33s 24.3% words/sec/thread:   18306 lr:  0.037872 avg.loss:  1.148612 ETA:   0h34m21s lr:  0.037651 avg.loss:  1.146444 ETA:   0h34m 9s 25.1% words/sec/thread:   18289 lr:  0.037443 avg.loss:  1.143665 ETA:   0h33m59s 27.5% words/sec/thread:   18270 lr:  0.036241 avg.loss:  1.128021 ETA:   0h32m56s 36.7% words/sec/thread:   18282 lr:  0.031639 avg.loss:  0.956202 ETA:   0h28m44s  0h25m38s 44.7% words/sec/thread:   18327 lr:  0.027673 avg.loss:  0.845854 ETA:   0h25m 4s 49.5% words/sec/thread:   18310 lr:  0.025244 avg.loss:  0.793717 ETA:   0h22m53s 53.3% words/sec/thread:   18301 lr:  0.023348 avg.loss:  0.757652 ETA:   0h21m11s 54.4% words/sec/thread:   18293 lr:  0.022813 avg.loss:  0.748252 ETA:   0h20m42s 0.022410 avg.loss:  0.741837 ETA:   0h20m20s 0.674886 ETA:   0h15m49s 65.1% words/sec/thread:   18324 lr:  0.017439 avg.loss:  0.674452 ETA:   0h15m48s 65.5% words/sec/thread:   18326 lr:  0.017237 avg.loss:  0.671649 ETA:   0h15m37s 67.5% words/sec/thread:   18335 lr:  0.016253 avg.loss:  0.659633 ETA:   0h14m43s 68.4% words/sec/thread:   18336 lr:  0.015788 avg.loss:  0.653912 ETA:   0h14m17s23s 74.0% words/sec/thread:   18326 lr:  0.013005 avg.loss:  0.623948 ETA:   0h11m47s 79.6% words/sec/thread:   18312 lr:  0.010201 avg.loss:  0.599839 ETA:   0h 9m15s 9m13s 81.3% words/sec/thread:   18310 lr:  0.009339 avg.loss:  0.592923 ETA:   0h 8m28s 91.7% words/sec/thread:   18334 lr:  0.004164 avg.loss:  0.557199 ETA:   0h 3m46s 92.7% words/sec/thread:   18337 lr:  0.003655 avg.loss:  0.554184 ETA:   0h 3m18s 93.2% words/sec/thread:   18336 lr:  0.003417 avg.loss:  0.552755 ETA:   0h 3m 5s48s 0.001437 avg.loss:  0.541484 ETA:   0h 1m18s 99.9% words/sec/thread:   18329 lr:  0.000072 avg.loss:  0.534624 ETA:   0h 0m 3s\n",
      "Read 87M wordsess:  80%|████████  | 4/5 [1:58:49<34:13, 2053.12s/it]\n",
      "Number of words:  710071\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   18493 lr:  0.000000 avg.loss:  0.488276 ETA:   0h 0m 0s  7.4% words/sec/thread:   18451 lr:  0.046281 avg.loss:  1.308667 ETA:   0h52m 3s  0h50m27s words/sec/thread:   18561 lr:  0.043727 avg.loss:  1.232731 ETA:   0h48m54s48m42s 13.5% words/sec/thread:   18580 lr:  0.043228 avg.loss:  1.213951 ETA:   0h48m17s 13.9% words/sec/thread:   18585 lr:  0.043050 avg.loss:  1.208019 ETA:   0h48m 5s% words/sec/thread:   18561 lr:  0.040468 avg.loss:  1.148283 ETA:   0h45m15s 20.1% words/sec/thread:   18543 lr:  0.039969 avg.loss:  1.141741 ETA:   0h44m44s 21.3% words/sec/thread:   18538 lr:  0.039329 avg.loss:  1.129544 ETA:   0h44m 2sh43m19s 25.5% words/sec/thread:   18521 lr:  0.037233 avg.loss:  1.046753 ETA:   0h41m43s lr:  0.036520 avg.loss:  1.010278 ETA:   0h40m54s  18533 lr:  0.036316 avg.loss:  1.000888 ETA:   0h40m40s 29.5% words/sec/thread:   18535 lr:  0.035256 avg.loss:  0.955412 ETA:   0h39m29s 29.5% words/sec/thread:   18535 lr:  0.035248 avg.loss:  0.955102 ETA:   0h39m28s% words/sec/thread:   18535 lr:  0.034873 avg.loss:  0.939816 ETA:   0h39m 3s 30.8% words/sec/thread:   18538 lr:  0.034575 avg.loss:  0.928396 ETA:   0h38m42s 0.034222 avg.loss:  0.915851 ETA:   0h38m18s 33.4% words/sec/thread:   18560 lr:  0.033284 avg.loss:  0.883218 ETA:   0h37m13s 33.7% words/sec/thread:   18561 lr:  0.033129 avg.loss:  0.878035 ETA:   0h37m 2sm 0s lr:  0.031737 avg.loss:  0.836163 ETA:   0h35m29s 0.834607 ETA:   0h35m25s 41.9% words/sec/thread:   18536 lr:  0.029046 avg.loss:  0.769911 ETA:   0h32m31s 42.2% words/sec/thread:   18532 lr:  0.028896 avg.loss:  0.766583 ETA:   0h32m21s 42.9% words/sec/thread:   18531 lr:  0.028560 avg.loss:  0.759571 ETA:   0h31m59s 43.8% words/sec/thread:   18525 lr:  0.028116 avg.loss:  0.749852 ETA:   0h31m30s 45.0% words/sec/thread:   18529 lr:  0.027498 avg.loss:  0.737058 ETA:   0h30m48s 0.712285 ETA:   0h29m15s 47.8% words/sec/thread:   18532 lr:  0.026082 avg.loss:  0.711744 ETA:   0h29m12s 48.9% words/sec/thread:   18534 lr:  0.025547 avg.loss:  0.702941 ETA:   0h28m36s 53.8% words/sec/thread:   18545 lr:  0.023124 avg.loss:  0.666720 ETA:   0h25m52s 53.9% words/sec/thread:   18546 lr:  0.023074 avg.loss:  0.666053 ETA:   0h25m49s 56.1% words/sec/thread:   18543 lr:  0.021967 avg.loss:  0.651402 ETA:   0h24m35s 59.0% words/sec/thread:   18534 lr:  0.020492 avg.loss:  0.633643 ETA:   0h22m57sm48s 62.7% words/sec/thread:   18516 lr:  0.018665 avg.loss:  0.613453 ETA:   0h20m55s 65.7% words/sec/thread:   18514 lr:  0.017139 avg.loss:  0.598342 ETA:   0h19m12s 74.2% words/sec/thread:   18521 lr:  0.012890 avg.loss:  0.563313 ETA:   0h14m26s48sm32s% words/sec/thread:   18495 lr:  0.006986 avg.loss:  0.522044 ETA:   0h 7m50s% words/sec/thread:   18495 lr:  0.006512 avg.loss:  0.519288 ETA:   0h 7m18s 6m26s 91.0% words/sec/thread:   18500 lr:  0.004524 avg.loss:  0.508030 ETA:   0h 5m 4s 0.490768 ETA:   0h 0m44s\n",
      "Training Progress: 100%|██████████| 5/5 [2:55:18<00:00, 2103.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train the FastText Model\n",
    "'''\n",
    "# version 1\n",
    "def train_fasttext_with_phrases(input_file):\n",
    "    # Read the file and build sentences\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentences.append(line.split())\n",
    "\n",
    "    # Detect and create bigrams/trigrams\n",
    "    bigram = Phraser(Phrases(sentences, min_count=5, threshold=10))\n",
    "    trigram = Phraser(Phrases(bigram[sentences], min_count=5, threshold=10))\n",
    "\n",
    "    # Apply the bigram/trigram models to the sentences\n",
    "    sentences_with_phrases = [trigram[bigram[sentence]] for sentence in sentences]\n",
    "\n",
    "    # Train FastText model\n",
    "    model = fasttext.train_unsupervised(input_file, model='skipgram')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model with the preprocessed Wikipedia text\n",
    "fasttext_model = train_fasttext_with_phrases('processed_wiki_id.txt')\n",
    "\n",
    "# Save the model\n",
    "fasttext_model.save_model(\"fasttext_indonesian_with_phrases.bin\")\n",
    "\n",
    "\n",
    "#version 2 with progress\n",
    "def train_fasttext_with_phrases(input_file, epochs=5, lr=0.05):\n",
    "    # Read the file and build sentences\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentences.append(line.split())\n",
    "\n",
    "    # Detect and create bigrams/trigrams\n",
    "    bigram = Phraser(Phrases(sentences, min_count=5, threshold=10))\n",
    "    trigram = Phraser(Phrases(bigram[sentences], min_count=5, threshold=10))\n",
    "\n",
    "    # Apply the bigram/trigram models to the sentences\n",
    "    sentences_with_phrases = [trigram[bigram[sentence]] for sentence in sentences]\n",
    "\n",
    "    # Create a temporary file to store the sentences with phrases\n",
    "    with open('temp_sentences_with_phrases.txt', 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences_with_phrases:\n",
    "            f.write(' '.join(sentence) + '\\n')\n",
    "\n",
    "    # Train FastText model with progress indication\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model = fasttext.train_unsupervised('temp_sentences_with_phrases.txt', model='skipgram', lr=lr, epoch=epoch)\n",
    "        print(f'Epoch {epoch}/{epochs} completed.')\n",
    "\n",
    "    return model\n",
    "'''\n",
    "\n",
    "# version 3\n",
    "\n",
    "def train_fasttext_with_phrases(input_file, epochs=5, lr=0.05):\n",
    "    # Read the file and build sentences\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentences.append(line.split())\n",
    "\n",
    "    # Detect and create bigrams/trigrams\n",
    "    bigram = Phraser(Phrases(sentences, min_count=5, threshold=10))\n",
    "    trigram = Phraser(Phrases(bigram[sentences], min_count=5, threshold=10))\n",
    "\n",
    "    # Apply the bigram/trigram models to the sentences\n",
    "    sentences_with_phrases = [trigram[bigram[sentence]] for sentence in sentences]\n",
    "\n",
    "    # Write the sentences with phrases to a temporary file\n",
    "    temp_file = 'temp_sentences_with_phrases.txt'\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences_with_phrases:\n",
    "            f.write(' '.join(sentence) + '\\n')\n",
    "\n",
    "    # Train FastText model with a progress bar\n",
    "    model = None\n",
    "    for epoch in tqdm(range(1, epochs + 1), desc='Training Progress'):\n",
    "        model = fasttext.train_unsupervised(temp_file, model='skipgram', lr=lr, epoch=epoch)\n",
    "\n",
    "    # Clean up the temporary file\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train the model with the preprocessed Wikipedia text\n",
    "fasttext_model = train_fasttext_with_phrases('processed_wiki_id.txt')\n",
    "\n",
    "# Save the model\n",
    "fasttext_model.save_model(\"fasttext_ina_100_with_phrases.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5710175   0.30058366  0.34796658  0.88001174  0.52425134 -0.02964741\n",
      " -0.17222954 -0.5002736   0.762855   -0.7790979   0.08115216  0.14185356\n",
      " -0.6173891  -0.0973238   0.3765095  -0.18282877 -0.02402508  0.19270276\n",
      " -0.63856393 -0.22311787  0.02718417  0.09362951 -0.13754086  0.12484796\n",
      "  0.29579833 -1.0931784  -0.21157692  0.85819167  0.21909578 -0.35906434\n",
      "  0.33472145 -0.01592343 -0.26751727 -0.82759494 -0.17570087 -0.4351454\n",
      " -0.0893899  -0.73693675 -0.04124119 -0.35097325 -0.89134544 -0.8440361\n",
      " -0.2711332   0.20650207  0.37705436  0.02467851 -0.34632164  0.0023733\n",
      "  0.334291    0.01247291 -0.6152394  -0.4529973  -0.43055084 -0.08579167\n",
      "  0.6661112  -0.42551488  0.7379743   0.06338852 -0.23205082 -0.49874333\n",
      " -0.94483197 -0.38910538  0.19761562 -0.48624828 -0.50785965 -0.03037283\n",
      "  0.05746352  0.21924978 -0.0416922  -0.31730434  0.40931764 -0.69406706\n",
      "  0.2582438   0.7814642  -1.0616256  -0.19469967 -0.15268809  0.24732283\n",
      "  0.3922155  -0.57278365 -0.31278664  0.31659085  0.18266565 -0.17567167\n",
      "  0.00166491  0.9856425   0.24358395 -0.22927389  0.488922    0.40644836\n",
      " -0.2326628   0.09825194  0.31876692 -0.58408046 -0.3358311  -0.56020945\n",
      "  0.1358487   0.05317448 -0.2999138  -0.1951633 ]\n",
      "[(0.9499713778495789, 'harimau_harimau'), (0.9327273368835449, 'macan_harimau'), (0.9088782072067261, 'singa_harimau'), (0.9017631411552429, 'harimaunya'), (0.8993623852729797, 'harimau_singa'), (0.8944528698921204, 'harimau_macan'), (0.8920230865478516, 'ekor_harimau'), (0.8858714699745178, 'harimau_putih'), (0.8777719736099243, 'seekor_harimau'), (0.8601592779159546, 'sang_harimau')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Using the Model\n",
    "# Load model\n",
    "model = fasttext.load_model(\"fasttext_ina_100_with_phrases.bin\")\n",
    "\n",
    "# Get word vector for a word in Bahasa Indonesia\n",
    "word_vector = model.get_word_vector(\"singa\")\n",
    "print(word_vector)\n",
    "# Find similar words\n",
    "similar_words = model.get_nearest_neighbors(\"harimau\")\n",
    "print(similar_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
