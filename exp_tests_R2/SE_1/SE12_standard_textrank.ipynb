{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. rutin2 membuat syspath ke root utk aktifkan __init__.py\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "sys.path.append(repo_root)\n",
    "\n",
    "#3. rutin3 Load the dataset\n",
    "#dataset_path = os.path.join(repo_root, \"notebooks/postager_nlp-id/dataset_ekstraksi_r29_pos_sm.xlsx\")\n",
    "dataset_path = os.path.join(repo_root, \"data/dataset_ekstraksi_r30_lg.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"judul\"] +\". \"+ df[\"isi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning paramater\n",
    "tuning_multiplier = 1  #aktor pengali dari score jika kata tersebut merupakan frase. default = 1 (perlu variasi 0.6 - 0.75)\n",
    "tuning_f_phrase = 3  #score minimum utk bisa disebut frase\n",
    "m_prediksi = 10  #jumlah top -n keyword prediksi\n",
    "n_top_phrase = 3   #jumlah frase yg akan di cari dalam fungsi get_top_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    usulan personil penting proposed key personnel...\n",
       "1    template document jtb gpf project mengacu kepa...\n",
       "2    change inquiry terkait usulan perubahan lingku...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 Preprocess\n",
    "'''\n",
    "stopwords tidak masuk dalam preprocessing\n",
    "'''\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "text = df['text'].apply(preprocess)\n",
    "text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('template document', 2), ('document jtb', 1), ('jtb gpf', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_phrase(corpus, n=n_top_phrase):  #perlu ada improvement karena phrase yg di hasilkan masih blm proper\n",
    "    vec1 = CountVectorizer(ngram_range=(2,3), max_features=2000).fit([corpus])\n",
    "    bag_of_words = vec1.transform([corpus])\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    # perlu di buat filter jika pola tidak mengikuti kaidah kata majemuk indonesia di excludekan.\n",
    "    return words_freq[:n]\n",
    "\n",
    "# example\n",
    "phrase = get_top_phrase(text[1], n=n_top_phrase)\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text and detect phrases\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_and_detect_phrases(text):\n",
    "    # Tokenize using CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3), min_df=3)  # Adjust ngram_range as needed\n",
    "    X = vectorizer.fit_transform(text)\n",
    "\n",
    "    # Inverse transform to get list of tokens per document\n",
    "    tokens = vectorizer.inverse_transform(X)\n",
    "\n",
    "    # Create a DataFrame with the tokens\n",
    "    tokenized_df = pd.DataFrame({'tokens': list(tokens)}, index=df.index)\n",
    "\n",
    "    return tokenized_df\n",
    "\n",
    "words = tokenize_and_detect_phrases(text)\n",
    "words.head(3)\n",
    "words.to_excel('words_sample.xlsx', engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Load stopword\n",
    "def load_stopwords(repo_root, stopwords_file_path):\n",
    "    \"\"\"\n",
    "    Load stopwords from a file located in the repository.\n",
    "\n",
    "    :param repo_root: Root directory of the repository.\n",
    "    :param stopwords_file_path: Relative path to the stopwords file from the repo_root.\n",
    "    :return: Set of stopwords.\n",
    "    \"\"\"\n",
    "    stopwords_path = os.path.join(repo_root, stopwords_file_path)\n",
    "    with open(stopwords_path, 'r') as file:\n",
    "        stopwords = set(file.read().strip().splitlines())\n",
    "    return stopwords\n",
    "\n",
    "repo_root = repo_root\n",
    "stopwords = load_stopwords(repo_root, \"notebooks/stopwords_tuning/all_stop_words.txt\")\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Co-occurrence Matrix\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def build_co_occurrence_matrix(words, window_size=3):\n",
    "    co_occurrence = defaultdict(int)\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        window = words[i:i+window_size]\n",
    "        for j in range(window_size):\n",
    "            for k in range(j+1, window_size):\n",
    "                w1, w2 = sorted([window[j], window[k]])\n",
    "                if w1 != w2:\n",
    "                    co_occurrence[(w1, w2)] += 1\n",
    "    return co_occurrence\n",
    "\n",
    "# Build Graph and Compute TextRank\n",
    "def build_graph_and_compute_textrank(co_occurrence, w2v_model):\n",
    "    G = nx.Graph()\n",
    "    for (w1, w2), weight in co_occurrence.items():\n",
    "        if weight > 0:\n",
    "            G.add_edge(w1, w2, weight=weight)\n",
    "    return nx.pagerank(G)\n",
    "\n",
    "# Example\n",
    "# dibuat tokekization dahulu\n",
    "\n",
    "co_occurrence = build_co_occurrence_matrix(words)\n",
    "#co_occurrence\n",
    "\n",
    "scores = build_graph_and_compute_textrank(co_occurrence, w2v_model)\n",
    "print(\"mengandung stopword:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def build_graph(vocab_len, processed_text, vocabulary):\n",
    "    \"\"\"\n",
    "    Builds a weighted edge graph based on co-occurrences of words in the text.\n",
    "    + perlu ada tambahan formula untuk menghitung score kata yg ada dalam title menjadi lebih besar. (1, 1.5, 2)\n",
    "    \"\"\"\n",
    "    weighted_edge = np.zeros((vocab_len, vocab_len), dtype=np.float32)\n",
    "    score = np.ones((vocab_len), dtype=np.float32)\n",
    "    window_size = 3  \n",
    "    covered_coocurrences = []\n",
    "\n",
    "    for i in range(vocab_len):\n",
    "        for j in range(vocab_len):\n",
    "            if j == i:\n",
    "                weighted_edge[i][j] = 0\n",
    "            else:\n",
    "                for window_start in range(len(processed_text) - window_size):\n",
    "                    window_end = window_start + window_size\n",
    "                    window = processed_text[window_start:window_end]\n",
    "                    if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                        index_of_i = window_start + window.index(vocabulary[i])\n",
    "                        index_of_j = window_start + window.index(vocabulary[j])\n",
    "                        if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                            weighted_edge[i][j] += 1 / math.fabs(index_of_i - index_of_j)\n",
    "                            covered_coocurrences.append([index_of_i, index_of_j])\n",
    "\n",
    "    inout = np.sum(weighted_edge, axis=1)\n",
    "  \n",
    "    MAX_ITERATIONS = 50\n",
    "    d = 0.85\n",
    "    threshold = 0.0001\n",
    "    for _ in range(MAX_ITERATIONS):\n",
    "        prev_score = np.copy(score)\n",
    "        for i in range(vocab_len):\n",
    "            summation = 0\n",
    "            for j in range(vocab_len):\n",
    "                if weighted_edge[i][j] != 0:\n",
    "                    summation += (weighted_edge[i][j] / inout[j]) * score[j]\n",
    "            score[i] = (1 - d) + d * summation\n",
    "        if np.sum(np.fabs(prev_score - score)) <= threshold:\n",
    "            break\n",
    "\n",
    "    return vocabulary, score\n",
    "\n",
    "def score_phrases(unique_phrases, vocabulary, score, multiplier=tuning_multiplier):\n",
    "    \"\"\"\n",
    "    Computes the score of each phrase using the given vocabulary, word scores, and multiplier.\n",
    "    \"\"\"\n",
    "    phrase_scores = []\n",
    "    keywords = []\n",
    "    for phrase in unique_phrases:\n",
    "        phrase_score = 0\n",
    "        keyword = ''\n",
    "        for word in phrase:\n",
    "            keyword += str(word) + \" \"\n",
    "            phrase_score += score[vocabulary.index(word)]\n",
    "        phrase_score *= multiplier\n",
    "        phrase_scores.append(phrase_score)\n",
    "        keywords.append(keyword.strip())\n",
    "\n",
    "    return keywords, phrase_scores\n",
    "\n",
    "\n",
    "def get_top_phrase(corpus, n=n_top_phrase):  #perlu ada improvement karena phrase yg di hasilkan masih blm proper\n",
    "    vec1 = CountVectorizer(ngram_range=(2,3),  \n",
    "            max_features=2000).fit([corpus])\n",
    "    bag_of_words = vec1.transform([corpus])\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    # perlu di buat filter jika pola tidak mengikuti kaidah kata majemuk indonesia di excludekan.\n",
    "    return words_freq[:n]\n",
    "\n",
    "def predict_keywords(text, m=10, f_phrase=5, tuning_multiplier=1):\n",
    "    \"\"\"\n",
    "    Predicts the top m keywords and top f_phrase phrases for the given text.\n",
    "    processed_text = text keseluruhan\n",
    "    vocabulary = unique word dalam proccesesed_text\n",
    "    \"\"\"\n",
    "    processed_text = word_tokenize(text)\n",
    "    vocabulary = list(set(processed_text))\n",
    "    vocab_len = len(vocabulary)\n",
    "    vocabulary, score = build_graph(vocab_len, processed_text, vocabulary)\n",
    "    unigram = pd.DataFrame({\n",
    "        'Keyword': vocabulary,\n",
    "        'Score': score\n",
    "    }).nlargest(m, 'Score')\n",
    "    \n",
    "    bi_trigram = pd.DataFrame(get_top_phrase(text, n=50), columns=['Phrase', 'Score'])\n",
    "    bi_trigram = bi_trigram[bi_trigram['Score'] >= f_phrase]\n",
    "    bi_trigram['Tokens'] = bi_trigram['Phrase'].apply(word_tokenize)\n",
    "    unique_phrases = bi_trigram['Tokens'].values.tolist()\n",
    "    keywords, phrase_scores = score_phrases(unique_phrases, vocabulary, score, tuning_multiplier) #BUG_1 not accesed by pylance, krn tidak di gunakan di procss selanjutnya\n",
    "    # memasukan score ke dalam dataframe\n",
    "    bi_trigram = pd.DataFrame({\n",
    "        'Phrase': keywords,\n",
    "        'Score': phrase_scores\n",
    "    }).nlargest(m, 'Score')\n",
    "\n",
    "      # Combine unigram and bi_trigram dataframes\n",
    "    predict_keywords = pd.concat([unigram, bi_trigram[['Phrase', 'Score']].rename(columns={'Phrase': 'Keyword'})])\\\n",
    "                    .sort_values('Score', ascending=False)\\\n",
    "                    .nlargest(m, 'Score')\\\n",
    "                    .reset_index(drop=True)\n",
    "\n",
    "    return predict_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 778...!                  Keyword     Score\n",
      "0      rockwool malaysia  5.646585\n",
      "1                usa aml  4.865897\n",
      "2                    aml  3.340652\n",
      "3               rockwool  3.226768\n",
      "4  approved manufacturer  3.114578\n",
      "5               malaysia  2.419817\n",
      "6             insulation  2.122567\n",
      "7                lapinus  1.846593\n",
      "8               approved  1.558756\n",
      "9           manufacturer  1.555823\n",
      "                   0        1    2         3                      4         5  \\\n",
      "0  rockwool malaysia  usa aml  aml  rockwool  approved manufacturer  malaysia   \n",
      "\n",
      "            6        7         8             9  \n",
      "0  insulation  lapinus  approved  manufacturer  \n",
      "      0     1     2     3     4     5     6     7     8     9\n",
      "0  5.65  4.87  3.34  3.23  3.11  2.42  2.12  1.85  1.56  1.56\n",
      "keyphrase.shape[1] : 20\n",
      "0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "predict_textrank = pd.DataFrame()\n",
    "data_ind = 778\n",
    "print('Processing index', data_ind, end='...! ')\n",
    "keyphrase = predict_keywords(df_tr[data_ind], m_prediksi, tuning_f_phrase, tuning_multiplier).reset_index(drop=True)\n",
    "print(keyphrase)\n",
    "a = pd.DataFrame(keyphrase.Keyword).T.reset_index(drop=True)\n",
    "print(a)\n",
    "b = pd.DataFrame(keyphrase.Score).round(2).T.reset_index(drop=True)\n",
    "print(b)\n",
    "keyphrase = pd.concat([a, b], axis=1)\n",
    "print('keyphrase.shape[1] :', keyphrase.shape[1])\n",
    "    \n",
    "# Ensure that keyphrase has the same number of columns as max_columns by filling in with NaN\n",
    "max_columns = 20\n",
    "missing_cols = max_columns - keyphrase.shape[1]\n",
    "print(missing_cols)\n",
    "\n",
    "if missing_cols > 0:\n",
    "    # diisi Nan\n",
    "    #nan_cols = pd.DataFrame(np.nan, index=keyphrase.index, columns=[f'col{col}' for col in range(keyphrase.shape[1], max_columns)])\n",
    "    #keyphrase = pd.concat([keyphrase, nan_cols], axis=1)\n",
    "\n",
    "    # diisi 0\n",
    "    zero_cols = pd.DataFrame(11, index=keyphrase.index, columns=[f'col{col}' for col in range(keyphrase.shape[1], max_columns)])\n",
    "    keyphrase = pd.concat([keyphrase, zero_cols], axis=1)\n",
    "\n",
    "predict_textrank = pd.concat([predict_textrank, keyphrase], ignore_index=True)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rockwool malaysia</td>\n",
       "      <td>usa aml</td>\n",
       "      <td>aml</td>\n",
       "      <td>rockwool</td>\n",
       "      <td>approved manufacturer</td>\n",
       "      <td>malaysia</td>\n",
       "      <td>insulation</td>\n",
       "      <td>lapinus</td>\n",
       "      <td>approved</td>\n",
       "      <td>manufacturer</td>\n",
       "      <td>5.65</td>\n",
       "      <td>4.87</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.12</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0        1    2         3                      4         5  \\\n",
       "0  rockwool malaysia  usa aml  aml  rockwool  approved manufacturer  malaysia   \n",
       "\n",
       "            6        7         8             9     0     1     2     3     4  \\\n",
       "0  insulation  lapinus  approved  manufacturer  5.65  4.87  3.34  3.23  3.11   \n",
       "\n",
       "      5     6     7     8     9  \n",
       "0  2.42  2.12  1.85  1.56  1.56  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pembayaran rekening tambahan side letter sehubungan bersama memohon membayarkan tagihan sebagai terinci tabel dapat rekening tambahan side letter cl inv iii march cover letter invoice inv pp rek usd march correct invoice progress feb inv mp rek usd march correct invoice payment milestone inv cco rek usd march correct invoice dapat perhatian ucapkan\n"
     ]
    }
   ],
   "source": [
    "print(df_tr[976])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict keywords for all sentences in the dataframe and save the in dataframe\n",
    "#from utils import build_graph, score_phrases, get_top_phrase, predict_keywords, write_excel\n",
    "predict_textrank = pd.DataFrame()\n",
    "for i in df_tr.index: # ada error di index 394\n",
    "#for i in df.loc[970:].index:\n",
    "    print('Processing index', i, end='...! ')\n",
    "    keyphrase = predict_keywords(df_tr[i], m_prediksi, tuning_f_phrase, tuning_multiplier).reset_index(drop=True)\n",
    "    a = pd.DataFrame(keyphrase.Keyword).T.reset_index(drop=True)\n",
    "    b = pd.DataFrame(keyphrase.Score).round(2).T.reset_index(drop=True)\n",
    "    keyphrase = pd.concat([a, b], axis=1)\n",
    "    \n",
    "    # Ensure that keyphrase has the same number of columns as max_columns by filling in with NaN\n",
    "    max_columns = 20\n",
    "    missing_cols = max_columns - keyphrase.shape[1]\n",
    "    if missing_cols > 0:\n",
    "        nan_cols = pd.DataFrame(np.nan, index=keyphrase.index, columns=[f'col{col}' for col in range(keyphrase.shape[1], max_columns)])\n",
    "        keyphrase = pd.concat([keyphrase, nan_cols], axis=1)\n",
    "\n",
    "    predict_textrank = pd.concat([predict_textrank, keyphrase], ignore_index=True)\n",
    "    print('Done')\n",
    "#predict_textrank.columns = ['key_1', 'key_2','key_3','score_1', 'score_2','score_3'] \n",
    "predict_textrank.columns = ['key_1', 'key_2','key_3', 'key_4', 'key_5','key_6', 'key_7', 'key_8','key_9','key_10','score_1', 'score_2','score_3','score_4', 'score_5','score_6','score_7', 'score_8','score_9','score_10'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eval\n",
    "\n",
    "targets = df[[\"k1\", \"k2\", \"k3\",\"k4\", \"k5\", \"k6\",\"k7\"]].values.tolist()\n",
    "df_targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>key_4</th>\n",
       "      <th>key_5</th>\n",
       "      <th>key_6</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partial_match</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_match</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key_1          key_2       key_3       key_4     key_5     key_6  \\\n",
       "0       no_match       no_match    no_match    no_match  no_match  no_match   \n",
       "1  partial_match  partial_match  full_match  full_match  no_match  no_match   \n",
       "2       no_match  partial_match    no_match    no_match  no_match  no_match   \n",
       "\n",
       "      key_7          key_8     key_9    key_10  flex_recall  flex_prec  \n",
       "0  no_match       no_match  no_match  no_match        0.000        0.0  \n",
       "1  no_match       no_match  no_match  no_match        0.571        0.4  \n",
       "2  no_match  partial_match  no_match  no_match        0.286        0.2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation TextRank top 10\n",
    "predict_textrank_list_10 = predict_textrank[['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10']].values.tolist()\n",
    "eval_textrank_10 = eval(predict_textrank_list_10, targets, True).round(3)\n",
    "eval_textrank_10.columns = ['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_10 = eval_textrank_10[['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_10.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_10 = eval_textrank_10['flex_recall'].mean()\n",
    "textrank_prec_10 = eval_textrank_10['flex_prec'].mean()\n",
    "textrank_f1_10 = 2 * (textrank_prec_10 * textrank_recall_10) / (textrank_prec_10 + textrank_recall_10)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_10 = pd.DataFrame({'textrank': [textrank_recall_10, textrank_prec_10, textrank_f1_10]}, index=['recall', 'precision', 'F1'])\n",
    "summary_10 = summary_10.round(3)\n",
    "#summary_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>key_4</th>\n",
       "      <th>key_5</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partial_match</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_match</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key_1          key_2       key_3       key_4     key_5  \\\n",
       "0       no_match       no_match    no_match    no_match  no_match   \n",
       "1  partial_match  partial_match  full_match  full_match  no_match   \n",
       "2       no_match  partial_match    no_match    no_match  no_match   \n",
       "\n",
       "   flex_recall  flex_prec  \n",
       "0        0.000        0.0  \n",
       "1        0.571        0.8  \n",
       "2        0.143        0.2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation TextRank top 5\n",
    "predict_textrank_list_5 = predict_textrank[['key_1','key_2','key_3', 'key_4','key_5']].values.tolist()\n",
    "eval_textrank_5 = eval(predict_textrank_list_5, targets, True).round(3)\n",
    "eval_textrank_5.columns = ['key_1','key_2','key_3', 'key_4','key_5','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_5 = eval_textrank_5[['key_1','key_2','key_3', 'key_4','key_5', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_5.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_5 = eval_textrank_5['flex_recall'].mean()\n",
    "textrank_prec_5 = eval_textrank_5['flex_prec'].mean()\n",
    "textrank_f1_5 = 2 * (textrank_prec_5 * textrank_recall_5) / (textrank_prec_5 + textrank_recall_5)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_5 = pd.DataFrame({'textrank': [textrank_recall_5, textrank_prec_5, textrank_f1_5]}, index=['recall', 'precision', 'F1'])\n",
    "summary_5 = summary_5.round(3)\n",
    "summary_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partial_match</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>0.429</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_match</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key_1          key_2       key_3  flex_recall  flex_prec\n",
       "0       no_match       no_match    no_match        0.000      0.000\n",
       "1  partial_match  partial_match  full_match        0.429      1.000\n",
       "2       no_match  partial_match    no_match        0.143      0.333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation TextRank top 3\n",
    "predict_textrank_list_3 = predict_textrank[['key_1','key_2','key_3']].values.tolist()\n",
    "eval_textrank_3 = eval(predict_textrank_list_3, targets, True).round(3)\n",
    "eval_textrank_3.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_3 = eval_textrank_3[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_3.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_3 = eval_textrank_3['flex_recall'].mean()\n",
    "textrank_prec_3 = eval_textrank_3['flex_prec'].mean()\n",
    "textrank_f1_3 = 2 * (textrank_prec_3 * textrank_recall_3) / (textrank_prec_3 + textrank_recall_3)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_3 = pd.DataFrame({'textrank': [textrank_recall_3, textrank_prec_3, textrank_f1_3]}, index=['recall', 'precision', 'F1'])\n",
    "summary_3 = summary_3.round(3)\n",
    "#summary_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_textrank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Combine dataframe predict_textrank, df_targets and eval_textrank\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predict_textrank_10 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mpredict_textrank\u001b[49m, df_targets, eval_textrank_10], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m predict_textrank_10\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_textrank' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_10 = pd.concat([predict_textrank, df_targets, eval_textrank_10], axis=1)\n",
    "#predict_textrank_10.head(3)\n",
    "\n",
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_5 = pd.concat([predict_textrank, df_targets, eval_textrank_5], axis=1)\n",
    "#predict_textrank_5.head(3)\n",
    "\n",
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_3 = pd.concat([predict_textrank, df_targets, eval_textrank_3], axis=1)\n",
    "predict_textrank_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "property 'book' of 'OpenpyxlWriter' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jimx/Documents/GitHub/kw_ina_extraction/exp_tesis/SE2_tr_phrase_cv.ipynb Cell 22\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jimx/Documents/GitHub/kw_ina_extraction/exp_tesis/SE2_tr_phrase_cv.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sheet_name_3 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSE2_tr_phrase_cv_3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jimx/Documents/GitHub/kw_ina_extraction/exp_tesis/SE2_tr_phrase_cv.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSE2_tr_phrase_cv.xlsx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jimx/Documents/GitHub/kw_ina_extraction/exp_tesis/SE2_tr_phrase_cv.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m write_excel(predict_textrank_10, sheet_name_10, output_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimx/Documents/GitHub/kw_ina_extraction/exp_tesis/SE2_tr_phrase_cv.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m write_excel(predict_textrank_5, sheet_name_5, output_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimx/Documents/GitHub/kw_ina_extraction/exp_tesis/SE2_tr_phrase_cv.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m write_excel(predict_textrank_3, sheet_name_3, output_file)\n",
      "File \u001b[0;32m~/Documents/GitHub/kw_ina_extraction/utils/ia_file_operation.py:16\u001b[0m, in \u001b[0;36mwrite_excel\u001b[0;34m(df, sheet_name, filename)\u001b[0m\n\u001b[1;32m     13\u001b[0m     book \u001b[39m=\u001b[39m Workbook()  \u001b[39m# If the file doesn't exist, create a new workbook\u001b[39;00m\n\u001b[1;32m     15\u001b[0m writer \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mExcelWriter(filename, engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenpyxl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m writer\u001b[39m.\u001b[39;49mbook \u001b[39m=\u001b[39m book\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m sheet_name \u001b[39min\u001b[39;00m book\u001b[39m.\u001b[39msheetnames:  \u001b[39m# If sheet already exists, delete it\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m#idx = book.sheetnames.index(sheet_name)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     sheet \u001b[39m=\u001b[39m book[sheet_name]\n",
      "\u001b[0;31mAttributeError\u001b[0m: property 'book' of 'OpenpyxlWriter' object has no setter"
     ]
    }
   ],
   "source": [
    "# Write predictions to excel file\n",
    "from utils import write_excel\n",
    "\n",
    "sheet_name_10 = 'SE12_standard_textrank_10'\n",
    "sheet_name_5 = 'SE12_standard_textrank_5'\n",
    "sheet_name_3 = 'SE12_standard_textrank_3'\n",
    "\n",
    "output_file = 'SE12_standard_textrank.xlsx'\n",
    "write_excel(predict_textrank_10, sheet_name_10, output_file)\n",
    "write_excel(predict_textrank_5, sheet_name_5, output_file)\n",
    "write_excel(predict_textrank_3, sheet_name_3, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_ina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
