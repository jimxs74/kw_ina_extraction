{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R1 dengan w2v model yg di peroleh dari pretraining model wikipedia indonesia.\n",
    "yg di process unigram, basis dari paper Yujun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. rutin2 membuat syspath ke root utk aktifkan __init__.py\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "sys.path.append(repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. rutin3 Load the dataset\n",
    "dataset_path = os.path.join(repo_root, \"data/dataset_ekstraksi_r29.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"judul\"] +\". \"+ df[\"isi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemover, ArrayDictionary\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    fungsi untuk menghilangkan karakter yg tidak bermakna dan menghilangkan stopword.\n",
    "    referensi stopword: tala + sastrawi + custom\n",
    "    '''\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    stopwords_path = os.path.join(repo_root, \"data/all_stop_words.txt\")\n",
    "    with open(stopwords_path, 'r') as f:\n",
    "        stopwords = [line.strip() for line in f]\n",
    "    \n",
    "    dictionary = ArrayDictionary(stopwords)\n",
    "    str = StopWordRemover(dictionary)\n",
    "    text = str.remove(text) # 2x cleaning stop word\n",
    "    text = str.remove(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_tokenize(text):\n",
    "    '''\n",
    "    fungsi untuk memproses text yg sudah di process di preprocess() menjadi token\n",
    "    dilakukan 3x preprocessing, karena stopword masih sering lewat kalau hanya 1x\n",
    "    '''\n",
    "    text = preprocess(text)\n",
    "    text = preprocess(text)\n",
    "    text = preprocess(text)\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token]  # remove any empty tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word_embeddings(text_row, w2v_model):\n",
    "    # Get the word embeddings for each word in the preprocessed text\n",
    "    word_embeddings = {}\n",
    "    for word in text_row:\n",
    "        if word in w2v_model.wv:\n",
    "            word_embeddings[word] = w2v_model.wv[word]\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_weighted_graph(word_embeddings):\n",
    "    # Calculate the cosine similarity matrix between all pairs of word embeddings\n",
    "    cosine_similarities = cosine_similarity(list(word_embeddings.values()))\n",
    "    # Construct a weighted graph representation of the text\n",
    "    graph = {}\n",
    "    for i, word_i in enumerate(word_embeddings.keys()):\n",
    "        graph[word_i] = {}\n",
    "        for j, word_j in enumerate(word_embeddings.keys()):\n",
    "            if i != j:\n",
    "                graph[word_i][word_j] = cosine_similarities[i][j]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(graph, d=0.85, max_iter=100, tol=1e-4):\n",
    "    # Initialize all node scores to 1\n",
    "    scores = {node: 1 for node in graph.keys()}\n",
    "    # Iterate until convergence\n",
    "    for i in range(max_iter):\n",
    "        old_scores = dict(scores)\n",
    "        for node_i, neighbors in graph.items():\n",
    "            # Calculate the new score for node i\n",
    "            score_i = 1 - d\n",
    "            for node_j, weight_ij in neighbors.items():\n",
    "                score_i += d * weight_ij * scores[node_j] / sum(graph[node_j].values())\n",
    "            scores[node_i] = score_i\n",
    "        # Check for convergence\n",
    "        max_diff = max([abs(old_scores[node] - scores[node]) for node in graph.keys()])\n",
    "        if max_diff < tol:\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(w2v_model, text_row, n=10):\n",
    "    preprocessed_text = preprocess_tokenize(text_row)\n",
    "    word_embeddings = gen_word_embeddings(preprocessed_text, w2v_model)\n",
    "    graph = construct_weighted_graph(word_embeddings)\n",
    "    scores = textrank(graph)\n",
    "    sorted_words = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_n_keywords = [word for word, score in sorted_words[:n]]\n",
    "    keyword_df = pd.DataFrame({'keywords': top_n_keywords, 'score': [score for word, score in sorted_words[:n]]})\n",
    "\n",
    "    return keyword_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mindex:\n\u001b[1;32m      7\u001b[0m     text_row \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m][i]\n\u001b[0;32m----> 8\u001b[0m     unigram \u001b[39m=\u001b[39m extract_keywords(w2v_model, text_row, n\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     a \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(unigram\u001b[39m.\u001b[39mkeywords)\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     b \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(unigram\u001b[39m.\u001b[39mscore)\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mextract_keywords\u001b[0;34m(w2v_model, text_row, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m word_embeddings \u001b[39m=\u001b[39m gen_word_embeddings(preprocessed_text, w2v_model)\n\u001b[1;32m      4\u001b[0m graph \u001b[39m=\u001b[39m construct_weighted_graph(word_embeddings)\n\u001b[0;32m----> 5\u001b[0m scores \u001b[39m=\u001b[39m textrank(graph)\n\u001b[1;32m      6\u001b[0m sorted_words \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(scores\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m item: item[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m top_n_keywords \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word, score \u001b[39min\u001b[39;00m sorted_words[:n]]\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mtextrank\u001b[0;34m(graph, d, max_iter, tol)\u001b[0m\n\u001b[1;32m      9\u001b[0m     score_i \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m d\n\u001b[1;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m node_j, weight_ij \u001b[39min\u001b[39;00m neighbors\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 11\u001b[0m         score_i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m d \u001b[39m*\u001b[39m weight_ij \u001b[39m*\u001b[39m scores[node_j] \u001b[39m/\u001b[39m \u001b[39msum\u001b[39m(graph[node_j]\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     12\u001b[0m     scores[node_i] \u001b[39m=\u001b[39m score_i\n\u001b[1;32m     13\u001b[0m \u001b[39m# Check for convergence\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "pred_w2v_tr_tune = pd.DataFrame()\n",
    "w2v_path = os.path.join(repo_root, \"models/w2v/idwiki_word2vec_100_new_lower.model\")\n",
    "w2v_model = Word2Vec.load(w2v_path)\n",
    "for i in df.index:\n",
    "    text_row = df['text'][i]\n",
    "    unigram = extract_keywords(w2v_model, text_row, n=3).reset_index(drop=True)\n",
    "    a = pd.DataFrame(unigram.keywords).T.reset_index(drop=True)\n",
    "    b = pd.DataFrame(unigram.score).T.reset_index(drop=True)\n",
    "\n",
    "    # add extra empty columns to a and b dataframes if necessary\n",
    "    if a.shape[1] < 3:\n",
    "        for i in range(3 - a.shape[1]):\n",
    "            a[f'col{i+1}'] = ''\n",
    "            b[f'col{i+1}'] = ''\n",
    "    unigram = pd.concat([a, b], axis=1)\n",
    "    if unigram.shape[1] < 6:\n",
    "        for i in range(6 - unigram.shape[1]):\n",
    "            unigram[f'col{i+1}'] = ''\n",
    "    unigram.columns = ['key_1', 'key_2','key_3','score_1', 'score_2','score_3']\n",
    "\n",
    "    pred_w2v_tr_tune = pd.concat([pred_w2v_tr_tune, unigram], ignore_index=True)\n",
    "    pred_w2v_tr_tune[['score_1', 'score_2', 'score_3']] = pred_w2v_tr_tune[['score_1', 'score_2', 'score_3']].round(3)\n",
    "\n",
    "pred_w2v_tr_tune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eval\n",
    "\n",
    "targets = df[[\"k1\", \"k2\", \"k3\",\"k4\", \"k5\", \"k6\",\"k7\"]].values.tolist()\n",
    "df_targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation TextRank\n",
    "predict_w2v_tr_list = pred_w2v_tr_tune[['key_1','key_2','key_3']].values.tolist()\n",
    "eval_w2v_textrank = eval(predict_w2v_tr_list, targets, True).round(3)\n",
    "eval_w2v_textrank.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_w2v_textrank = eval_w2v_textrank[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "eval_w2v_textrank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "w2v_textrank_recall = eval_w2v_textrank['flex_recall'].mean()\n",
    "w2v_textrank_prec = eval_w2v_textrank['flex_prec'].mean()\n",
    "w2v_textrank_f1 = 2 * (w2v_textrank_prec * w2v_textrank_recall) / (w2v_textrank_prec + w2v_textrank_recall)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary = pd.DataFrame({'textrank': [w2v_textrank_recall, w2v_textrank_prec, w2v_textrank_f1]}, index=['recall', 'precision', 'F1'])\n",
    "summary = summary.round(3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_w2v_textrank = pd.concat([pred_w2v_tr_tune, df_targets, eval_w2v_textrank], axis=1)\n",
    "predict_w2v_textrank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to excel file\n",
    "from utils import write_excel\n",
    "\n",
    "sheet_name = 'w2v_ia_textrank'\n",
    "output_file = 'w2v_ia_textrank.xlsx'\n",
    "write_excel(predict_w2v_textrank, sheet_name, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
