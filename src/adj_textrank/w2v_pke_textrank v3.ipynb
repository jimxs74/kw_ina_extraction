{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine PKE textrank and word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. rutin2 membuat syspath ke root utk aktifkan __init__.py\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "sys.path.append(repo_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. rutin3 Load the dataset\n",
    "dataset_path = os.path.join(repo_root, \"notebooks/postager_nlp-id/dataset_ekstraksi_r29_pos_sm.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"judul\"] +\". \"+ df[\"isi\"]\n",
    "df_pos = df['pos_sentence_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemover, ArrayDictionary\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    stopwords_path = os.path.join(repo_root, \"data/all_stop_words.txt\")\n",
    "    with open(stopwords_path, 'r') as f:\n",
    "        stopwords = [line.strip() for line in f]\n",
    "\n",
    "    dictionary = ArrayDictionary(stopwords)\n",
    "    str = StopWordRemover(dictionary)\n",
    "    text = str.remove(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess)\n",
    "df_tr = df['preprocessed_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(words, n=2):\n",
    "    \"\"\"Generate ngrams from a list of words.\"\"\"\n",
    "    return [\" \".join(gram) for gram in ngrams(words, n)]\n",
    "\n",
    "def get_phrase_embedding(phrase, w2v_model):\n",
    "    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_keyphrases_with_ngrams(text, w2v_model):\n",
    "    # Tokenize the text into unigrams\n",
    "    unigrams = text.split()\n",
    "    \n",
    "    # Generate bigrams and trigrams\n",
    "    bigrams = generate_ngrams(unigrams, 2)\n",
    "    trigrams = generate_ngrams(unigrams, 3)\n",
    "    \n",
    "    # Combine unigrams, bigrams, and trigrams\n",
    "    all_tokens = unigrams + bigrams + trigrams\n",
    "    \n",
    "    # Get embeddings for each token (averaging word embeddings for bigrams/trigrams)\n",
    "    token_embeddings = [get_phrase_embedding(token, w2v_model) for token in all_tokens]\n",
    "    \n",
    "    # Filter out tokens that don't have embeddings\n",
    "    tokens, embeddings = zip(*[(token, emb) for token, emb in zip(all_tokens, token_embeddings) if emb is not None])\n",
    "    \n",
    "    # Compute the cosine similarity between token embeddings\n",
    "    cosine_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Create a graph and connect tokens with high similarity\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            if cosine_matrix[i][j] > 0.5:  # This threshold can be adjusted\n",
    "                G.add_edge(tokens[i], tokens[j], weight=cosine_matrix[i][j])\n",
    "    \n",
    "    # Compute the PageRank scores to rank the tokens\n",
    "    scores = nx.pagerank(G)\n",
    "\n",
    "    # Extract top N keyphrases along with their scores\n",
    "    ranked_tokens = sorted(((scores[token], token) for token in tokens if token in scores), reverse=True)\n",
    "    keyphrases_with_scores = [(token, score) for score, token in ranked_tokens[:3]]\n",
    "\n",
    "    return keyphrases_with_scores\n",
    "\n",
    "# Note: This is just the function definition for reference.\n",
    "# You can use this updated function in your environment to process the dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = os.path.join(repo_root, \"models/w2v/idwiki_word2vec_100_new_lower.model\")\n",
    "w2v_model = Word2Vec.load(w2v_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('acuan pengelolaan atas', 0.023077737722580618), ('demikian acuan pengelolaan', 0.022917046023445624), ('processing facilities', 0.022451316561733103)]\n"
     ]
    }
   ],
   "source": [
    "# Assuming w2v_model is your loaded Word2Vec model\n",
    "text = df_tr[1]\n",
    "keyphrases = extract_keyphrases_with_ngrams(text, w2v_model)\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Iterate over the dataframe\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df_pos:\n\u001b[0;32m----> 6\u001b[0m     keyphrases_with_scores \u001b[39m=\u001b[39m extract_keyphrases_with_ngrams(text, w2v_model)\n\u001b[1;32m      7\u001b[0m     results\u001b[39m.\u001b[39mextend(keyphrases_with_scores)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Convert the results to a dataframe\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m, in \u001b[0;36mextract_keyphrases_with_ngrams\u001b[0;34m(text, w2v_model)\u001b[0m\n\u001b[1;32m     33\u001b[0m token_embeddings \u001b[39m=\u001b[39m [get_phrase_embedding(token, w2v_model) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m all_tokens]\n\u001b[1;32m     35\u001b[0m \u001b[39m# Filter out tokens that don't have embeddings\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m tokens, embeddings \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[(token, emb) \u001b[39mfor\u001b[39;00m token, emb \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(all_tokens, token_embeddings) \u001b[39mif\u001b[39;00m emb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n\u001b[1;32m     38\u001b[0m \u001b[39m# Compute the cosine similarity between token embeddings\u001b[39;00m\n\u001b[1;32m     39\u001b[0m cosine_matrix \u001b[39m=\u001b[39m cosine_similarity(embeddings)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# Assuming df_pos is your dataframe with texts\n",
    "results = []\n",
    "\n",
    "# Iterate over the dataframe\n",
    "for text in df_pos:\n",
    "    keyphrases_with_scores = extract_keyphrases_with_ngrams(text, w2v_model)\n",
    "    results.extend(keyphrases_with_scores)\n",
    "\n",
    "# Convert the results to a dataframe\n",
    "predict_textrank = pd.DataFrame(results, columns=['Keyword', 'Score'])\n",
    "predict_textrank.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_ina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
