{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine PKE textrank and word2vec N-gram\n",
    "Feature \n",
    "\n",
    "V1 Feature\n",
    "- Membuat jalan program\n",
    "- Ada deteksi unigram/bigram/trigram\n",
    "\n",
    "V2\n",
    "- filter jika kata tidak ada dalam model embedding w2v\n",
    "- implementasi stopwords\n",
    "\n",
    "V3\n",
    "- deteksi bigram dari frekuency lebih dari 2x\n",
    "\n",
    "V4\n",
    "- detection bigram and trigram using nlp-id\n",
    "- preprocessing tanpa stopwords\n",
    "\n",
    "V5\n",
    "- unigram and trigram di normalize hurufnya\n",
    "- score jika kata ada dalam judul\n",
    "\n",
    "V6\n",
    "- membuat filter Verb dan Noun dari nlp-id, masih ada bug\n",
    "- Minimize user warning\n",
    "- running time 30 data : 66s\n",
    "- Solve bagian preprocessing, untuk menjadi kecil semua\n",
    "- tambahkan progress bar di main process\n",
    "\n",
    "V7\n",
    "- perbaiki bug di fungsi filter_tokens_by_pos, agar tidak ada konversi ke sentence\n",
    "- perbaiki fungsi write excel #blm berfungsi\n",
    "- membuat visualisasi dalam graph\n",
    "\n",
    "V8\n",
    "- perbaikan dalam fungsi 'get_phrase_embedding' agar word dengan vector 0 tidak masuk dalam process selanjutnya\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. rutin2 membuat syspath ke root utk aktifkan __init__.py\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "sys.path.append(repo_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. rutin3 Load the dataset\n",
    "#dataset_path = os.path.join(repo_root, \"notebooks/postager_nlp-id/dataset_ekstraksi_r29_pos_sm.xlsx\")\n",
    "dataset_path = os.path.join(repo_root, \"data/dataset_ekstraksi_r29_sm.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"judul\"] +\". \"+ df[\"isi\"]\n",
    "#df_pos = df['pos_sentence_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df['text'].apply(preprocess)\n",
    "df[\"judul\"] = df[\"judul\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(words, n=2):\n",
    "    \"\"\"Generate ngrams from a list of words.\"\"\"\n",
    "    return [\" \".join(gram) for gram in ngrams(words, n)]\n",
    "\n",
    "def get_phrase_embedding(phrase, w2v_model):\n",
    "    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from collections import Counter\n",
    "#from nlp_id.tokenizer import PhraseTokenizer \n",
    "#from nlp_id.postag import PosTag\n",
    "\n",
    "from tokenizer_local import PhraseTokenizer \n",
    "from postag_local import PosTag\n",
    "\n",
    "\n",
    "model_path = os.path.join(repo_root, \"notebooks/nlp-id_retraining/train_tuned.pkl\") #add_8\n",
    "\n",
    "def detect_bigram(text, available_tokens,):\n",
    "    \n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only bigrams whose individual words are in available_tokens\n",
    "    bigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 1 and all(word in available_tokens for word in phrase.split())]\n",
    "\n",
    "    return bigrams_only\n",
    "\n",
    "def detect_trigram(text, available_tokens):\n",
    "\n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only trigrams whose individual words are in available_tokens\n",
    "    trigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 2 and all(word in available_tokens for word in phrase.split())]\n",
    "\n",
    "    return trigrams_only\n",
    "\n",
    "def get_unique_tokens_pos(all_tokens, model_path):\n",
    "    \"\"\"\n",
    "    Get unique POS tags for tokens.\n",
    "    \"\"\"\n",
    "    postagger = PosTag(model_path)\n",
    "    pos_tokens = []\n",
    "    seen_tokens = set()\n",
    "    \n",
    "    for token in all_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            tokens_pos = postagger.get_phrase_tag(token)\n",
    "            pos_tokens.append(tokens_pos)\n",
    "    return pos_tokens\n",
    "\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists into a single list.\n",
    "    \"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "\n",
    "def filter_tokens_by_pos(flat_tokens, pos_filters):\n",
    "    \"\"\"\n",
    "    Filter tokens based on their POS tags and ensure they're unique.\n",
    "    \"\"\"\n",
    "    seen_tokens = set()\n",
    "    return [token[0] for token in flat_tokens if token[1] in pos_filters and not (token[0] in seen_tokens or seen_tokens.add(token[0]))]\n",
    "\n",
    "def extract_keyphrases_with_ngrams_graph(text, w2v_model, judul, available_tokens, n=3):\n",
    "    # Read stopwords from the file\n",
    "    #stopwords_path = os.path.join(repo_root, \"data/all_stop_words.txt\") \n",
    "    stopwords_path = os.path.join(repo_root, \"notebooks/stopwords_tuning/all_stop_words.txt\")\n",
    "    with open(stopwords_path, 'r') as file:\n",
    "        stopwords = set(file.read().strip().splitlines())\n",
    "\n",
    "    # Tokenize the text into unigrams\n",
    "    #unigrams = [word for word in text.split() if word not in stopwords]\n",
    "\n",
    "    # Tokenize the text into unigrams that are in available_tokens\n",
    "    unigrams = [word for word in text.split() if word not in stopwords and word in available_tokens]\n",
    "\n",
    "    # Generate bigrams and trigrams using nlp-id\n",
    "    bigrams = detect_bigram(text, available_tokens)\n",
    "    trigrams = detect_trigram(text, available_tokens)\n",
    "    \n",
    "    # Combine unigrams, filtered bigrams, and filtered trigrams\n",
    "    all_tokens = unigrams + bigrams + trigrams\n",
    "\n",
    "    # Filter tokens only for selected POS\n",
    "    pos_tokens = get_unique_tokens_pos(all_tokens, model_path)\n",
    "    flat_pos_tokens = flatten_list_of_lists(pos_tokens)\n",
    "    selected_pos = {'NN', 'NNP', 'VB', 'NP', 'VP'} # FW di exclude\n",
    "    filtered_tokens = filter_tokens_by_pos(flat_pos_tokens, selected_pos)\n",
    "\n",
    "    # Get embeddings for each token (averaging word embeddings for bigrams/trigrams)\n",
    "    token_embeddings = [get_phrase_embedding(token, w2v_model) for token in filtered_tokens]\n",
    "    \n",
    "    # Filter out tokens that don't have embeddings\n",
    "    tokens, embeddings = zip(*[(token, emb) for token, emb in zip(filtered_tokens, token_embeddings) if emb is not None])\n",
    "    # todo : masih ada token bahasa asing atau token aneh yg lolos. \n",
    "\n",
    "    # Compute the cosine similarity between token embeddings\n",
    "    cosine_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Create a graph and connect tokens with high similarity\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            if cosine_matrix[i][j] > 0.5:  # This threshold can be adjusted\n",
    "                G.add_edge(tokens[i], tokens[j], weight=cosine_matrix[i][j])\n",
    "    \n",
    "    # Create labels dictionary using the tokens\n",
    "    labels = {token: token for token in tokens}\n",
    "\n",
    "    # Compute the PageRank scores to rank the tokens\n",
    "    scores = nx.pagerank(G)\n",
    "\n",
    "    # Modify scores if token is in title letter\n",
    "    for token in scores:\n",
    "        if any(token in title for title in judul):\n",
    "            scores[token] *= 2\n",
    "\n",
    "    # Extract top N keyphrases along with their scores\n",
    "    ranked_tokens = sorted(((scores[token], token) for token in tokens if token in scores), reverse=True)\n",
    "    \n",
    "    keyphrases_with_scores = []\n",
    "    seen_tokens = set()  # Set to keep track of tokens that have already been added\n",
    "\n",
    "    for score, token in ranked_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            keyphrases_with_scores.append((token, score))\n",
    "            seen_tokens.add(token)  # Mark the token as seen\n",
    "            if len(keyphrases_with_scores) >= n:\n",
    "                break  # Stop when the desired number of keyphrases is reached\n",
    "\n",
    "    return keyphrases_with_scores, G, labels\n",
    "\n",
    "\n",
    "def visualize_graph(G, labels):\n",
    "\n",
    "    # Remove self-loops (edges that connect a node to itself)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\")\n",
    "    nx.draw_networkx_labels(G, pos, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_path = os.path.join(repo_root, \"models/w2v_100/idwiki_word2vec_100_new_lower.model\")\n",
    "w2v_path = os.path.join(repo_root, \"models/w2v_200/idwiki_word2vec_200_new_lower.model\")\n",
    "w2v_model = Word2Vec.load(w2v_path)\n",
    "\n",
    "# Get available tokens from the Word2Vec model\n",
    "available_tokens = set(w2v_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 0...! Done\n",
      "Processing index 1...! Done\n",
      "Processing index 2...! Done\n",
      "Processing index 3...! Done\n",
      "Processing index 4...! Done\n",
      "Processing index 5...! Done\n",
      "Processing index 6...! Done\n",
      "Processing index 7...! Done\n",
      "Processing index 8...! Done\n",
      "Processing index 9...! Done\n",
      "Processing index 10...! Done\n",
      "Processing index 11...! Done\n",
      "Processing index 12...! Done\n",
      "Processing index 13...! Done\n",
      "Processing index 14...! Done\n",
      "Processing index 15...! Done\n",
      "Processing index 16...! Done\n",
      "Processing index 17...! Done\n",
      "Processing index 18...! Done\n",
      "Processing index 19...! Done\n",
      "Processing index 20...! Done\n",
      "Processing index 21...! Done\n",
      "Processing index 22...! Done\n",
      "Processing index 23...! Done\n",
      "Processing index 24...! Done\n",
      "Processing index 25...! Done\n",
      "Processing index 26...! Done\n",
      "Processing index 27...! Done\n",
      "Processing index 28...! Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usulan</td>\n",
       "      <td>personnel</td>\n",
       "      <td>proposed</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>project</td>\n",
       "      <td>facilities</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ruang</td>\n",
       "      <td>usulan</td>\n",
       "      <td>ruangan</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      key_1      key_2       key_3  score_1  score_2  score_3\n",
       "0    usulan  personnel    proposed    0.041    0.030    0.030\n",
       "1  document    project  facilities    0.151    0.133    0.083\n",
       "2     ruang     usulan     ruangan    0.041    0.040    0.035"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_textrank = pd.DataFrame()\n",
    "for i in df.index:\n",
    "    print('Processing index', i, end='...! ')\n",
    "    text = df[\"text\"][i] # sblm di preprocess\n",
    "    #text = df_tr[i] # setelah di preprocess\n",
    "    ls_judul = preprocess(df[\"judul\"][i]).split()\n",
    "    keyphrases,_,_ = extract_keyphrases_with_ngrams_graph(text, w2v_model, ls_judul, available_tokens, 3)\n",
    "    df_keyphrases = pd.DataFrame(keyphrases, columns=['Keyword', 'Score'])\n",
    "    a = pd.DataFrame(df_keyphrases.Keyword).T.reset_index(drop=True)\n",
    "    b = pd.DataFrame(df_keyphrases.Score).round(3).T.reset_index(drop=True)\n",
    "    df_keyphrases = pd.concat([a, b], axis=1)\n",
    "\n",
    "    # Check if there are missing columns and add them with zero values\n",
    "    missing_columns = 6 - df_keyphrases.shape[1]\n",
    "    for _ in range(missing_columns):\n",
    "        df_keyphrases[df_keyphrases.shape[1]] = 0\n",
    "\n",
    "    df_keyphrases.columns = ['key_1', 'key_2','key_3','score_1', 'score_2','score_3']\n",
    "    predict_textrank = pd.concat([predict_textrank, df_keyphrases], ignore_index=True)\n",
    "    print('Done')\n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eval\n",
    "\n",
    "targets = df[[\"k1\", \"k2\", \"k3\",\"k4\", \"k5\", \"k6\",\"k7\"]].values.tolist()\n",
    "df_targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key_1     key_2     key_3  flex_recall  flex_prec\n",
       "0     full_match  no_match  no_match        0.143      0.333\n",
       "1  partial_match  no_match  no_match        0.143      0.333\n",
       "2  partial_match  no_match  no_match        0.143      0.333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation TextRank\n",
    "predict_textrank_list = predict_textrank[['key_1','key_2','key_3']].values.tolist()\n",
    "eval_textrank = eval(predict_textrank_list, targets, True).round(3)\n",
    "eval_textrank.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank = eval_textrank[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "eval_textrank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textrank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textrank\n",
       "recall        0.133\n",
       "precision     0.310\n",
       "F1            0.186"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall = eval_textrank['flex_recall'].mean()\n",
    "textrank_prec = eval_textrank['flex_prec'].mean()\n",
    "textrank_f1 = 2 * (textrank_prec * textrank_recall) / (textrank_prec + textrank_recall)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary = pd.DataFrame({'textrank': [textrank_recall, textrank_prec, textrank_f1]}, index=['recall', 'precision', 'F1'])\n",
    "summary = summary.round(3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usulan</td>\n",
       "      <td>personnel</td>\n",
       "      <td>proposed</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>persetujuan tertulis</td>\n",
       "      <td>prosedur</td>\n",
       "      <td>usulan</td>\n",
       "      <td>pengganti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>full_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>project</td>\n",
       "      <td>facilities</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.083</td>\n",
       "      <td>template document</td>\n",
       "      <td>exhibit c</td>\n",
       "      <td>acuan</td>\n",
       "      <td>pengelolaan</td>\n",
       "      <td>dokumen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ruang</td>\n",
       "      <td>usulan</td>\n",
       "      <td>ruangan</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.035</td>\n",
       "      <td>ruang kantor</td>\n",
       "      <td>change inquiry</td>\n",
       "      <td>lingkup kerja</td>\n",
       "      <td>akomodasi</td>\n",
       "      <td>services for company</td>\n",
       "      <td>exhibit a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      key_1      key_2       key_3  score_1  score_2  score_3  \\\n",
       "0    usulan  personnel    proposed    0.041    0.030    0.030   \n",
       "1  document    project  facilities    0.151    0.133    0.083   \n",
       "2     ruang     usulan     ruangan    0.041    0.040    0.035   \n",
       "\n",
       "                      0               1              2            3  \\\n",
       "0  persetujuan tertulis        prosedur         usulan    pengganti   \n",
       "1     template document       exhibit c          acuan  pengelolaan   \n",
       "2          ruang kantor  change inquiry  lingkup kerja    akomodasi   \n",
       "\n",
       "                      4          5   6          key_1     key_2     key_3  \\\n",
       "0                   NaN        NaN NaN     full_match  no_match  no_match   \n",
       "1               dokumen        NaN NaN  partial_match  no_match  no_match   \n",
       "2  services for company  exhibit a NaN  partial_match  no_match  no_match   \n",
       "\n",
       "   flex_recall  flex_prec  \n",
       "0        0.143      0.333  \n",
       "1        0.143      0.333  \n",
       "2        0.143      0.333  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank = pd.concat([predict_textrank, df_targets, eval_textrank], axis=1)\n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_textrank.to_excel(\"w2v_textrank_ngram_v8.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute 'book'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m sheet_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw2v_tr_phrase\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      5\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw2v_textrank_ngram_v8.xlsx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m write_excel(predict_textrank, sheet_name, output_file)\n",
      "File \u001b[0;32m~/Documents/GitHub/kw_ina_extraction/utils/ia_file_operation.py:16\u001b[0m, in \u001b[0;36mwrite_excel\u001b[0;34m(df, sheet_name, filename)\u001b[0m\n\u001b[1;32m     13\u001b[0m     book \u001b[39m=\u001b[39m Workbook()  \u001b[39m# If the file doesn't exist, create a new workbook\u001b[39;00m\n\u001b[1;32m     15\u001b[0m writer \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mExcelWriter(filename, engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenpyxl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m writer\u001b[39m.\u001b[39;49mbook \u001b[39m=\u001b[39m book\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m sheet_name \u001b[39min\u001b[39;00m book\u001b[39m.\u001b[39msheetnames:  \u001b[39m# If sheet already exists, delete it\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m#idx = book.sheetnames.index(sheet_name)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     sheet \u001b[39m=\u001b[39m book[sheet_name]\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute 'book'"
     ]
    }
   ],
   "source": [
    "# Write predictions to excel file\n",
    "from utils import write_excel\n",
    "\n",
    "sheet_name = 'w2v_tr_phrase'\n",
    "output_file = 'w2v_textrank_ngram_v8.xlsx'\n",
    "write_excel(predict_textrank, sheet_name, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.Graph()\n",
    "labels = {}\n",
    "\n",
    "# sample data untuk digambarkan\n",
    "text = df[\"text\"][10]\n",
    "judul = df[\"judul\"][10]\n",
    "\n",
    "#Add nodes and edges to G and define labels as needed\n",
    "predict_textrank = pd.DataFrame()\n",
    "ls_judul = judul.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases, G, labels = extract_keyphrases_with_ngrams_graph(text, w2v_model, ls_judul, available_tokens, 3)\n",
    "df_keyphrases = pd.DataFrame(keyphrases, columns=['Keyword', 'Score'])\n",
    "a = pd.DataFrame(df_keyphrases.Keyword).T.reset_index(drop=True)\n",
    "b = pd.DataFrame(df_keyphrases.Score).round(3).T.reset_index(drop=True)\n",
    "df_keyphrases = pd.concat([a, b], axis=1)\n",
    "\n",
    "# Check if there are missing columns and add them with zero values\n",
    "missing_columns = 6 - df_keyphrases.shape[1]\n",
    "for _ in range(missing_columns):\n",
    "    df_keyphrases[df_keyphrases.shape[1]] = 0\n",
    "\n",
    "df_keyphrases.columns = ['key_1', 'key_2','key_3','score_1', 'score_2','score_3']\n",
    "predict_textrank = pd.concat([predict_textrank, df_keyphrases], ignore_index=True)\n",
    "\n",
    "# Call the visualize_graph function to display the graph\n",
    "visualize_graph(G, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_list = predict_textrank[['key_1', 'key_2', 'key_3']].values.tolist()[0]\n",
    "top3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph_focus(G, labels, focus_nodes=None):\n",
    "    # Remove self-loops (edges that connect a node to itself)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\")\n",
    "    nx.draw_networkx_labels(G, pos, labels)\n",
    "    \n",
    "    # If focus_nodes are provided, adjust the plot limits to zoom into that region\n",
    "    if focus_nodes:\n",
    "        x_values = [pos[node][0] for node in focus_nodes]\n",
    "        y_values = [pos[node][1] for node in focus_nodes]\n",
    "        margin = 0.1  # You can adjust this value for more or less zoom\n",
    "        plt.xlim(min(x_values) - margin, max(x_values) + margin)\n",
    "        plt.ylim(min(y_values) - margin, max(y_values) + margin)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call the visualize_graph function to display the graph\n",
    "visualize_graph_focus(G, labels, ['kesiapan','meragukan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(G, labels, focus_nodes=None):\n",
    "    # Remove self-loops (edges that connect a node to itself)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\")\n",
    "    nx.draw_networkx_labels(G, pos, labels)\n",
    "    \n",
    "    # If focus_nodes are provided, adjust the plot limits to zoom into that region\n",
    "    if focus_nodes:\n",
    "        x_values = [pos[node][0] for node in focus_nodes]\n",
    "        y_values = [pos[node][1] for node in focus_nodes]\n",
    "        margin = 0.1  # You can adjust this value for more or less zoom\n",
    "        plt.xlim(min(x_values) - margin, max(x_values) + margin)\n",
    "        plt.ylim(min(y_values) - margin, max(y_values) + margin)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melihat list token untuk pengecheckan kenapa ada token bahasa asing masih masuk.\n",
    "with open('available_tokens.txt', 'w') as file:\n",
    "    for token in available_tokens:\n",
    "        file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do next additional : \n",
    "- variasi menggunakan Glove atau vector yg labih tinggi\n",
    "- beberapa kata foreign word seharusnya tidak ada dalam model w2v bahasa indonesia, tetapi masih lolos. untuk di check vector yg zero agar tidak masuk dalam process selanjutnya. modifikasi perlu dilakukan dalam fungsi \"get_phrase_embedding\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_ina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
