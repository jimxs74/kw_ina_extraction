{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitur 132a\n",
    "Token phrase ditambahkan pre token pembentuknya (pembeda dg 131a-colabs)\n",
    "- Fitur 1_phrase_detection = yes, Phrase detection menggunakan nlp-id\n",
    "- Fitur 2_edge_weight = yes, Cooccurences matrix + cosinus similarity (fasttext 200)\n",
    "- Fitur 3_POS filter = yes\n",
    "- fitur 4_posisi_kata = yes, judul * 2\n",
    "- fitur 5_phrase_score = yes, bigram = 2x, trigram = 3x\n",
    "- fitur 6_combined_TFIDF = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. rutin2 membuat syspath ke root utk aktifkan __init__.py\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "sys.path.append(repo_root)\n",
    "\n",
    "#3. rutin3 Load the dataset\n",
    "dataset_path = os.path.join(repo_root, \"data/dataset_ekstraksi_r30_lg.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"judul\"] +\". \"+ df[\"isi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP & PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Preprocess\n",
    "'''\n",
    "stopwords tidak masuk dalam preprocessing\n",
    "'''\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df['text'].apply(preprocess)\n",
    "df[\"judul\"] = df[\"judul\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Load stopword\n",
    "def load_stopwords(repo_root, stopwords_file_path):\n",
    "    \"\"\"\n",
    "    Load stopwords from a file located in the repository.\n",
    "\n",
    "    :param repo_root: Root directory of the repository.\n",
    "    :param stopwords_file_path: Relative path to the stopwords file from the repo_root.\n",
    "    :return: Set of stopwords.\n",
    "    \"\"\"\n",
    "    stopwords_path = os.path.join(repo_root, stopwords_file_path)\n",
    "    with open(stopwords_path, 'r') as file:\n",
    "        stopwords = set(file.read().strip().splitlines())\n",
    "    return stopwords\n",
    "\n",
    "repo_root = repo_root\n",
    "stopwords = load_stopwords(repo_root, \"notebooks/stopwords_tuning/all_stop_words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITUR-1 Phrase Detection\n",
    "\n",
    "Dilakukan sebelum komputasi TextRank\n",
    "detect_all_tokens\n",
    "|--incorporate_bigrams_trigrams(unigrams, bigrams, trigrams)\n",
    "   |--detect_bigram(text):\n",
    "   |--detect_trigram(text):\n",
    "\n",
    "ada 3 pilihan\n",
    "1. frase menggantikan token pembentuknya\n",
    "2. frase ditambahkan pada awal token pembentuk pertama\n",
    "3. frase ditambahkan pada akhir token pembentuk terakhir\n",
    "4. frase ditambahkan 2x pre+post token pembentuk terakhir\n",
    "\n",
    "Pemilihannya see script dibawah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template document jtb gpf project mengacu kepada dokumen jtb cp ctr exhibit coordination procedure kami sampaikan template document yang akan dipergunakan pada proyek jambaran tiung biru jtb gas processing facilities gpf demikian disampaikan sebagai acuan pengelolaan dokumen atas perhatiannya kami ucapkan terima kasih\n",
      "================================================================\n",
      "['template document', 'template', 'document', 'jtb', 'gpf', 'project', 'mengacu', 'kepada', 'dokumen', 'jtb', 'cp', 'ctr', 'exhibit', 'coordination', 'procedure', 'kami', 'sampaikan', 'template document', 'template', 'document', 'yang', 'akan', 'dipergunakan', 'pada', 'proyek', 'jambaran', 'tiung', 'biru', 'jtb', 'gas', 'processing', 'facilities', 'gpf', 'demikian', 'disampaikan', 'sebagai', 'acuan', 'pengelolaan', 'dokumen', 'atas', 'perhatiannya', 'kami', 'ucapkan', 'terima', 'kasih']\n"
     ]
    }
   ],
   "source": [
    "# FITUR-1 Fungsi Phrase Detection\n",
    "\n",
    "#from collections import Counter\n",
    "#from nltk.util import ngrams\n",
    "#from nlp_id_local.tokenizer import PhraseTokenizer \n",
    "#from nlp_id_local.postag import PosTag\n",
    "\n",
    "from nlp_id.tokenizer import PhraseTokenizer \n",
    "from nlp_id.postag import PosTag\n",
    "\n",
    "def detect_bigram(text):\n",
    "    \n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only bigrams whose individual words are in available_tokens\n",
    "    bigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 1]\n",
    "\n",
    "    return bigrams_only\n",
    "\n",
    "def detect_trigram(text):\n",
    "\n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only trigrams whose individual words are in available_tokens\n",
    "    trigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 2 ]\n",
    "\n",
    "    return trigrams_only\n",
    "\n",
    "\"\"\"\n",
    "# Phrase replace constructed Token\n",
    "def incorporate_bigrams_trigrams(unigrams, bigrams, trigrams):\n",
    "    combined_tokens = []\n",
    "    skip = 0\n",
    "\n",
    "    for i in range(len(unigrams)):\n",
    "        if skip > 0:\n",
    "            skip -= 1\n",
    "            continue\n",
    "\n",
    "        bigram_formed = ' '.join(unigrams[i:i+2]) in bigrams\n",
    "        trigram_formed = ' '.join(unigrams[i:i+3]) in trigrams\n",
    "\n",
    "        if bigram_formed:\n",
    "            combined_tokens.append(' '.join(unigrams[i:i+2]))\n",
    "            skip = 1\n",
    "        elif trigram_formed:\n",
    "            combined_tokens.append(' '.join(unigrams[i:i+3]))\n",
    "            skip = 2\n",
    "        else:\n",
    "            combined_tokens.append(unigrams[i])\n",
    "\n",
    "    return combined_tokens\n",
    "\"\"\"\n",
    "\n",
    "# Phrase add pre-constructed Token\n",
    "def incorporate_bigrams_trigrams(unigrams, bigrams, trigrams):\n",
    "    combined_tokens = []\n",
    "\n",
    "    for i in range(len(unigrams)):\n",
    "        bigram_formed = ' '.join(unigrams[i:i+2]) in bigrams\n",
    "        trigram_formed = ' '.join(unigrams[i:i+3]) in trigrams\n",
    "\n",
    "        if trigram_formed:\n",
    "            combined_tokens.append(' '.join(unigrams[i:i+3]))\n",
    "        elif bigram_formed:\n",
    "            combined_tokens.append(' '.join(unigrams[i:i+2]))\n",
    "\n",
    "        combined_tokens.append(unigrams[i])\n",
    "\n",
    "    return combined_tokens\n",
    "\n",
    "\"\"\"\n",
    "# Phrase add twice pre/post-constructed Token\n",
    "def incorporate_bigrams_trigrams(unigrams, bigrams, trigrams):\n",
    "    combined_tokens = []\n",
    "\n",
    "    for i in range(len(unigrams)):\n",
    "        bigram_formed = ' '.join(unigrams[i:i+2]) in bigrams\n",
    "        trigram_formed = ' '.join(unigrams[i:i+3]) in trigrams\n",
    "\n",
    "        if trigram_formed:\n",
    "            phrase = ' '.join(unigrams[i:i+3])\n",
    "            combined_tokens.append(phrase)  # Append before the unigrams\n",
    "            combined_tokens.extend(unigrams[i:i+3])\n",
    "            combined_tokens.append(phrase)  # Append after the unigrams\n",
    "        elif bigram_formed:\n",
    "            phrase = ' '.join(unigrams[i:i+2])\n",
    "            combined_tokens.append(phrase)  # Append before the unigrams\n",
    "            combined_tokens.extend(unigrams[i:i+2])\n",
    "            combined_tokens.append(phrase)  # Append after the unigrams\n",
    "        else:\n",
    "            combined_tokens.append(unigrams[i])\n",
    "\n",
    "    return combined_tokens\n",
    "\"\"\"\n",
    "\n",
    "def detect_all_tokens(text):\n",
    "    unigrams = [word for word in text.split()]\n",
    "    bigrams = detect_bigram(text)\n",
    "    trigrams = detect_trigram(text)\n",
    "    \n",
    "    # Incorporating bigrams and trigrams into the sequence of tokens\n",
    "    all_tokens = incorporate_bigrams_trigrams(unigrams, bigrams, trigrams)\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "# example\n",
    "text = df[\"text\"][1]\n",
    "words = detect_all_tokens(text)\n",
    "print(text)\n",
    "print(\"================================================================\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD EMBEDDING SELECTION\n",
    "ada 2 pilihan dalam vector soze sama = 200\n",
    "1. word2vec\n",
    "2. fasttext\n",
    "\n",
    "pilihan see script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load Word2vec Model\n",
    "w2v_model_path = os.path.join(repo_root, \"models/embedding_model/w2v_wiki_own_phrase_training_200.model\")\n",
    "\n",
    "def load_word2vec_model(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"The provided Word2Vec model path does not exist: {model_path}\")\n",
    "    \n",
    "    w2v_model = Word2Vec.load(model_path) \n",
    "    available_tokens = set(w2v_model.wv.key_to_index)\n",
    "    \n",
    "    return w2v_model, available_tokens\n",
    "    \n",
    "w2v_model, available_tokens = load_word2vec_model(w2v_model_path)\n",
    "\n",
    "# Function to compose vector for a phrase if not defined on the w2v model\n",
    "def get_phrase_embedding(phrase, w2v_model):\n",
    "    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example\n",
    "phrase = \"bekerja sama\"\n",
    "phrase_vector = get_phrase_embedding(phrase, w2v_model)\n",
    "phrase_vector\n",
    "\n",
    "# Cosine similarity word2vec\n",
    "def get_cosine_similarity(w1, w2, w2v_model):\n",
    "    vec1 = get_phrase_embedding(w1, w2v_model)\n",
    "    vec2 = get_phrase_embedding(w2, w2v_model)\n",
    "\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return 0\n",
    "\n",
    "    similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# example, seharusnya dekat hubungan 2 vector ini\n",
    "w1 = 'gotong royong'\n",
    "w2 = 'bekerja sama'\n",
    "w2v_model, available_tokens = load_word2vec_model(w2v_model_path)\n",
    "cos_sim = get_cosine_similarity(w1, w2, w2v_model)\n",
    "cos_sim\n",
    "\n",
    "'''\n",
    "\n",
    "# Path to the FastText model\n",
    "fasttext_model_path = os.path.join(repo_root, \"models/embedding_model/fasttext_ina_200_with_phrases.bin\")\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "def load_fasttext_model(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"The provided FastText model path does not exist: {model_path}\")\n",
    "    \n",
    "    # Use load_facebook_model for full model loading\n",
    "    ft_model = load_facebook_model(model_path)\n",
    "    available_tokens = set(ft_model.wv.key_to_index)\n",
    "    \n",
    "    return ft_model, available_tokens\n",
    "    \n",
    "ft_model, available_tokens = load_fasttext_model(fasttext_model_path)\n",
    "\n",
    "# Check the size of the word vectors\n",
    "vector_size = ft_model.wv.vector_size\n",
    "print(\"The size of the FastText word vectors is:\", vector_size)\n",
    "\n",
    "# Function to compose vector for a phrase if not defined in the FastText model\n",
    "def get_phrase_embedding(phrase, ft_model):\n",
    "    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    embeddings = [ft_model.wv[word] for word in words if word in ft_model.wv.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Example usage\n",
    "phrase = \"bekerja sama\"\n",
    "phrase_vector = get_phrase_embedding(phrase, ft_model)\n",
    "#print(phrase_vector)\n",
    "\n",
    "# Cosine similarity fasttext\n",
    "def get_cosine_similarity(w1, w2, ft_model):\n",
    "    vec1 = get_phrase_embedding(w1, ft_model)\n",
    "    vec2 = get_phrase_embedding(w2, ft_model)\n",
    "\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return 0\n",
    "\n",
    "    similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Example usage with the FastText model\n",
    "w1 = 'gotong royong'\n",
    "w2 = 'bekerja sama'\n",
    "ft_model, available_tokens = load_fasttext_model(fasttext_model_path)\n",
    "cos_sim = get_cosine_similarity(w1, w2, ft_model)\n",
    "#print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITUR-2 dan FUNGSI KOMPUTASI TEXTRANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mengandung stopword: {'template': 0.0421080428605898, 'template document': 0.03960151787052916, 'document': 0.04131462711681653, 'jtb': 0.04895191665665787, 'gpf': 0.037328154194784864, 'project': 0.018234573903987993, 'mengacu': 0.018818659884664517, 'kepada': 0.022595764760747386, 'dokumen': 0.036897513323890886, 'cp': 0.022306826832770847, 'ctr': 0.02463487901139442, 'exhibit': 0.022603521806791767, 'coordination': 0.029099841554282316, 'procedure': 0.025660045177046495, 'kami': 0.043783472954031066, 'sampaikan': 0.019721460849721408, 'yang': 0.026942005814030903, 'akan': 0.03484519004679631, 'dipergunakan': 0.028651558606253402, 'pada': 0.024362089789276273, 'proyek': 0.011888497262118821, 'tiung': 0.017959596975130408, 'biru': 0.020135927145690236, 'gas': 0.022290050038132364, 'processing': 0.029938532091744824, 'facilities': 0.02697690664000319, 'demikian': 0.022758168515149793, 'disampaikan': 0.02906134032888613, 'sebagai': 0.030953231538632595, 'acuan': 0.03004362755749552, 'pengelolaan': 0.026752032802252843, 'atas': 0.025493099193814796, 'perhatiannya': 0.02632635305826673, 'ucapkan': 0.03415113163727836, 'terima': 0.024687506761558266, 'kasih': 0.012122335438780924}\n"
     ]
    }
   ],
   "source": [
    "#text = df[\"text\"][1]\n",
    "#words = detect_all_tokens(text)\n",
    "\n",
    "# FITUR-2 dan FUNGSI KOMPUTASI TEXTRANK\n",
    "\n",
    "# Build Co-occurrence Matrix\n",
    "def build_co_occurrence_matrix(words, window_size=3):\n",
    "    co_occurrence = defaultdict(int)\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        window = words[i:i+window_size]\n",
    "        for j in range(window_size):\n",
    "            for k in range(j+1, window_size):\n",
    "                w1, w2 = sorted([window[j], window[k]])\n",
    "                if w1 != w2:\n",
    "                    co_occurrence[(w1, w2)] += 1\n",
    "    return co_occurrence\n",
    "\n",
    "# Build Graph and Compute TextRank\n",
    "def build_graph_and_compute_textrank(co_occurrence, embedding_model):\n",
    "    G = nx.Graph()\n",
    "    for (w1, w2), weight1 in co_occurrence.items():\n",
    "        weight2 = get_cosine_similarity(w1, w2, embedding_model)\n",
    "        weight3 = weight1 * weight2\n",
    "        if weight2 > 0:\n",
    "            G.add_edge(w1, w2, weight=weight3)\n",
    "    return nx.pagerank(G)\n",
    "\n",
    "\n",
    "# Example\n",
    "co_occurrence = build_co_occurrence_matrix(words)\n",
    "#co_occurrence\n",
    "\n",
    "#scores = build_graph_and_compute_textrank(co_occurrence, w2v_model)\n",
    "scores = build_graph_and_compute_textrank(co_occurrence, ft_model)\n",
    "\n",
    "print(\"mengandung stopword:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template': 0.0421080428605898,\n",
       " 'template document': 0.03960151787052916,\n",
       " 'document': 0.04131462711681653,\n",
       " 'project': 0.018234573903987993,\n",
       " 'mengacu': 0.018818659884664517,\n",
       " 'ctr': 0.02463487901139442,\n",
       " 'exhibit': 0.022603521806791767,\n",
       " 'coordination': 0.029099841554282316,\n",
       " 'procedure': 0.025660045177046495,\n",
       " 'tiung': 0.017959596975130408,\n",
       " 'biru': 0.020135927145690236,\n",
       " 'processing': 0.029938532091744824,\n",
       " 'facilities': 0.02697690664000319,\n",
       " 'acuan': 0.03004362755749552,\n",
       " 'pengelolaan': 0.026752032802252843,\n",
       " 'perhatiannya': 0.02632635305826673}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning stop word for only unigram\n",
    "def filter_stopwords(scores, stopwords):\n",
    "    \"\"\"\n",
    "    Filters out stopwords from the scores dictionary only for unigrams. \n",
    "    \"\"\"\n",
    "    filtered_scores = {}\n",
    "    for term, score in scores.items():\n",
    "        words = term.split()\n",
    "        # Filter out the term if it is a unigram and a stopword\n",
    "        if len(words) == 1 and words[0] in stopwords:\n",
    "            continue\n",
    "        # Include the term otherwise\n",
    "        filtered_scores[term] = score\n",
    "    return filtered_scores\n",
    "\n",
    "scores_wo_stopword = filter_stopwords(scores, stopwords)\n",
    "scores_wo_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('template', 0.0421080428605898, 'FW'), ('template document', 0.03960151787052916, 'NP'), ('document', 0.04131462711681653, 'NNP'), ('project', 0.018234573903987993, 'NN'), ('mengacu', 0.018818659884664517, 'VB'), ('ctr', 0.02463487901139442, 'NN'), ('exhibit', 0.022603521806791767, 'NN'), ('coordination', 0.029099841554282316, 'FW'), ('procedure', 0.025660045177046495, 'NN'), ('tiung', 0.017959596975130408, 'NN'), ('processing', 0.029938532091744824, 'NN'), ('facilities', 0.02697690664000319, 'NN'), ('acuan', 0.03004362755749552, 'NN'), ('pengelolaan', 0.026752032802252843, 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# FITUR 3_POS_filter\n",
    "\n",
    "def postag_tokens(tokens):\n",
    "    postagger = PosTag()\n",
    "    pos_tags = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if ' ' in token:\n",
    "            # Process the token as a phrase\n",
    "            phrase_tags = postagger.get_phrase_tag(token)\n",
    "            # Ensure phrase_tags are in the format [(word, tag), ...]\n",
    "            pos_tags.extend(phrase_tags)\n",
    "        else:\n",
    "            # Process the token as a single word\n",
    "            word_tag = postagger.get_pos_tag(token)\n",
    "            # Ensure word_tag is in the format [(word, tag)]\n",
    "            if word_tag:\n",
    "                pos_tags.extend(word_tag)\n",
    "\n",
    "    return pos_tags\n",
    "\n",
    "def attach_pos_tags_with_scores(scores, pos_tags):\n",
    "    # Create a dictionary from POS tags list\n",
    "    pos_dict = {word: tag for word, tag in pos_tags}\n",
    "    \n",
    "    # Attach POS tags to keywords with scores\n",
    "    return [(word, score, pos_dict.get(word, 'UNK')) for word, score in scores.items()]\n",
    "\n",
    "def filter_by_pos(keywords_with_pos, selected_pos):\n",
    "    return [item for item in keywords_with_pos if item[2] in selected_pos]\n",
    "\n",
    "# Example usage\n",
    "\n",
    "def process_scores_with_pos_tags(scores_wo_stopword, selected_pos_tags):\n",
    "    # Get POS tags for all unique words in the scores dictionary\n",
    "    unique_words = list(set([word for word in scores_wo_stopword.keys()]))\n",
    "    pos_tags = postag_tokens(unique_words)\n",
    "\n",
    "    # Attach POS tags to the scores\n",
    "    keywords_with_pos = attach_pos_tags_with_scores(scores_wo_stopword, pos_tags)\n",
    "\n",
    "    # Filter by Selected POS Tags\n",
    "    scores_w_F3 = filter_by_pos(keywords_with_pos, selected_pos_tags)\n",
    "    \n",
    "    return scores_w_F3\n",
    "\n",
    "# Example usage\n",
    "selected_pos_tags = {'NN', 'NNP', 'VB', 'NP', 'VP', 'FW'}\n",
    "scores_w_F123 = process_scores_with_pos_tags(scores_wo_stopword, selected_pos_tags)\n",
    "print(scores_w_F123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_w_F1234 : [('template', 0.0842160857211796, 'FW'), ('template document', 0.07920303574105832, 'NP'), ('document', 0.08262925423363306, 'NNP'), ('project', 0.036469147807975985, 'NN'), ('mengacu', 0.018818659884664517, 'VB'), ('ctr', 0.02463487901139442, 'NN'), ('exhibit', 0.022603521806791767, 'NN'), ('coordination', 0.029099841554282316, 'FW'), ('procedure', 0.025660045177046495, 'NN'), ('tiung', 0.017959596975130408, 'NN'), ('processing', 0.029938532091744824, 'NN'), ('facilities', 0.02697690664000319, 'NN'), ('acuan', 0.03004362755749552, 'NN'), ('pengelolaan', 0.026752032802252843, 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Fitur 4_posisi_kata = yes, judul * 2\n",
    "# baru bisa di lakukan setelah textRank\n",
    "\n",
    "def adjust_scores_for_title(scores_w_F3, title, title_score=2):\n",
    "    # Split the title into individual words for comparison\n",
    "    title_words = set(title.split())\n",
    "\n",
    "    # Create a new list to store adjusted scores\n",
    "    adjusted_scores = []\n",
    "\n",
    "    for token, score, pos in scores_w_F3:\n",
    "        # Check if the token (or any word in a phrase) is in the title\n",
    "        if any(word in title_words for word in token.lower().split()):\n",
    "            adjusted_score = score * title_score\n",
    "        else:\n",
    "            adjusted_score = score\n",
    "\n",
    "        # Append the adjusted score tuple to the new list\n",
    "        adjusted_scores.append((token, adjusted_score, pos))\n",
    "\n",
    "    return adjusted_scores\n",
    "\n",
    "# Example Usage\n",
    "title = df[\"judul\"][1]# Replace with actual title\n",
    "#scores_w_F3 = [('template', 0.0421080428605898, 'FW'), ('template document', 0.03960151787052916, 'NP'), ...]  # truncated for brevity\n",
    "scores_w_F1234 = adjust_scores_for_title(scores_w_F123, title)\n",
    "\n",
    "print(\"scores_w_F1234 :\", scores_w_F1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_w_F12345 : [('template document', 0.15840607148211663, 'NP'), ('template', 0.0842160857211796, 'FW'), ('document', 0.08262925423363306, 'NNP'), ('project', 0.036469147807975985, 'NN'), ('acuan', 0.03004362755749552, 'NN'), ('processing', 0.029938532091744824, 'NN'), ('coordination', 0.029099841554282316, 'FW'), ('facilities', 0.02697690664000319, 'NN'), ('pengelolaan', 0.026752032802252843, 'NN'), ('procedure', 0.025660045177046495, 'NN'), ('ctr', 0.02463487901139442, 'NN'), ('exhibit', 0.022603521806791767, 'NN'), ('mengacu', 0.018818659884664517, 'VB'), ('tiung', 0.017959596975130408, 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def filter_and_rank_keywords(scores_w_F1234, bigram_weight=2, trigram_weight=3):\n",
    "    adjusted_scores = []\n",
    "\n",
    "    for token, score, pos in scores_w_F1234:\n",
    "        token_count = len(token.split())\n",
    "\n",
    "        # Adjust the score based on the number of words in the token\n",
    "        if token_count == 2:  # Bigram\n",
    "            adjusted_score = score * bigram_weight\n",
    "        elif token_count == 3:  # Trigram\n",
    "            adjusted_score = score * trigram_weight\n",
    "        else:  # Unigram or more than 3 words\n",
    "            adjusted_score = score\n",
    "\n",
    "        adjusted_scores.append((token, adjusted_score, pos))\n",
    "\n",
    "    # Sort the words based on adjusted scores\n",
    "    adjusted_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return adjusted_scores\n",
    "\n",
    "# Example Usage\n",
    "scores_w_F12345 = filter_and_rank_keywords(scores_w_F1234, bigram_weight=2, trigram_weight=3)\n",
    "\n",
    "print(\"scores_w_F12345 :\", scores_w_F12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitur 6_combined_TFIDF \n",
    "# Compute TFIDF\n",
    "# Determine document = \n",
    "# Determine corpus = \n",
    "# determine TFIDF score for only word on previous list = scores_w_F12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitur 6_combined_TFIDF = no\n",
    "# baru bisa di lakukan setelah textRank\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_ngram_type(token):\n",
    "    return len(token.split())\n",
    "\n",
    "def compute_and_adjust_tfidf_scores(words, scores):\n",
    "    \"\"\"\n",
    "    Computes TF-IDF scores for the given words and adjusts the TextRank scores based on n-gram type and TF-IDF scores.\n",
    "\n",
    "    Parameters:\n",
    "    - words: List of words from the text.\n",
    "    - scores: Dictionary of TextRank scores.\n",
    "\n",
    "    Returns:\n",
    "    - Adjusted TextRank scores.\n",
    "    \"\"\"\n",
    "    # Compute TF-IDF scores\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(words)])\n",
    "    tfidf_scores = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).A1))\n",
    "    \n",
    "    # Modify scores based on n-gram type and TF-IDF scores\n",
    "    for token in scores:\n",
    "        ngram_type = get_ngram_type(token)\n",
    "        tfidf_score = tfidf_scores.get(token, 1)  # default to 1 if token not in TF-IDF scores\n",
    "        if ngram_type == 1:  # Unigram\n",
    "            scores[token] *= tfidf_score\n",
    "        elif ngram_type == 2:  # Bigram\n",
    "            scores[token] *= 2 * tfidf_score\n",
    "        elif ngram_type == 3:  # Trigram\n",
    "            scores[token] *= 2 * tfidf_score\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN PROCESS ITERATION FOR ALL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_keyphrases_dataframe(keyphrases_scores):\n",
    "    \"\"\"\n",
    "    Formats the keyphrases and their scores into a structured DataFrame.\n",
    "    Parameters:\n",
    "    - keyphrases_scores: List of tuples containing keywords, scores, and POS tags.\n",
    "    \"\"\"\n",
    "    df_keyphrases = pd.DataFrame(keyphrases_scores, columns=['Keyword', 'Score', 'pos'])\n",
    "    \n",
    "    # Transpose and reindex the DataFrame to ensure consistent structure\n",
    "    a = pd.DataFrame(df_keyphrases.Keyword).T.reset_index(drop=True)\n",
    "    a = a.reindex(columns=range(10), fill_value=0)\n",
    "    b = pd.DataFrame(df_keyphrases.Score).round(3).T.reset_index(drop=True)\n",
    "    b = b.reindex(columns=range(10), fill_value=0)\n",
    "    c = pd.DataFrame(df_keyphrases.pos).T.reset_index(drop=True)\n",
    "    c = c.reindex(columns=range(10), fill_value=0)\n",
    "\n",
    "    df_keyphrases = pd.concat([a, b, c], axis=1)\n",
    "\n",
    "    # Ensure the DataFrame has 30 columns, adding missing ones with zero values\n",
    "    missing_columns = 30 - df_keyphrases.shape[1]\n",
    "    for _ in range(missing_columns):\n",
    "        df_keyphrases[df_keyphrases.shape[1]] = 0\n",
    "\n",
    "    # Naming the columns\n",
    "    df_keyphrases.columns = ['key_1', 'key_2', 'key_3', 'key_4', 'key_5', 'key_6', 'key_7', 'key_8', 'key_9', 'key_10',\n",
    "                             'score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'score_6', 'score_7', 'score_8', 'score_9', 'score_10',\n",
    "                             'pos_1', 'pos_2', 'pos_3', 'pos_4', 'pos_5', 'pos_6', 'pos_7', 'pos_8', 'pos_9', 'pos_10']\n",
    "\n",
    "    return df_keyphrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_textrank = pd.DataFrame()\n",
    "predict_textrank = pd.DataFrame()\n",
    "#for i in df.index:\n",
    "for i in df.loc[2:5].index:    \n",
    "    print('Processing index', i, end='...! ')\n",
    "    # Processing Text\n",
    "    text = df[\"text\"][i]\n",
    "    words = detect_all_tokens(text)\n",
    "    # Postagging\n",
    "    pos_tokens = postag_tokens(words)\n",
    "    # Building Co-occurrence Matrix\n",
    "    co_occurrence = build_co_occurrence_matrix(words)\n",
    "    # Building Graph and Computing TextRank\n",
    "    #scores = build_graph_and_compute_textrank(co_occurrence, w2v_model)\n",
    "    scores = build_graph_and_compute_textrank(co_occurrence, ft_model)\n",
    "\n",
    "    # FITUR-5 phrase_phrase_weight\n",
    "    # Filtering and Ranking Keywords\n",
    "    scores_w_F5 = filter_and_rank_keywords(scores, stopwords, num_keywords=10, bigram_weight=2, trigram_weight=3)\n",
    "    #scores_wo_F5_phrase_weight = filter_and_rank_keywords(scores, stopwords, num_keywords=10)\n",
    "\n",
    "    #FITUR-3 POS FILTER\n",
    "    # Attaching POS Tags\n",
    "    keywords_with_pos = attach_pos_tags(scores_w_F5, pos_tokens)\n",
    "    # Filtering by Selected POS Tags\n",
    "    selected_pos_tags = {'NN', 'NNP', 'VB', 'NP', 'VP', 'FW'}\n",
    "    scores_w_F35 = filter_by_pos(keywords_with_pos, selected_pos_tags)\n",
    "\n",
    "    #FITUR-4 WEIGHT WORD POSITION\n",
    "    title = df[\"judul\"]\n",
    "    scores_w_F345 = adjust_scores_for_title(scores_w_F35, title)\n",
    "\n",
    "    #FITUR-6 combined_TFIDF\n",
    "    #words = detect_all_tokens(text)\n",
    "    #scores_w_F3456 = compute_and_adjust_tfidf_scores(words, scores_w_F345)\n",
    "\n",
    "    df_keyphrases = format_keyphrases_dataframe(scores_w_F345)\n",
    "\n",
    "    predict_textrank = pd.concat([predict_textrank, df_keyphrases], ignore_index=True)\n",
    "    print('Done')\n",
    "    \n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 2...! Done\n",
      "Processing index 3...! "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Processing index 4...! Done\n",
      "Processing index 5...! Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>key_4</th>\n",
       "      <th>key_5</th>\n",
       "      <th>key_6</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_1</th>\n",
       "      <th>pos_2</th>\n",
       "      <th>pos_3</th>\n",
       "      <th>pos_4</th>\n",
       "      <th>pos_5</th>\n",
       "      <th>pos_6</th>\n",
       "      <th>pos_7</th>\n",
       "      <th>pos_8</th>\n",
       "      <th>pos_9</th>\n",
       "      <th>pos_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>change inquiry</td>\n",
       "      <td>kantor</td>\n",
       "      <td>ruangan</td>\n",
       "      <td>usulan</td>\n",
       "      <td>tidak mengalami</td>\n",
       "      <td>lapangan</td>\n",
       "      <td>appendix</td>\n",
       "      <td>lokasi</td>\n",
       "      <td>artikel</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NP</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>VP</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>counter part</td>\n",
       "      <td>counter</td>\n",
       "      <td>engineering</td>\n",
       "      <td>instrument</td>\n",
       "      <td>pengadaan</td>\n",
       "      <td>lead</td>\n",
       "      <td>control</td>\n",
       "      <td>menanggapi</td>\n",
       "      <td>konstruksi</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NP</td>\n",
       "      <td>FW</td>\n",
       "      <td>FW</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>FW</td>\n",
       "      <td>VP</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>site survey</td>\n",
       "      <td>survey</td>\n",
       "      <td>lapangan</td>\n",
       "      <td>iwan</td>\n",
       "      <td>manager</td>\n",
       "      <td>hamzah</td>\n",
       "      <td>tanggai</td>\n",
       "      <td>construction</td>\n",
       "      <td>kunjungan</td>\n",
       "      <td>berkoordinasi</td>\n",
       "      <td>...</td>\n",
       "      <td>NP</td>\n",
       "      <td>FW</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>VP</td>\n",
       "      <td>FW</td>\n",
       "      <td>NN</td>\n",
       "      <td>VP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            key_1    key_2        key_3       key_4            key_5  \\\n",
       "0  change inquiry   kantor      ruangan      usulan  tidak mengalami   \n",
       "1    counter part  counter  engineering  instrument        pengadaan   \n",
       "2     site survey   survey     lapangan        iwan          manager   \n",
       "\n",
       "      key_6     key_7         key_8       key_9         key_10  ...  pos_1  \\\n",
       "0  lapangan  appendix        lokasi     artikel              0  ...     NP   \n",
       "1      lead   control    menanggapi  konstruksi              0  ...     NP   \n",
       "2    hamzah   tanggai  construction   kunjungan  berkoordinasi  ...     NP   \n",
       "\n",
       "   pos_2  pos_3  pos_4  pos_5  pos_6  pos_7  pos_8  pos_9  pos_10  \n",
       "0     NN     NN     NN     VP     NN     NN     NN     NN       0  \n",
       "1     FW     FW     NN     NN     NN     FW     VP     NN       0  \n",
       "2     FW     NN     NN     NN     NN     VP     FW     NN      VP  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Word2Vec Model\n",
    "#w2v_model_path = os.path.join(repo_root, \"models/embedding_model/w2v_wiki_own_phrase_training_200.model\")\n",
    "#w2v_model, available_tokens = load_word2vec_model(w2v_model_path)\n",
    "\n",
    "# Load Fasttext Model\n",
    "#fasttext_model_path = os.path.join(repo_root, \"models/embedding_model/fasttext_ina_200_with_phrases.bin\") # sudah dipanggil diatas\n",
    "#ft_model, available_tokens = load_fasttext_model(fasttext_model_path) # udah dipanggil diatas\n",
    "\n",
    "predict_textrank = pd.DataFrame()\n",
    "#for i in df.index:\n",
    "for i in df.loc[2:5].index:    \n",
    "    print('Processing index', i, end='...! ')\n",
    "    # Processing Text\n",
    "    text = df[\"text\"][i]\n",
    "    words = detect_all_tokens(text)\n",
    "    # Postagging\n",
    "    pos_tokens = postag_tokens(words)\n",
    "    # Building Co-occurrence Matrix\n",
    "    co_occurrence = build_co_occurrence_matrix(words)\n",
    "    # Building Graph and Computing TextRank\n",
    "    #scores = build_graph_and_compute_textrank(co_occurrence, w2v_model)\n",
    "    scores = build_graph_and_compute_textrank(co_occurrence, ft_model)\n",
    "\n",
    "    # FITUR-5 phrase_phrase_weight\n",
    "    # Filtering and Ranking Keywords\n",
    "    scores_w_F5 = filter_and_rank_keywords(scores, stopwords, num_keywords=10, bigram_weight=2, trigram_weight=3)\n",
    "    #scores_wo_F5_phrase_weight = filter_and_rank_keywords(scores, stopwords, num_keywords=10)\n",
    "\n",
    "    #FITUR-3 POS FILTER\n",
    "    # Attaching POS Tags\n",
    "    keywords_with_pos = attach_pos_tags(scores_w_F5, pos_tokens)\n",
    "    # Filtering by Selected POS Tags\n",
    "    selected_pos_tags = {'NN', 'NNP', 'VB', 'NP', 'VP', 'FW'}\n",
    "    scores_w_F35 = filter_by_pos(keywords_with_pos, selected_pos_tags)\n",
    "\n",
    "    #FITUR-4 WEIGHT WORD POSITION\n",
    "    title = df[\"judul\"]\n",
    "    scores_w_F345 = adjust_scores_for_title(scores_w_F35, title)\n",
    "\n",
    "    #FITUR-6 combined_TFIDF\n",
    "    #words = detect_all_tokens(text)\n",
    "    #scores_w_F3456 = compute_and_adjust_tfidf_scores(words, scores_w_F345)\n",
    "\n",
    "    df_keyphrases = format_keyphrases_dataframe(scores_w_F345)\n",
    "\n",
    "    \n",
    "    # existing\n",
    "    #df_keyphrases = pd.DataFrame(scores_w_F3456, columns=['Keyword', 'Score', 'pos'])\n",
    "    #a = pd.DataFrame(df_keyphrases.Keyword).T.reset_index(drop=True)\n",
    "    #a = a.reindex(columns=range(10), fill_value=0)\n",
    "    #b = pd.DataFrame(df_keyphrases.Score).round(3).T.reset_index(drop=True)\n",
    "    #b = b.reindex(columns=range(10), fill_value=0)\n",
    "    #c = pd.DataFrame(df_keyphrases.pos).round(3).T.reset_index(drop=True)\n",
    "    #c = c.reindex(columns=range(10), fill_value=0)\n",
    "    #df_keyphrases = pd.concat([a, b, c], axis=1)\n",
    "\n",
    "    # Check if there are missing columns and add them with zero values\n",
    "    #missing_columns = 30 - df_keyphrases.shape[1]\n",
    "    #for _ in range(missing_columns):\n",
    "    #    df_keyphrases[df_keyphrases.shape[1]] = 0\n",
    "\n",
    "    #df_keyphrases.columns = ['key_1', 'key_2','key_3', 'key_4', 'key_5','key_6', 'key_7', 'key_8','key_9','key_10',\n",
    "    #                         'score_1', 'score_2','score_3','score_4', 'score_5','score_6','score_7', 'score_8','score_9','score_10',\n",
    "    #                         'pos_1', 'pos_2','pos_3', 'pos_4', 'pos_5','pos_6', 'pos_7', 'pos_8','pos_9','pos_10']\n",
    "    \n",
    "\n",
    "    # Adding the formatted DataFrame to the cumulative DataFrame\n",
    "    predict_textrank = pd.concat([predict_textrank, df_keyphrases], ignore_index=True)\n",
    "    print('Done')\n",
    "    \n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(preds_row, targets_row):\n",
    "    result = []\n",
    "    strict_accum = 0\n",
    "    strict_prec = 0\n",
    "    flex_accum = 0\n",
    "    flex_prec = 0\n",
    "\n",
    "    for pred in preds_row:\n",
    "        pred = str(pred).lower().strip()\n",
    "        state = 'no_match'\n",
    "\n",
    "        for target in targets_row:\n",
    "            target = str(target).lower().strip()\n",
    "\n",
    "            if pred == target:\n",
    "                state = 'full_match'\n",
    "                flex_accum += 1\n",
    "                strict_accum += 1\n",
    "                break\n",
    "            elif pred in target:\n",
    "                state = 'partial_match'\n",
    "                flex_accum += 1\n",
    "                strict_accum += 0.5\n",
    "                break\n",
    "            # perlu ditambahkan jika pred ada terdiri dari bbrp kata, dan katanya berisisan dengan target, maka menjadi partial match\n",
    "            '''\n",
    "            elif target in pred:\n",
    "                state = 'partial_match'\n",
    "                flex_accum += 1\n",
    "                strict_accum += 0.5\n",
    "            '''\n",
    "        result.append(state)\n",
    "\n",
    "    len_targets_row = len(targets_row)\n",
    "    len_preds_row = len(preds_row)\n",
    "\n",
    "    if len_preds_row != 0:\n",
    "        flex_recall = flex_accum / len_targets_row\n",
    "        flex_prec = flex_accum / len_preds_row\n",
    "\n",
    "    if len_targets_row != 0:\n",
    "        strict_recall = strict_accum / len_targets_row\n",
    "        strict_prec = strict_accum / len_preds_row\n",
    "\n",
    "    result.extend([strict_recall, strict_prec, flex_recall, flex_prec])\n",
    "\n",
    "    return result\n",
    "\n",
    "def eval(predictions, targets, to_dataframe = False):\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "      result.append(check_similarity(predictions[i], targets[i]))\n",
    "    \n",
    "    if to_dataframe:\n",
    "        return pd.DataFrame(result)\n",
    "    else:\n",
    "        return result\n",
    "    \n",
    "def evaluate_prediction(predict_list, targets_list):\n",
    "    '''\n",
    "    fungsi untuk membandingkan hasil prediksi dengan gold truth\n",
    "    dengan hasil exact match =1, partial match = 1, no match = 0, utk kemudian dihitung per row\n",
    "    '''\n",
    "    evaluation = eval(predict_list, targets_list, True).round(3)\n",
    "    evaluation.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "    evaluation = evaluation[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']]\n",
    "    return evaluation\n",
    "\n",
    "def summarize_evaluation(eval_method):\n",
    "    '''\n",
    "    fungsi untuk menghitung score metric recall, precision, dan F1\n",
    "    dengan paramater flexible : exact match =1, partial match = 1, no match = 0\n",
    "    hasil dalam bentuk dataframe summary\n",
    "    '''\n",
    "    recall = eval_method['flex_recall'].mean()\n",
    "    prec = eval_method['flex_prec'].mean()\n",
    "    f1 = 2 * (prec * recall) / (prec + recall)\n",
    "\n",
    "    summary = pd.DataFrame({'metric': [recall, prec, f1]}, index=['recall', 'precision', 'F1'])\n",
    "    summary = summary.round(3)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import eval\n",
    "targets = df[[\"k1\", \"k2\", \"k3\",\"k4\", \"k5\", \"k6\",\"k7\"]].values.tolist()\n",
    "df_targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation TextRank top 10\n",
    "predict_textrank_list_10 = predict_textrank[['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10']].values.tolist()\n",
    "eval_textrank_10 = eval(predict_textrank_list_10, targets, True).round(3)\n",
    "eval_textrank_10.columns = ['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_10 = eval_textrank_10[['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_10.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_10 = eval_textrank_10['flex_recall'].mean()\n",
    "textrank_prec_10 = eval_textrank_10['flex_prec'].mean()\n",
    "textrank_f1_10 = 2 * (textrank_prec_10 * textrank_recall_10) / (textrank_prec_10 + textrank_recall_10)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_10 = pd.DataFrame({'textrank': [textrank_recall_10, textrank_prec_10, textrank_f1_10]}, index=['recall', 'precision', 'F1'])\n",
    "summary_10 = summary_10.round(3)\n",
    "#summary_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textrank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textrank\n",
       "recall        0.174\n",
       "precision     0.243\n",
       "F1            0.202"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation TextRank top 5\n",
    "predict_textrank_list_5 = predict_textrank[['key_1','key_2','key_3', 'key_4','key_5']].values.tolist()\n",
    "eval_textrank_5 = eval(predict_textrank_list_5, targets, True).round(3)\n",
    "eval_textrank_5.columns = ['key_1','key_2','key_3', 'key_4','key_5','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_5 = eval_textrank_5[['key_1','key_2','key_3', 'key_4','key_5', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_5.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_5 = eval_textrank_5['flex_recall'].mean()\n",
    "textrank_prec_5 = eval_textrank_5['flex_prec'].mean()\n",
    "textrank_f1_5 = 2 * (textrank_prec_5 * textrank_recall_5) / (textrank_prec_5 + textrank_recall_5)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_5 = pd.DataFrame({'textrank': [textrank_recall_5, textrank_prec_5, textrank_f1_5]}, index=['recall', 'precision', 'F1'])\n",
    "summary_5 = summary_5.round(3)\n",
    "summary_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textrank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textrank\n",
       "recall        0.125\n",
       "precision     0.292\n",
       "F1            0.175"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation TextRank top 3\n",
    "predict_textrank_list_3 = predict_textrank[['key_1','key_2','key_3']].values.tolist()\n",
    "eval_textrank_3 = eval(predict_textrank_list_3, targets, True).round(3)\n",
    "eval_textrank_3.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_3 = eval_textrank_3[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_3.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_3 = eval_textrank_3['flex_recall'].mean()\n",
    "textrank_prec_3 = eval_textrank_3['flex_prec'].mean()\n",
    "textrank_f1_3 = 2 * (textrank_prec_3 * textrank_recall_3) / (textrank_prec_3 + textrank_recall_3)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_3 = pd.DataFrame({'textrank': [textrank_recall_3, textrank_prec_3, textrank_f1_3]}, index=['recall', 'precision', 'F1'])\n",
    "summary_3 = summary_3.round(3)\n",
    "summary_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>key_4</th>\n",
       "      <th>key_5</th>\n",
       "      <th>key_6</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>...</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>personil penting</td>\n",
       "      <td>personil</td>\n",
       "      <td>organisasi</td>\n",
       "      <td>fase</td>\n",
       "      <td>tender</td>\n",
       "      <td>pengganti</td>\n",
       "      <td>usulan</td>\n",
       "      <td>email tersendiri</td>\n",
       "      <td>usulan personil penting</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>usulan</td>\n",
       "      <td>pengganti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>acuan</td>\n",
       "      <td>template document</td>\n",
       "      <td>tiung</td>\n",
       "      <td>ctr</td>\n",
       "      <td>exhibit</td>\n",
       "      <td>processing</td>\n",
       "      <td>facilities</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>acuan</td>\n",
       "      <td>pengelolaan</td>\n",
       "      <td>dokumen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>0.429</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kantor</td>\n",
       "      <td>ruangan</td>\n",
       "      <td>usulan</td>\n",
       "      <td>change inquiry</td>\n",
       "      <td>artikel</td>\n",
       "      <td>lokasi</td>\n",
       "      <td>accommodation</td>\n",
       "      <td>klarifikasi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>lingkup kerja</td>\n",
       "      <td>akomodasi</td>\n",
       "      <td>services for company</td>\n",
       "      <td>exhibit a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              key_1     key_2              key_3           key_4    key_5  \\\n",
       "0  personil penting  personil         organisasi            fase   tender   \n",
       "1          document     acuan  template document           tiung      ctr   \n",
       "2            kantor   ruangan             usulan  change inquiry  artikel   \n",
       "\n",
       "       key_6          key_7             key_8                    key_9 key_10  \\\n",
       "0  pengganti         usulan  email tersendiri  usulan personil penting      0   \n",
       "1    exhibit     processing        facilities                        0      0   \n",
       "2     lokasi  accommodation       klarifikasi                        0      0   \n",
       "\n",
       "   ...              2            3                     4          5    6  \\\n",
       "0  ...         usulan    pengganti                   NaN        NaN  NaN   \n",
       "1  ...          acuan  pengelolaan               dokumen        NaN  NaN   \n",
       "2  ...  lingkup kerja    akomodasi  services for company  exhibit a  NaN   \n",
       "\n",
       "           key_1       key_2       key_3  flex_recall  flex_prec  \n",
       "0       no_match    no_match    no_match        0.000      0.000  \n",
       "1  partial_match  full_match  full_match        0.429      1.000  \n",
       "2  partial_match    no_match    no_match        0.143      0.333  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_10 = pd.concat([predict_textrank, df_targets, eval_textrank_10], axis=1)\n",
    "#predict_textrank_10.head(3)\n",
    "\n",
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_5 = pd.concat([predict_textrank, df_targets, eval_textrank_5], axis=1)\n",
    "#predict_textrank_5.head(3)\n",
    "\n",
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_3 = pd.concat([predict_textrank, df_targets, eval_textrank_3], axis=1)\n",
    "predict_textrank_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def write_excel(df, sheet_name, filename):\n",
    "    \"\"\"\n",
    "    Writes the given dataframe to an excel file with the given filename and sheet name.\n",
    "    If the sheet already exists in the file, the data in the sheet will be overwritten.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to load the existing workbook\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl', mode='a') as writer:\n",
    "            if sheet_name in writer.book.sheetnames:\n",
    "                # If sheet already exists, remove it\n",
    "                sheet = writer.book[sheet_name]\n",
    "                writer.book.remove(sheet)\n",
    "\n",
    "            # Write the dataframe to the excel file\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If the file doesn't exist, create a new workbook\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl', mode='w') as writer:\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to excel file\n",
    "#from utils import write_excel\n",
    "\n",
    "sheet_name_10 = 'SE132a_adjusted_textrank_10'\n",
    "sheet_name_5 = 'SE132a_adjusted_textrank_5'\n",
    "sheet_name_3 = 'SE132a_adjusted_textrank_3'\n",
    "\n",
    "output_file = 'SE132a_adjusted_textrank.xlsx'\n",
    "write_excel(predict_textrank_10, sheet_name_10, output_file)\n",
    "write_excel(predict_textrank_5, sheet_name_5, output_file)\n",
    "write_excel(predict_textrank_3, sheet_name_3, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDITIONAL UTILITIES \n",
    "pilihan: \n",
    "- menunjukan gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "'''\n",
    "def get_unique_tokens_pos(all_tokens, pos_model_path):\n",
    "    \"\"\"\n",
    "    Get unique POS tags for tokens.\n",
    "    \"\"\"\n",
    "    pos_model_path = os.path.join(repo_root, \"notebooks/nlp-id_retraining/train_tuned.pkl\")\n",
    "    postagger = PosTag(pos_model_path)\n",
    "    pos_tokens = []\n",
    "    seen_tokens = set()\n",
    "    \n",
    "    for token in all_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            tokens_pos = postagger.get_phrase_tag(token)\n",
    "            pos_tokens.append(tokens_pos)\n",
    "    return pos_tokens\n",
    "'''\n",
    "'''\n",
    "# Fungsi Visualisasi\n",
    "def visualize_graph(G, labels):\n",
    "    # Remove self-loops (edges that connect a node to itself)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    #nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\", node_size=5000, node_color='skyblue')\n",
    "    nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\")\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=12)\n",
    "    plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def generate_ngrams(words, n=2):\n",
    "    \"\"\"Generate ngrams from a list of words.\"\"\"\n",
    "    return [\" \".join(gram) for gram in ngrams(words, n)]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
