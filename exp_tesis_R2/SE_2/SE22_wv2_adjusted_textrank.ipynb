{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitur 132a\n",
    "Token phrase ditambahkan pre token pembentuknya (pembeda dg 131a-colabs)\n",
    "- Fitur 1_phrase_detection = yes, Phrase detection menggunakan nlp-id\n",
    "- Fitur 2_edge_weight = yes, Cooccurences matrix + cosinus similarity (fasttext 200)\n",
    "- Fitur 3_POS filter = yes\n",
    "- fitur 4_posisi_kata = yes, judul * 2\n",
    "- fitur 5_phrase_score = yes, bigram = 2x, trigram = 3x\n",
    "- fitur 6_combined_TFIDF = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. rutin2 membuat syspath ke root utk aktifkan __init__.py\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "sys.path.append(repo_root)\n",
    "\n",
    "#3. rutin3 Load the dataset\n",
    "dataset_path = os.path.join(repo_root, \"data/dataset_ekstraksi_r30_lg.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"judul\"] +\". \"+ df[\"isi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP & PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Preprocess\n",
    "'''\n",
    "stopwords tidak masuk dalam preprocessing\n",
    "'''\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df['text'].apply(preprocess)\n",
    "df[\"judul\"] = df[\"judul\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Load stopword\n",
    "def load_stopwords(repo_root, stopwords_file_path):\n",
    "    \"\"\"\n",
    "    Load stopwords from a file located in the repository.\n",
    "\n",
    "    :param repo_root: Root directory of the repository.\n",
    "    :param stopwords_file_path: Relative path to the stopwords file from the repo_root.\n",
    "    :return: Set of stopwords.\n",
    "    \"\"\"\n",
    "    stopwords_path = os.path.join(repo_root, stopwords_file_path)\n",
    "    with open(stopwords_path, 'r') as file:\n",
    "        stopwords = set(file.read().strip().splitlines())\n",
    "    return stopwords\n",
    "\n",
    "repo_root = repo_root\n",
    "stopwords = load_stopwords(repo_root, \"notebooks/stopwords_tuning/all_stop_words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITUR-1 Phrase Detection\n",
    "\n",
    "Dilakukan sebelum komputasi TextRank\n",
    "detect_all_tokens\n",
    "|--incorporate_bigrams_trigrams(unigrams, bigrams, trigrams)\n",
    "   |--detect_bigram(text):\n",
    "   |--detect_trigram(text):\n",
    "\n",
    "ada 3 pilihan\n",
    "1. frase menggantikan token pembentuknya\n",
    "2. frase ditambahkan pada awal token pembentuk pertama\n",
    "3. frase ditambahkan pada akhir token pembentuk terakhir\n",
    "4. frase ditambahkan 2x pre+post token pembentuk terakhir\n",
    "\n",
    "Pemilihannya see script dibawah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template document jtb gpf project mengacu kepada dokumen jtb cp ctr exhibit coordination procedure kami sampaikan template document yang akan dipergunakan pada proyek jambaran tiung biru jtb gas processing facilities gpf demikian disampaikan sebagai acuan pengelolaan dokumen atas perhatiannya kami ucapkan terima kasih\n",
      "================================================================\n",
      "['template document', 'jtb', 'gpf', 'project', 'mengacu', 'kepada', 'dokumen', 'jtb', 'cp', 'ctr', 'exhibit', 'coordination', 'procedure', 'kami', 'sampaikan', 'template document', 'yang', 'akan', 'dipergunakan', 'pada', 'proyek', 'jambaran', 'tiung', 'biru', 'jtb', 'gas', 'processing', 'facilities', 'gpf', 'demikian', 'disampaikan', 'sebagai', 'acuan', 'pengelolaan', 'dokumen', 'atas', 'perhatiannya', 'kami', 'ucapkan', 'terima', 'kasih']\n"
     ]
    }
   ],
   "source": [
    "# FITUR-1 Fungsi Phrase Detection\n",
    "\n",
    "from nlp_id.tokenizer import PhraseTokenizer \n",
    "from nlp_id.postag import PosTag\n",
    "\n",
    "def detect_bigram(text):\n",
    "    \n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only bigrams whose individual words are in available_tokens\n",
    "    bigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 1]\n",
    "\n",
    "    return bigrams_only\n",
    "\n",
    "def detect_trigram(text):\n",
    "\n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only trigrams whose individual words are in available_tokens\n",
    "    trigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 2 ]\n",
    "\n",
    "    return trigrams_only\n",
    "\n",
    "# Phrase replace constructed Token\n",
    "def incorporate_bigrams_trigrams(unigrams, bigrams, trigrams):\n",
    "    combined_tokens = []\n",
    "    skip = 0\n",
    "\n",
    "    for i in range(len(unigrams)):\n",
    "        if skip > 0:\n",
    "            skip -= 1\n",
    "            continue\n",
    "\n",
    "        bigram_formed = ' '.join(unigrams[i:i+2]) in bigrams\n",
    "        trigram_formed = ' '.join(unigrams[i:i+3]) in trigrams\n",
    "\n",
    "        if bigram_formed:\n",
    "            combined_tokens.append(' '.join(unigrams[i:i+2]))\n",
    "            skip = 1\n",
    "        elif trigram_formed:\n",
    "            combined_tokens.append(' '.join(unigrams[i:i+3]))\n",
    "            skip = 2\n",
    "        else:\n",
    "            combined_tokens.append(unigrams[i])\n",
    "\n",
    "    return combined_tokens\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Phrase add pre-constructed Token\n",
    "def incorporate_bigrams_trigrams(unigrams, bigrams, trigrams):\n",
    "    combined_tokens = []\n",
    "\n",
    "    for i in range(len(unigrams)):\n",
    "        bigram_formed = ' '.join(unigrams[i:i+2]) in bigrams\n",
    "        trigram_formed = ' '.join(unigrams[i:i+3]) in trigrams\n",
    "\n",
    "        if trigram_formed:\n",
    "            combined_tokens.append(' '.join(unigrams[i:i+3]))\n",
    "        elif bigram_formed:\n",
    "            combined_tokens.append(' '.join(unigrams[i:i+2]))\n",
    "\n",
    "        combined_tokens.append(unigrams[i])\n",
    "\n",
    "    return combined_tokens\n",
    "\n",
    "# Phrase add twice pre/post-constructed Token\n",
    "def incorporate_bigrams_trigrams(unigrams, bigrams, trigrams):\n",
    "    combined_tokens = []\n",
    "\n",
    "    for i in range(len(unigrams)):\n",
    "        bigram_formed = ' '.join(unigrams[i:i+2]) in bigrams\n",
    "        trigram_formed = ' '.join(unigrams[i:i+3]) in trigrams\n",
    "\n",
    "        if trigram_formed:\n",
    "            phrase = ' '.join(unigrams[i:i+3])\n",
    "            combined_tokens.append(phrase)  # Append before the unigrams\n",
    "            combined_tokens.extend(unigrams[i:i+3])\n",
    "            combined_tokens.append(phrase)  # Append after the unigrams\n",
    "        elif bigram_formed:\n",
    "            phrase = ' '.join(unigrams[i:i+2])\n",
    "            combined_tokens.append(phrase)  # Append before the unigrams\n",
    "            combined_tokens.extend(unigrams[i:i+2])\n",
    "            combined_tokens.append(phrase)  # Append after the unigrams\n",
    "        else:\n",
    "            combined_tokens.append(unigrams[i])\n",
    "\n",
    "    return combined_tokens\n",
    "\"\"\"\n",
    "\n",
    "def detect_all_tokens(text):\n",
    "    unigrams = [word for word in text.split()]\n",
    "    bigrams = detect_bigram(text)\n",
    "    trigrams = detect_trigram(text)\n",
    "    \n",
    "    # Incorporating bigrams and trigrams into the sequence of tokens\n",
    "    all_tokens = incorporate_bigrams_trigrams(unigrams, bigrams, trigrams)\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "# example\n",
    "text = df[\"text\"][1]\n",
    "words = detect_all_tokens(text)\n",
    "print(text)\n",
    "print(\"================================================================\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD EMBEDDING SELECTION\n",
    "ada 2 pilihan dalam vector soze sama = 200\n",
    "1. word2vec\n",
    "2. fasttext\n",
    "\n",
    "pilihan see script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Path to the FastText model\\nfasttext_model_path = os.path.join(repo_root, \"models/embedding_model/fasttext_ina_200_with_phrases.bin\")\\nfrom gensim.models.fasttext import load_facebook_model\\n\\ndef load_fasttext_model(model_path):\\n    if not os.path.exists(model_path):\\n        raise FileNotFoundError(f\"The provided FastText model path does not exist: {model_path}\")\\n    \\n    # Use load_facebook_model for full model loading\\n    ft_model = load_facebook_model(model_path)\\n    available_tokens = set(ft_model.wv.key_to_index)\\n    \\n    return ft_model, available_tokens\\n    \\nft_model, available_tokens = load_fasttext_model(fasttext_model_path)\\n\\n# Check the size of the word vectors\\nvector_size = ft_model.wv.vector_size\\nprint(\"The size of the FastText word vectors is:\", vector_size)\\n\\n# Function to compose vector for a phrase if not defined in the FastText model\\ndef get_phrase_embedding(phrase, ft_model):\\n    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\\n    words = phrase.split()\\n    embeddings = [ft_model.wv[word] for word in words if word in ft_model.wv.key_to_index]\\n    if embeddings:\\n        return np.mean(embeddings, axis=0)\\n    else:\\n        return None\\n    \\n# Example usage\\nphrase = \"bekerja sama\"\\nphrase_vector = get_phrase_embedding(phrase, ft_model)\\n#print(phrase_vector)\\n\\n# Cosine similarity fasttext\\ndef get_cosine_similarity(w1, w2, ft_model):\\n    vec1 = get_phrase_embedding(w1, ft_model)\\n    vec2 = get_phrase_embedding(w2, ft_model)\\n\\n    if vec1 is None or vec2 is None:\\n        return 0\\n\\n    similarity = cosine_similarity([vec1], [vec2])[0][0]\\n    return similarity\\n\\n# Example usage with the FastText model\\nw1 = \\'gotong royong\\'\\nw2 = \\'bekerja sama\\'\\nft_model, available_tokens = load_fasttext_model(fasttext_model_path)\\ncos_sim = get_cosine_similarity(w1, w2, ft_model)\\n#print(cos_sim)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load Word2vec Model\n",
    "w2v_model_path = os.path.join(repo_root, \"models/embedding_model/w2v_wiki_own_phrase_training_200.model\")\n",
    "\n",
    "def load_word2vec_model(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"The provided Word2Vec model path does not exist: {model_path}\")\n",
    "    \n",
    "    w2v_model = Word2Vec.load(model_path) \n",
    "    available_tokens = set(w2v_model.wv.key_to_index)\n",
    "    \n",
    "    return w2v_model, available_tokens\n",
    "    \n",
    "w2v_model, available_tokens = load_word2vec_model(w2v_model_path)\n",
    "\n",
    "# Function to compose vector for a phrase if not defined on the w2v model\n",
    "def get_phrase_embedding(phrase, w2v_model):\n",
    "    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example\n",
    "phrase = \"bekerja sama\"\n",
    "phrase_vector = get_phrase_embedding(phrase, w2v_model)\n",
    "phrase_vector\n",
    "\n",
    "# Cosine similarity word2vec\n",
    "def get_cosine_similarity(w1, w2, w2v_model):\n",
    "    vec1 = get_phrase_embedding(w1, w2v_model)\n",
    "    vec2 = get_phrase_embedding(w2, w2v_model)\n",
    "\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return 0\n",
    "\n",
    "    similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# example, seharusnya dekat hubungan 2 vector ini\n",
    "w1 = 'gotong royong'\n",
    "w2 = 'bekerja sama'\n",
    "w2v_model, available_tokens = load_word2vec_model(w2v_model_path)\n",
    "cos_sim = get_cosine_similarity(w1, w2, w2v_model)\n",
    "cos_sim\n",
    "\n",
    "'''\n",
    "\n",
    "# Path to the FastText model\n",
    "fasttext_model_path = os.path.join(repo_root, \"models/embedding_model/fasttext_ina_200_with_phrases.bin\")\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "def load_fasttext_model(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"The provided FastText model path does not exist: {model_path}\")\n",
    "    \n",
    "    # Use load_facebook_model for full model loading\n",
    "    ft_model = load_facebook_model(model_path)\n",
    "    available_tokens = set(ft_model.wv.key_to_index)\n",
    "    \n",
    "    return ft_model, available_tokens\n",
    "    \n",
    "ft_model, available_tokens = load_fasttext_model(fasttext_model_path)\n",
    "\n",
    "# Check the size of the word vectors\n",
    "vector_size = ft_model.wv.vector_size\n",
    "print(\"The size of the FastText word vectors is:\", vector_size)\n",
    "\n",
    "# Function to compose vector for a phrase if not defined in the FastText model\n",
    "def get_phrase_embedding(phrase, ft_model):\n",
    "    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    embeddings = [ft_model.wv[word] for word in words if word in ft_model.wv.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Example usage\n",
    "phrase = \"bekerja sama\"\n",
    "phrase_vector = get_phrase_embedding(phrase, ft_model)\n",
    "#print(phrase_vector)\n",
    "\n",
    "# Cosine similarity fasttext\n",
    "def get_cosine_similarity(w1, w2, ft_model):\n",
    "    vec1 = get_phrase_embedding(w1, ft_model)\n",
    "    vec2 = get_phrase_embedding(w2, ft_model)\n",
    "\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return 0\n",
    "\n",
    "    similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Example usage with the FastText model\n",
    "w1 = 'gotong royong'\n",
    "w2 = 'bekerja sama'\n",
    "ft_model, available_tokens = load_fasttext_model(fasttext_model_path)\n",
    "cos_sim = get_cosine_similarity(w1, w2, ft_model)\n",
    "#print(cos_sim)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITUR-2 dan FUNGSI KOMPUTASI TEXTRANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mengandung stopword: {'jtb': 0.04765153950540772, 'template document': 0.01778821089161495, 'gpf': 0.0354932823234596, 'project': 0.007845528991248282, 'kepada': 0.017736815900557416, 'mengacu': 0.00950470378968954, 'dokumen': 0.04800071234642289, 'cp': 0.025762095350920004, 'ctr': 0.036981538414791446, 'exhibit': 0.03519708358041934, 'coordination': 0.03617453415290588, 'procedure': 0.02437059202761161, 'kami': 0.04541964720385343, 'sampaikan': 0.01721824391794156, 'yang': 0.03016847879639734, 'akan': 0.03778132942970166, 'dipergunakan': 0.01353760444483702, 'pada': 0.03234450834588251, 'proyek': 0.01941791708534691, 'jambaran': 0.030469681718265702, 'tiung': 0.036362765637758795, 'biru': 0.02333437514626239, 'gas': 0.019775381190861224, 'processing': 0.03764123833441418, 'facilities': 0.03483608072073667, 'demikian': 0.017640185360616106, 'disampaikan': 0.023888128557881524, 'sebagai': 0.030781873038837018, 'acuan': 0.03990109235818122, 'pengelolaan': 0.03049966213654013, 'atas': 0.02915017972229755, 'perhatiannya': 0.025265034192931517, 'ucapkan': 0.0405025636885238, 'terima': 0.027343776325365443, 'kasih': 0.01421361537151767}\n"
     ]
    }
   ],
   "source": [
    "#text = df[\"text\"][1]\n",
    "#words = detect_all_tokens(text)\n",
    "\n",
    "# FITUR-2 dan FUNGSI KOMPUTASI TEXTRANK\n",
    "\n",
    "# Build Co-occurrence Matrix\n",
    "def build_co_occurrence_matrix(words, window_size=3):\n",
    "    co_occurrence = defaultdict(int)\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        window = words[i:i+window_size]\n",
    "        for j in range(window_size):\n",
    "            for k in range(j+1, window_size):\n",
    "                w1, w2 = sorted([window[j], window[k]])\n",
    "                if w1 != w2:\n",
    "                    co_occurrence[(w1, w2)] += 1\n",
    "    return co_occurrence\n",
    "\n",
    "# Build Graph and Compute TextRank\n",
    "def build_graph_and_compute_textrank(co_occurrence, embedding_model):\n",
    "    G = nx.Graph()\n",
    "    for (w1, w2), weight1 in co_occurrence.items():\n",
    "        weight2 = get_cosine_similarity(w1, w2, embedding_model)\n",
    "        weight3 = weight1 * weight2\n",
    "        if weight2 > 0:\n",
    "            G.add_edge(w1, w2, weight=weight3)\n",
    "    return nx.pagerank(G)\n",
    "\n",
    "\n",
    "# Example\n",
    "co_occurrence = build_co_occurrence_matrix(words)\n",
    "#co_occurrence\n",
    "\n",
    "scores = build_graph_and_compute_textrank(co_occurrence, w2v_model)\n",
    "#scores = build_graph_and_compute_textrank(co_occurrence, ft_model)\n",
    "\n",
    "print(\"mengandung stopword:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template document': 0.01778821089161495,\n",
       " 'project': 0.007845528991248282,\n",
       " 'mengacu': 0.00950470378968954,\n",
       " 'ctr': 0.036981538414791446,\n",
       " 'exhibit': 0.03519708358041934,\n",
       " 'coordination': 0.03617453415290588,\n",
       " 'procedure': 0.02437059202761161,\n",
       " 'jambaran': 0.030469681718265702,\n",
       " 'tiung': 0.036362765637758795,\n",
       " 'biru': 0.02333437514626239,\n",
       " 'processing': 0.03764123833441418,\n",
       " 'facilities': 0.03483608072073667,\n",
       " 'acuan': 0.03990109235818122,\n",
       " 'pengelolaan': 0.03049966213654013,\n",
       " 'perhatiannya': 0.025265034192931517}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning stop word for only unigram\n",
    "def filter_stopwords(scores, stopwords):\n",
    "    \"\"\"\n",
    "    Filters out stopwords from the scores dictionary only for unigrams. \n",
    "    \"\"\"\n",
    "    filtered_scores = {}\n",
    "    for term, score in scores.items():\n",
    "        words = term.split()\n",
    "        # Filter out the term if it is a unigram and a stopword\n",
    "        if len(words) == 1 and words[0] in stopwords:\n",
    "            continue\n",
    "        # Include the term otherwise\n",
    "        filtered_scores[term] = score\n",
    "    return filtered_scores\n",
    "\n",
    "scores_w_F12 = filter_stopwords(scores, stopwords)\n",
    "scores_w_F12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_w_F123 with POS filter : [('template document', 0.01778821089161495, 'NP'), ('project', 0.007845528991248282, 'NN'), ('mengacu', 0.00950470378968954, 'VB'), ('ctr', 0.036981538414791446, 'NN'), ('exhibit', 0.03519708358041934, 'NN'), ('coordination', 0.03617453415290588, 'FW'), ('procedure', 0.02437059202761161, 'NN'), ('jambaran', 0.030469681718265702, 'NN'), ('tiung', 0.036362765637758795, 'NN'), ('processing', 0.03764123833441418, 'NN'), ('facilities', 0.03483608072073667, 'NN'), ('acuan', 0.03990109235818122, 'NN'), ('pengelolaan', 0.03049966213654013, 'NN')]\n",
      "======================================================\n",
      "scores_w_F123 with no POS filter : [('template document', 0.01778821089161495, 'NP'), ('project', 0.007845528991248282, 'NN'), ('mengacu', 0.00950470378968954, 'VB'), ('ctr', 0.036981538414791446, 'NN'), ('exhibit', 0.03519708358041934, 'NN'), ('coordination', 0.03617453415290588, 'FW'), ('procedure', 0.02437059202761161, 'NN'), ('jambaran', 0.030469681718265702, 'NN'), ('tiung', 0.036362765637758795, 'NN'), ('biru', 0.02333437514626239, 'JJ'), ('processing', 0.03764123833441418, 'NN'), ('facilities', 0.03483608072073667, 'NN'), ('acuan', 0.03990109235818122, 'NN'), ('pengelolaan', 0.03049966213654013, 'NN'), ('perhatiannya', 0.025265034192931517, 'UNK')]\n"
     ]
    }
   ],
   "source": [
    "# FITUR 3_POS_filter\n",
    "\n",
    "def postag_tokens(tokens):\n",
    "    postagger = PosTag()\n",
    "    pos_tags = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if ' ' in token:\n",
    "            # Process the token as a phrase\n",
    "            phrase_tags = postagger.get_phrase_tag(token)\n",
    "            # Ensure phrase_tags are in the format [(word, tag), ...]\n",
    "            pos_tags.extend(phrase_tags)\n",
    "        else:\n",
    "            # Process the token as a single word\n",
    "            word_tag = postagger.get_pos_tag(token)\n",
    "            # Ensure word_tag is in the format [(word, tag)]\n",
    "            if word_tag:\n",
    "                pos_tags.extend(word_tag)\n",
    "\n",
    "    return pos_tags\n",
    "\n",
    "def attach_pos_tags_with_scores(scores, pos_tags):\n",
    "    # Create a dictionary from POS tags list\n",
    "    pos_dict = {word: tag for word, tag in pos_tags}\n",
    "    \n",
    "    # Attach POS tags to keywords with scores\n",
    "    return [(word, score, pos_dict.get(word, 'UNK')) for word, score in scores.items()]\n",
    "\n",
    "def filter_by_pos(keywords_with_pos, selected_pos):\n",
    "    return [item for item in keywords_with_pos if item[2] in selected_pos]\n",
    "\n",
    "# Example usage\n",
    "\n",
    "def F3_scores_w_pos_filter_no_filter(scores_wo_stopword, apply_filter=True, selected_pos_tags=None):\n",
    "    # Get POS tags for all unique words in the scores dictionary\n",
    "    unique_words = list(set([word for word in scores_wo_stopword.keys()]))\n",
    "    pos_tags = postag_tokens(unique_words)\n",
    "\n",
    "    # Attach POS tags to the scores\n",
    "    keywords_with_pos = attach_pos_tags_with_scores(scores_wo_stopword, pos_tags)\n",
    "\n",
    "    # If apply_filter is True, filter by Selected POS Tags\n",
    "    if apply_filter and selected_pos_tags:\n",
    "        scores_w_F3 = filter_by_pos(keywords_with_pos, selected_pos_tags)\n",
    "    else:\n",
    "        scores_w_F3 = keywords_with_pos\n",
    "\n",
    "    return scores_w_F3\n",
    "\n",
    "# Example usage\n",
    "selected_pos_tags = {'NN', 'NNP', 'VB', 'NP', 'VP', 'FW'}\n",
    "scores_w_F123 = F3_scores_w_pos_filter_no_filter(scores_w_F12, apply_filter=True, selected_pos_tags=selected_pos_tags)\n",
    "print(\"scores_w_F123 with POS filter :\", scores_w_F123)\n",
    "print(\"======================================================\")\n",
    "scores_w_F123 = F3_scores_w_pos_filter_no_filter(scores_w_F12, apply_filter=True, selected_pos_tags=None)\n",
    "print(\"scores_w_F123 with no POS filter :\", scores_w_F123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_w_F1234 : [('template document', 0.0355764217832299, 'NP'), ('project', 0.015691057982496565, 'NN'), ('mengacu', 0.00950470378968954, 'VB'), ('ctr', 0.036981538414791446, 'NN'), ('exhibit', 0.03519708358041934, 'NN'), ('coordination', 0.03617453415290588, 'FW'), ('procedure', 0.02437059202761161, 'NN'), ('jambaran', 0.030469681718265702, 'NN'), ('tiung', 0.036362765637758795, 'NN'), ('biru', 0.02333437514626239, 'JJ'), ('processing', 0.03764123833441418, 'NN'), ('facilities', 0.03483608072073667, 'NN'), ('acuan', 0.03990109235818122, 'NN'), ('pengelolaan', 0.03049966213654013, 'NN'), ('perhatiannya', 0.025265034192931517, 'UNK')]\n"
     ]
    }
   ],
   "source": [
    "# Fitur 4_posisi_kata = yes, judul * 2\n",
    "# baru bisa di lakukan setelah textRank\n",
    "\n",
    "def F4_scores_weight_for_title(scores_w_F3, title, title_score=2):\n",
    "    # Split the title into individual words for comparison\n",
    "    title_words = set(title.split())\n",
    "\n",
    "    # Create a new list to store adjusted scores\n",
    "    adjusted_scores = []\n",
    "\n",
    "    for token, score, pos in scores_w_F3:\n",
    "        # Check if the token (or any word in a phrase) is in the title\n",
    "        if any(word in title_words for word in token.lower().split()):\n",
    "            adjusted_score = score * title_score\n",
    "        else:\n",
    "            adjusted_score = score\n",
    "\n",
    "        # Append the adjusted score tuple to the new list\n",
    "        adjusted_scores.append((token, adjusted_score, pos))\n",
    "\n",
    "    return adjusted_scores\n",
    "\n",
    "# Example Usage\n",
    "title = df[\"judul\"][1]# Replace with actual title\n",
    "#scores_w_F3 = [('template', 0.0421080428605898, 'FW'), ('template document', 0.03960151787052916, 'NP'), ...]  # truncated for brevity\n",
    "scores_w_F1234 = F4_scores_weight_for_title(scores_w_F123, title)\n",
    "\n",
    "print(\"scores_w_F1234 :\", scores_w_F1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_w_F12345 : [('acuan', 0.03990109235818122, 'NN'), ('processing', 0.03764123833441418, 'NN'), ('ctr', 0.036981538414791446, 'NN'), ('tiung', 0.036362765637758795, 'NN'), ('coordination', 0.03617453415290588, 'FW'), ('exhibit', 0.03519708358041934, 'NN'), ('facilities', 0.03483608072073667, 'NN'), ('pengelolaan', 0.03049966213654013, 'NN'), ('jambaran', 0.030469681718265702, 'NN'), ('perhatiannya', 0.025265034192931517, 'UNK'), ('procedure', 0.02437059202761161, 'NN'), ('biru', 0.02333437514626239, 'JJ'), ('project', 0.015691057982496565, 'NN'), ('mengacu', 0.00950470378968954, 'VB'), ('template document', 0, 'NP')]\n"
     ]
    }
   ],
   "source": [
    "# Fitur 5_Phrase Score\n",
    "\"\"\"\n",
    "# Metode A dengan pembobotan\n",
    "def F5_scores_weight_for_phrase(scores_w_F1234, bigram_weight=2, trigram_weight=3):\n",
    "    adjusted_scores = []\n",
    "\n",
    "    for token, score, pos in scores_w_F1234:\n",
    "        token_count = len(token.split())\n",
    "\n",
    "        # Adjust the score based on the number of words in the token\n",
    "        if token_count == 2:  # Bigram\n",
    "            adjusted_score = score * bigram_weight\n",
    "        elif token_count == 3:  # Trigram\n",
    "            adjusted_score = score * trigram_weight\n",
    "        else:  # Unigram or more than 3 words\n",
    "            adjusted_score = score\n",
    "\n",
    "        adjusted_scores.append((token, adjusted_score, pos))\n",
    "\n",
    "    # Sort the words based on adjusted scores\n",
    "    adjusted_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return adjusted_scores\n",
    "\n",
    "# Example Usage\n",
    "scores_w_F12345 = F5_scores_weight_for_phrase(scores_w_F1234, bigram_weight=2, trigram_weight=3)\n",
    "\n",
    "print(\"scores_w_F12345 :\", scores_w_F12345)\n",
    "\"\"\"\n",
    "\n",
    "# Metode B dengan akumulasi score token pembentuknya\n",
    "\n",
    "def F5_scores_weight_for_phrase(scores_w_F1234):\n",
    "    adjusted_scores = []\n",
    "\n",
    "    # Convert the scores list to a dictionary for easy score lookup\n",
    "    score_dict = {token: score for token, score, pos in scores_w_F1234}\n",
    "\n",
    "    for token, score, pos in scores_w_F1234:\n",
    "        token_count = len(token.split())\n",
    "\n",
    "        # Adjust the score for bigram and trigram\n",
    "        if token_count == 2 or token_count == 3:\n",
    "            # Split the token into unigrams\n",
    "            unigrams = token.split()\n",
    "            # Sum the scores of the unigrams\n",
    "            adjusted_score = sum(score_dict.get(unigram, 0) for unigram in unigrams)\n",
    "        else:\n",
    "            # For unigram or tokens with more than 3 words, keep the original score\n",
    "            adjusted_score = score\n",
    "\n",
    "        adjusted_scores.append((token, adjusted_score, pos))\n",
    "\n",
    "    # Sort the words based on adjusted scores\n",
    "    adjusted_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return adjusted_scores\n",
    "\n",
    "# Example Usage\n",
    "scores_w_F12345 = F5_scores_weight_for_phrase(scores_w_F1234)\n",
    "\n",
    "print(\"scores_w_F12345 :\", scores_w_F12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_w_F12345 : [('acuan', 0.03990109235818122, 'NN'), ('processing', 0.03764123833441418, 'NN'), ('ctr', 0.036981538414791446, 'NN'), ('tiung', 0.036362765637758795, 'NN'), ('coordination', 0.03617453415290588, 'FW'), ('exhibit', 0.03519708358041934, 'NN'), ('facilities', 0.03483608072073667, 'NN'), ('pengelolaan', 0.03049966213654013, 'NN'), ('jambaran', 0.030469681718265702, 'NN'), ('perhatiannya', 0.025265034192931517, 'UNK'), ('procedure', 0.02437059202761161, 'NN'), ('biru', 0.02333437514626239, 'JJ'), ('project', 0.015691057982496565, 'NN'), ('mengacu', 0.00950470378968954, 'VB'), ('template document', 0, 'NP')]\n"
     ]
    }
   ],
   "source": [
    "# Fitur 6_TF-IDF Score weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def F6_scores_tfidf(scores_w_F12345, document, corpus):\n",
    "\n",
    "  '''\n",
    "  fungsi untuk mencari score tfidf dari sebuah word dalam document dan corpus\n",
    "  word = token yang diperoleh dari list scores_w_F12345\n",
    "  document = f[\"text\"][i] dengan i adalah row cell dimana word berada\n",
    "  corpus = df[\"text\"]\n",
    "  '''\n",
    "  # Convert the corpus into a format suitable for TF-IDF\n",
    "  corpus_list = corpus.tolist()\n",
    "  # Initialize a TF-IDF Vectorizer\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  # Compute the TF-IDF matrix\n",
    "  tfidf_matrix = vectorizer.fit_transform(corpus_list)\n",
    "  # Convert the TF-IDF matrix to a DataFrame\n",
    "  tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "  # Find the index of the document in the corpus\n",
    "  doc_index = corpus_list.index(document)\n",
    "  # Extract the TF-IDF scores for the document\n",
    "  doc_tfidf_scores = tfidf_df.iloc[doc_index]\n",
    "  # Multiply the TextRank scores with the corresponding TF-IDF scores\n",
    "  updated_F6_scores = []\n",
    "  for word, score, pos in scores_w_F12345:\n",
    "      tfidf_score = doc_tfidf_scores[word] if word in doc_tfidf_scores else 0\n",
    "      #print('word: ',word, 'tfidfscore: ',tfidf_score)\n",
    "      updated_score = score * tfidf_score\n",
    "      updated_F6_scores.append((word, updated_score, pos))\n",
    "\n",
    "  return updated_F6_scores\n",
    "\n",
    "# Example\n",
    "document = text\n",
    "corpus = df[\"text\"]\n",
    "scores_w_F123456 = F6_scores_tfidf(scores_w_F12345, document, corpus)\n",
    "\n",
    "print(\"scores_w_F12345 :\", scores_w_F12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN PROCESS ITERATION FOR ALL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_keyphrases_dataframe(keyphrases_scores):\n",
    "    \"\"\"\n",
    "    Formats the keyphrases and their scores into a structured DataFrame.\n",
    "    Parameters:\n",
    "    - keyphrases_scores: List of tuples containing keywords, scores, and POS tags.\n",
    "    \"\"\"\n",
    "    df_keyphrases = pd.DataFrame(keyphrases_scores, columns=['Keyword', 'Score', 'pos'])\n",
    "\n",
    "    # Transpose and reindex the DataFrame to ensure consistent structure\n",
    "    a = pd.DataFrame(df_keyphrases.Keyword).T.reset_index(drop=True)\n",
    "    a = a.reindex(columns=range(10), fill_value=0)\n",
    "    b = pd.DataFrame(df_keyphrases.Score).round(3).T.reset_index(drop=True)\n",
    "    b = b.reindex(columns=range(10), fill_value=0)\n",
    "    c = pd.DataFrame(df_keyphrases.pos).T.reset_index(drop=True)\n",
    "    c = c.reindex(columns=range(10), fill_value=0)\n",
    "\n",
    "    df_keyphrases = pd.concat([a, b, c], axis=1)\n",
    "\n",
    "    # Ensure the DataFrame has 30 columns, adding missing ones with zero values\n",
    "    missing_columns = 30 - df_keyphrases.shape[1]\n",
    "    for _ in range(missing_columns):\n",
    "        df_keyphrases[df_keyphrases.shape[1]] = 0\n",
    "\n",
    "    # Naming the columns\n",
    "    df_keyphrases.columns = ['key_1', 'key_2', 'key_3', 'key_4', 'key_5', 'key_6', 'key_7', 'key_8', 'key_9', 'key_10',\n",
    "                             'score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'score_6', 'score_7', 'score_8', 'score_9', 'score_10',\n",
    "                             'pos_1', 'pos_2', 'pos_3', 'pos_4', 'pos_5', 'pos_6', 'pos_7', 'pos_8', 'pos_9', 'pos_10']\n",
    "\n",
    "    return df_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 0...! Done\n",
      "Processing index 1...! Done\n",
      "Processing index 2...! Done\n",
      "Processing index 3...! Done\n",
      "Processing index 4...! Done\n",
      "Processing index 5...! Done\n",
      "Processing index 6...! Done\n",
      "Processing index 7...! Done\n",
      "Processing index 8...! Done\n",
      "Processing index 9...! Done\n",
      "Processing index 10...! Done\n",
      "Processing index 11...! Done\n",
      "Processing index 12...! Done\n",
      "Processing index 13...! Done\n",
      "Processing index 14...! Done\n",
      "Processing index 15...! Done\n",
      "Processing index 16...! Done\n",
      "Processing index 17...! Done\n",
      "Processing index 18...! Done\n",
      "Processing index 19...! Done\n",
      "Processing index 20...! Done\n",
      "Processing index 21...! Done\n",
      "Processing index 22...! Done\n",
      "Processing index 23...! Done\n",
      "Processing index 24...! Done\n",
      "Processing index 25...! Done\n",
      "Processing index 26...! Done\n",
      "Processing index 27...! Done\n",
      "Processing index 28...! Done\n",
      "Processing index 29...! Done\n",
      "Processing index 30...! Done\n",
      "Processing index 31...! Done\n",
      "Processing index 32...! Done\n",
      "Processing index 33...! Done\n",
      "Processing index 34...! Done\n",
      "Processing index 35...! Done\n",
      "Processing index 36...! Done\n",
      "Processing index 37...! Done\n",
      "Processing index 38...! Done\n",
      "Processing index 39...! Done\n",
      "Processing index 40...! Done\n",
      "Processing index 41...! Done\n",
      "Processing index 42...! Done\n",
      "Processing index 43...! Done\n",
      "Processing index 44...! Done\n",
      "Processing index 45...! Done\n",
      "Processing index 46...! Done\n",
      "Processing index 47...! Done\n",
      "Processing index 48...! Done\n",
      "Processing index 49...! Done\n",
      "Processing index 50...! Done\n",
      "Processing index 51...! Done\n",
      "Processing index 52...! Done\n",
      "Processing index 53...! Done\n",
      "Processing index 54...! Done\n",
      "Processing index 55...! Done\n",
      "Processing index 56...! Done\n",
      "Processing index 57...! Done\n",
      "Processing index 58...! Done\n",
      "Processing index 59...! Done\n",
      "Processing index 60...! Done\n",
      "Processing index 61...! Done\n",
      "Processing index 62...! Done\n",
      "Processing index 63...! Done\n",
      "Processing index 64...! Done\n",
      "Processing index 65...! Done\n",
      "Processing index 66...! Done\n",
      "Processing index 67...! Done\n",
      "Processing index 68...! Done\n",
      "Processing index 69...! Done\n",
      "Processing index 70...! Done\n",
      "Processing index 71...! Done\n",
      "Processing index 72...! Done\n",
      "Processing index 73...! Done\n",
      "Processing index 74...! Done\n",
      "Processing index 75...! Done\n",
      "Processing index 76...! Done\n",
      "Processing index 77...! Done\n",
      "Processing index 78...! Done\n",
      "Processing index 79...! Done\n",
      "Processing index 80...! Done\n",
      "Processing index 81...! Done\n",
      "Processing index 82...! Done\n",
      "Processing index 83...! Done\n",
      "Processing index 84...! Done\n",
      "Processing index 85...! Done\n",
      "Processing index 86...! Done\n",
      "Processing index 87...! Done\n",
      "Processing index 88...! Done\n",
      "Processing index 89...! Done\n",
      "Processing index 90...! Done\n",
      "Processing index 91...! Done\n",
      "Processing index 92...! Done\n",
      "Processing index 93...! Done\n",
      "Processing index 94...! Done\n",
      "Processing index 95...! Done\n",
      "Processing index 96...! Done\n",
      "Processing index 97...! Done\n",
      "Processing index 98...! Done\n",
      "Processing index 99...! Done\n",
      "Processing index 100...! Done\n",
      "Processing index 101...! Done\n",
      "Processing index 102...! Done\n",
      "Processing index 103...! Done\n",
      "Processing index 104...! Done\n",
      "Processing index 105...! Done\n",
      "Processing index 106...! Done\n",
      "Processing index 107...! Done\n",
      "Processing index 108...! Done\n",
      "Processing index 109...! Done\n",
      "Processing index 110...! Done\n",
      "Processing index 111...! Done\n",
      "Processing index 112...! Done\n",
      "Processing index 113...! Done\n",
      "Processing index 114...! Done\n",
      "Processing index 115...! Done\n",
      "Processing index 116...! Done\n",
      "Processing index 117...! Done\n",
      "Processing index 118...! Done\n",
      "Processing index 119...! Done\n",
      "Processing index 120...! Done\n",
      "Processing index 121...! Done\n",
      "Processing index 122...! Done\n",
      "Processing index 123...! Done\n",
      "Processing index 124...! Done\n",
      "Processing index 125...! Done\n",
      "Processing index 126...! Done\n",
      "Processing index 127...! Done\n",
      "Processing index 128...! Done\n",
      "Processing index 129...! Done\n",
      "Processing index 130...! Done\n",
      "Processing index 131...! Done\n",
      "Processing index 132...! Done\n",
      "Processing index 133...! Done\n",
      "Processing index 134...! Done\n",
      "Processing index 135...! Done\n",
      "Processing index 136...! Done\n",
      "Processing index 137...! Done\n",
      "Processing index 138...! Done\n",
      "Processing index 139...! Done\n",
      "Processing index 140...! Done\n",
      "Processing index 141...! Done\n",
      "Processing index 142...! Done\n",
      "Processing index 143...! Done\n",
      "Processing index 144...! Done\n",
      "Processing index 145...! Done\n",
      "Processing index 146...! Done\n",
      "Processing index 147...! Done\n",
      "Processing index 148...! Done\n",
      "Processing index 149...! Done\n",
      "Processing index 150...! Done\n",
      "Processing index 151...! Done\n",
      "Processing index 152...! Done\n",
      "Processing index 153...! Done\n",
      "Processing index 154...! Done\n",
      "Processing index 155...! Done\n",
      "Processing index 156...! Done\n",
      "Processing index 157...! Done\n",
      "Processing index 158...! Done\n",
      "Processing index 159...! Done\n",
      "Processing index 160...! Done\n",
      "Processing index 161...! Done\n",
      "Processing index 162...! Done\n",
      "Processing index 163...! Done\n",
      "Processing index 164...! Done\n",
      "Processing index 165...! Done\n",
      "Processing index 166...! Done\n",
      "Processing index 167...! Done\n",
      "Processing index 168...! Done\n",
      "Processing index 169...! Done\n",
      "Processing index 170...! Done\n",
      "Processing index 171...! Done\n",
      "Processing index 172...! Done\n",
      "Processing index 173...! Done\n",
      "Processing index 174...! Done\n",
      "Processing index 175...! Done\n",
      "Processing index 176...! Done\n",
      "Processing index 177...! Done\n",
      "Processing index 178...! Done\n",
      "Processing index 179...! Done\n",
      "Processing index 180...! Done\n",
      "Processing index 181...! Done\n",
      "Processing index 182...! Done\n",
      "Processing index 183...! Done\n",
      "Processing index 184...! Done\n",
      "Processing index 185...! Done\n",
      "Processing index 186...! Done\n",
      "Processing index 187...! Done\n",
      "Processing index 188...! Done\n",
      "Processing index 189...! Done\n",
      "Processing index 190...! Done\n",
      "Processing index 191...! Done\n",
      "Processing index 192...! Done\n",
      "Processing index 193...! Done\n",
      "Processing index 194...! Done\n",
      "Processing index 195...! Done\n",
      "Processing index 196...! Done\n",
      "Processing index 197...! Done\n",
      "Processing index 198...! Done\n",
      "Processing index 199...! Done\n",
      "Processing index 200...! Done\n",
      "Processing index 201...! Done\n",
      "Processing index 202...! Done\n",
      "Processing index 203...! Done\n",
      "Processing index 204...! Done\n",
      "Processing index 205...! Done\n",
      "Processing index 206...! Done\n",
      "Processing index 207...! Done\n",
      "Processing index 208...! Done\n",
      "Processing index 209...! Done\n",
      "Processing index 210...! Done\n",
      "Processing index 211...! Done\n",
      "Processing index 212...! Done\n",
      "Processing index 213...! Done\n",
      "Processing index 214...! Done\n",
      "Processing index 215...! Done\n",
      "Processing index 216...! Done\n",
      "Processing index 217...! Done\n",
      "Processing index 218...! Done\n",
      "Processing index 219...! Done\n",
      "Processing index 220...! Done\n",
      "Processing index 221...! Done\n",
      "Processing index 222...! Done\n",
      "Processing index 223...! Done\n",
      "Processing index 224...! Done\n",
      "Processing index 225...! Done\n",
      "Processing index 226...! Done\n",
      "Processing index 227...! Done\n",
      "Processing index 228...! Done\n",
      "Processing index 229...! Done\n",
      "Processing index 230...! Done\n",
      "Processing index 231...! Done\n",
      "Processing index 232...! Done\n",
      "Processing index 233...! Done\n",
      "Processing index 234...! Done\n",
      "Processing index 235...! Done\n",
      "Processing index 236...! Done\n",
      "Processing index 237...! Done\n",
      "Processing index 238...! Done\n",
      "Processing index 239...! Done\n",
      "Processing index 240...! Done\n",
      "Processing index 241...! Done\n",
      "Processing index 242...! Done\n",
      "Processing index 243...! Done\n",
      "Processing index 244...! Done\n",
      "Processing index 245...! Done\n",
      "Processing index 246...! Done\n",
      "Processing index 247...! Done\n",
      "Processing index 248...! Done\n",
      "Processing index 249...! Done\n",
      "Processing index 250...! Done\n",
      "Processing index 251...! Done\n",
      "Processing index 252...! Done\n",
      "Processing index 253...! Done\n",
      "Processing index 254...! Done\n",
      "Processing index 255...! Done\n",
      "Processing index 256...! Done\n",
      "Processing index 257...! Done\n",
      "Processing index 258...! Done\n",
      "Processing index 259...! Done\n",
      "Processing index 260...! Done\n",
      "Processing index 261...! Done\n",
      "Processing index 262...! Done\n",
      "Processing index 263...! Done\n",
      "Processing index 264...! Done\n",
      "Processing index 265...! Done\n",
      "Processing index 266...! Done\n",
      "Processing index 267...! Done\n",
      "Processing index 268...! Done\n",
      "Processing index 269...! Done\n",
      "Processing index 270...! Done\n",
      "Processing index 271...! Done\n",
      "Processing index 272...! Done\n",
      "Processing index 273...! Done\n",
      "Processing index 274...! Done\n",
      "Processing index 275...! Done\n",
      "Processing index 276...! Done\n",
      "Processing index 277...! Done\n",
      "Processing index 278...! Done\n",
      "Processing index 279...! Done\n",
      "Processing index 280...! Done\n",
      "Processing index 281...! Done\n",
      "Processing index 282...! Done\n",
      "Processing index 283...! Done\n",
      "Processing index 284...! Done\n",
      "Processing index 285...! Done\n",
      "Processing index 286...! Done\n",
      "Processing index 287...! Done\n",
      "Processing index 288...! Done\n",
      "Processing index 289...! Done\n",
      "Processing index 290...! Done\n",
      "Processing index 291...! Done\n",
      "Processing index 292...! Done\n",
      "Processing index 293...! Done\n",
      "Processing index 294...! Done\n",
      "Processing index 295...! Done\n",
      "Processing index 296...! Done\n",
      "Processing index 297...! Done\n",
      "Processing index 298...! Done\n",
      "Processing index 299...! Done\n",
      "Processing index 300...! Done\n",
      "Processing index 301...! Done\n",
      "Processing index 302...! Done\n",
      "Processing index 303...! Done\n",
      "Processing index 304...! Done\n",
      "Processing index 305...! Done\n",
      "Processing index 306...! Done\n",
      "Processing index 307...! Done\n",
      "Processing index 308...! Done\n",
      "Processing index 309...! Done\n",
      "Processing index 310...! Done\n",
      "Processing index 311...! Done\n",
      "Processing index 312...! Done\n",
      "Processing index 313...! Done\n",
      "Processing index 314...! Done\n",
      "Processing index 315...! Done\n",
      "Processing index 316...! Done\n",
      "Processing index 317...! Done\n",
      "Processing index 318...! Done\n",
      "Processing index 319...! Done\n",
      "Processing index 320...! Done\n",
      "Processing index 321...! Done\n",
      "Processing index 322...! Done\n",
      "Processing index 323...! Done\n",
      "Processing index 324...! Done\n",
      "Processing index 325...! Done\n",
      "Processing index 326...! Done\n",
      "Processing index 327...! Done\n",
      "Processing index 328...! Done\n",
      "Processing index 329...! Done\n",
      "Processing index 330...! Done\n",
      "Processing index 331...! Done\n",
      "Processing index 332...! Done\n",
      "Processing index 333...! Done\n",
      "Processing index 334...! Done\n",
      "Processing index 335...! Done\n",
      "Processing index 336...! Done\n",
      "Processing index 337...! Done\n",
      "Processing index 338...! Done\n",
      "Processing index 339...! Done\n",
      "Processing index 340...! Done\n",
      "Processing index 341...! Done\n",
      "Processing index 342...! Done\n",
      "Processing index 343...! Done\n",
      "Processing index 344...! Done\n",
      "Processing index 345...! Done\n",
      "Processing index 346...! Done\n",
      "Processing index 347...! Done\n",
      "Processing index 348...! Done\n",
      "Processing index 349...! Done\n",
      "Processing index 350...! Done\n",
      "Processing index 351...! Done\n",
      "Processing index 352...! Done\n",
      "Processing index 353...! Done\n",
      "Processing index 354...! Done\n",
      "Processing index 355...! Done\n",
      "Processing index 356...! Done\n",
      "Processing index 357...! Done\n",
      "Processing index 358...! Done\n",
      "Processing index 359...! Done\n",
      "Processing index 360...! Done\n",
      "Processing index 361...! Done\n",
      "Processing index 362...! Done\n",
      "Processing index 363...! Done\n",
      "Processing index 364...! Done\n",
      "Processing index 365...! Done\n",
      "Processing index 366...! Done\n",
      "Processing index 367...! Done\n",
      "Processing index 368...! Done\n",
      "Processing index 369...! Done\n",
      "Processing index 370...! Done\n",
      "Processing index 371...! Done\n",
      "Processing index 372...! Done\n",
      "Processing index 373...! Done\n",
      "Processing index 374...! Done\n",
      "Processing index 375...! Done\n",
      "Processing index 376...! Done\n",
      "Processing index 377...! Done\n",
      "Processing index 378...! Done\n",
      "Processing index 379...! Done\n",
      "Processing index 380...! Done\n",
      "Processing index 381...! Done\n",
      "Processing index 382...! Done\n",
      "Processing index 383...! Done\n",
      "Processing index 384...! Done\n",
      "Processing index 385...! Done\n",
      "Processing index 386...! Done\n",
      "Processing index 387...! Done\n",
      "Processing index 388...! Done\n",
      "Processing index 389...! Done\n",
      "Processing index 390...! Done\n",
      "Processing index 391...! Done\n",
      "Processing index 392...! Done\n",
      "Processing index 393...! Done\n",
      "Processing index 394...! Done\n",
      "Processing index 395...! Done\n",
      "Processing index 396...! Done\n",
      "Processing index 397...! Done\n",
      "Processing index 398...! Done\n",
      "Processing index 399...! Done\n",
      "Processing index 400...! Done\n",
      "Processing index 401...! Done\n",
      "Processing index 402...! Done\n",
      "Processing index 403...! Done\n",
      "Processing index 404...! Done\n",
      "Processing index 405...! Done\n",
      "Processing index 406...! Done\n",
      "Processing index 407...! Done\n",
      "Processing index 408...! Done\n",
      "Processing index 409...! Done\n",
      "Processing index 410...! Done\n",
      "Processing index 411...! Done\n",
      "Processing index 412...! Done\n",
      "Processing index 413...! Done\n",
      "Processing index 414...! Done\n",
      "Processing index 415...! Done\n",
      "Processing index 416...! Done\n",
      "Processing index 417...! Done\n",
      "Processing index 418...! Done\n",
      "Processing index 419...! Done\n",
      "Processing index 420...! Done\n",
      "Processing index 421...! Done\n",
      "Processing index 422...! Done\n",
      "Processing index 423...! Done\n",
      "Processing index 424...! Done\n",
      "Processing index 425...! Done\n",
      "Processing index 426...! Done\n",
      "Processing index 427...! Done\n",
      "Processing index 428...! Done\n",
      "Processing index 429...! Done\n",
      "Processing index 430...! Done\n",
      "Processing index 431...! Done\n",
      "Processing index 432...! Done\n",
      "Processing index 433...! Done\n",
      "Processing index 434...! Done\n",
      "Processing index 435...! Done\n",
      "Processing index 436...! Done\n",
      "Processing index 437...! Done\n",
      "Processing index 438...! Done\n",
      "Processing index 439...! Done\n",
      "Processing index 440...! Done\n",
      "Processing index 441...! Done\n",
      "Processing index 442...! Done\n",
      "Processing index 443...! Done\n",
      "Processing index 444...! Done\n",
      "Processing index 445...! Done\n",
      "Processing index 446...! Done\n",
      "Processing index 447...! Done\n",
      "Processing index 448...! Done\n",
      "Processing index 449...! Done\n",
      "Processing index 450...! Done\n",
      "Processing index 451...! Done\n",
      "Processing index 452...! Done\n",
      "Processing index 453...! Done\n",
      "Processing index 454...! Done\n",
      "Processing index 455...! Done\n",
      "Processing index 456...! Done\n",
      "Processing index 457...! Done\n",
      "Processing index 458...! Done\n",
      "Processing index 459...! Done\n",
      "Processing index 460...! Done\n",
      "Processing index 461...! Done\n",
      "Processing index 462...! Done\n",
      "Processing index 463...! Done\n",
      "Processing index 464...! Done\n",
      "Processing index 465...! Done\n",
      "Processing index 466...! Done\n",
      "Processing index 467...! Done\n",
      "Processing index 468...! Done\n",
      "Processing index 469...! Done\n",
      "Processing index 470...! Done\n",
      "Processing index 471...! Done\n",
      "Processing index 472...! Done\n",
      "Processing index 473...! Done\n",
      "Processing index 474...! Done\n",
      "Processing index 475...! Done\n",
      "Processing index 476...! Done\n",
      "Processing index 477...! Done\n",
      "Processing index 478...! Done\n",
      "Processing index 479...! Done\n",
      "Processing index 480...! Done\n",
      "Processing index 481...! Done\n",
      "Processing index 482...! Done\n",
      "Processing index 483...! Done\n",
      "Processing index 484...! Done\n",
      "Processing index 485...! Done\n",
      "Processing index 486...! Done\n",
      "Processing index 487...! Done\n",
      "Processing index 488...! Done\n",
      "Processing index 489...! Done\n",
      "Processing index 490...! Done\n",
      "Processing index 491...! Done\n",
      "Processing index 492...! Done\n",
      "Processing index 493...! Done\n",
      "Processing index 494...! Done\n",
      "Processing index 495...! Done\n",
      "Processing index 496...! Done\n",
      "Processing index 497...! Done\n",
      "Processing index 498...! Done\n",
      "Processing index 499...! Done\n",
      "Processing index 500...! Done\n",
      "Processing index 501...! Done\n",
      "Processing index 502...! Done\n",
      "Processing index 503...! Done\n",
      "Processing index 504...! Done\n",
      "Processing index 505...! Done\n",
      "Processing index 506...! Done\n",
      "Processing index 507...! Done\n",
      "Processing index 508...! Done\n",
      "Processing index 509...! Done\n",
      "Processing index 510...! Done\n",
      "Processing index 511...! Done\n",
      "Processing index 512...! Done\n",
      "Processing index 513...! Done\n",
      "Processing index 514...! Done\n",
      "Processing index 515...! Done\n",
      "Processing index 516...! Done\n",
      "Processing index 517...! Done\n",
      "Processing index 518...! Done\n",
      "Processing index 519...! Done\n",
      "Processing index 520...! Done\n",
      "Processing index 521...! Done\n",
      "Processing index 522...! Done\n",
      "Processing index 523...! Done\n",
      "Processing index 524...! Done\n",
      "Processing index 525...! Done\n",
      "Processing index 526...! Done\n",
      "Processing index 527...! Done\n",
      "Processing index 528...! Done\n",
      "Processing index 529...! Done\n",
      "Processing index 530...! Done\n",
      "Processing index 531...! Done\n",
      "Processing index 532...! Done\n",
      "Processing index 533...! Done\n",
      "Processing index 534...! Done\n",
      "Processing index 535...! Done\n",
      "Processing index 536...! Done\n",
      "Processing index 537...! Done\n",
      "Processing index 538...! Done\n",
      "Processing index 539...! Done\n",
      "Processing index 540...! Done\n",
      "Processing index 541...! Done\n",
      "Processing index 542...! Done\n",
      "Processing index 543...! Done\n",
      "Processing index 544...! Done\n",
      "Processing index 545...! Done\n",
      "Processing index 546...! Done\n",
      "Processing index 547...! Done\n",
      "Processing index 548...! Done\n",
      "Processing index 549...! Done\n",
      "Processing index 550...! Done\n",
      "Processing index 551...! Done\n",
      "Processing index 552...! Done\n",
      "Processing index 553...! Done\n",
      "Processing index 554...! Done\n",
      "Processing index 555...! Done\n",
      "Processing index 556...! Done\n",
      "Processing index 557...! Done\n",
      "Processing index 558...! Done\n",
      "Processing index 559...! Done\n",
      "Processing index 560...! Done\n",
      "Processing index 561...! Done\n",
      "Processing index 562...! Done\n",
      "Processing index 563...! Done\n",
      "Processing index 564...! Done\n",
      "Processing index 565...! Done\n",
      "Processing index 566...! Done\n",
      "Processing index 567...! Done\n",
      "Processing index 568...! Done\n",
      "Processing index 569...! Done\n",
      "Processing index 570...! Done\n",
      "Processing index 571...! Done\n",
      "Processing index 572...! Done\n",
      "Processing index 573...! Done\n",
      "Processing index 574...! Done\n",
      "Processing index 575...! Done\n",
      "Processing index 576...! Done\n",
      "Processing index 577...! Done\n",
      "Processing index 578...! Done\n",
      "Processing index 579...! Done\n",
      "Processing index 580...! Done\n",
      "Processing index 581...! Done\n",
      "Processing index 582...! Done\n",
      "Processing index 583...! Done\n",
      "Processing index 584...! Done\n",
      "Processing index 585...! Done\n",
      "Processing index 586...! Done\n",
      "Processing index 587...! Done\n",
      "Processing index 588...! Done\n",
      "Processing index 589...! Done\n",
      "Processing index 590...! Done\n",
      "Processing index 591...! Done\n",
      "Processing index 592...! Done\n",
      "Processing index 593...! Done\n",
      "Processing index 594...! Done\n",
      "Processing index 595...! Done\n",
      "Processing index 596...! Done\n",
      "Processing index 597...! Done\n",
      "Processing index 598...! Done\n",
      "Processing index 599...! Done\n",
      "Processing index 600...! Done\n",
      "Processing index 601...! Done\n",
      "Processing index 602...! Done\n",
      "Processing index 603...! Done\n",
      "Processing index 604...! Done\n",
      "Processing index 605...! Done\n",
      "Processing index 606...! Done\n",
      "Processing index 607...! Done\n",
      "Processing index 608...! Done\n",
      "Processing index 609...! Done\n",
      "Processing index 610...! Done\n",
      "Processing index 611...! Done\n",
      "Processing index 612...! Done\n",
      "Processing index 613...! Done\n",
      "Processing index 614...! Done\n",
      "Processing index 615...! Done\n",
      "Processing index 616...! Done\n",
      "Processing index 617...! Done\n",
      "Processing index 618...! Done\n",
      "Processing index 619...! Done\n",
      "Processing index 620...! Done\n",
      "Processing index 621...! Done\n",
      "Processing index 622...! Done\n",
      "Processing index 623...! Done\n",
      "Processing index 624...! Done\n",
      "Processing index 625...! Done\n",
      "Processing index 626...! Done\n",
      "Processing index 627...! Done\n",
      "Processing index 628...! Done\n",
      "Processing index 629...! Done\n",
      "Processing index 630...! Done\n",
      "Processing index 631...! Done\n",
      "Processing index 632...! Done\n",
      "Processing index 633...! Done\n",
      "Processing index 634...! Done\n",
      "Processing index 635...! Done\n",
      "Processing index 636...! Done\n",
      "Processing index 637...! Done\n",
      "Processing index 638...! Done\n",
      "Processing index 639...! Done\n",
      "Processing index 640...! Done\n",
      "Processing index 641...! Done\n",
      "Processing index 642...! Done\n",
      "Processing index 643...! Done\n",
      "Processing index 644...! Done\n",
      "Processing index 645...! Done\n",
      "Processing index 646...! Done\n",
      "Processing index 647...! Done\n",
      "Processing index 648...! Done\n",
      "Processing index 649...! Done\n",
      "Processing index 650...! Done\n",
      "Processing index 651...! Done\n",
      "Processing index 652...! Done\n",
      "Processing index 653...! Done\n",
      "Processing index 654...! Done\n",
      "Processing index 655...! Done\n",
      "Processing index 656...! Done\n",
      "Processing index 657...! Done\n",
      "Processing index 658...! Done\n",
      "Processing index 659...! Done\n",
      "Processing index 660...! Done\n",
      "Processing index 661...! Done\n",
      "Processing index 662...! Done\n",
      "Processing index 663...! Done\n",
      "Processing index 664...! Done\n",
      "Processing index 665...! Done\n",
      "Processing index 666...! Done\n",
      "Processing index 667...! Done\n",
      "Processing index 668...! Done\n",
      "Processing index 669...! Done\n",
      "Processing index 670...! Done\n",
      "Processing index 671...! Done\n",
      "Processing index 672...! Done\n",
      "Processing index 673...! Done\n",
      "Processing index 674...! Done\n",
      "Processing index 675...! Done\n",
      "Processing index 676...! Done\n",
      "Processing index 677...! Done\n",
      "Processing index 678...! Done\n",
      "Processing index 679...! Done\n",
      "Processing index 680...! Done\n",
      "Processing index 681...! Done\n",
      "Processing index 682...! Done\n",
      "Processing index 683...! Done\n",
      "Processing index 684...! Done\n",
      "Processing index 685...! Done\n",
      "Processing index 686...! Done\n",
      "Processing index 687...! Done\n",
      "Processing index 688...! Done\n",
      "Processing index 689...! Done\n",
      "Processing index 690...! Done\n",
      "Processing index 691...! Done\n",
      "Processing index 692...! Done\n",
      "Processing index 693...! Done\n",
      "Processing index 694...! Done\n",
      "Processing index 695...! Done\n",
      "Processing index 696...! Done\n",
      "Processing index 697...! Done\n",
      "Processing index 698...! Done\n",
      "Processing index 699...! Done\n",
      "Processing index 700...! Done\n",
      "Processing index 701...! Done\n",
      "Processing index 702...! Done\n",
      "Processing index 703...! Done\n",
      "Processing index 704...! Done\n",
      "Processing index 705...! Done\n",
      "Processing index 706...! Done\n",
      "Processing index 707...! Done\n",
      "Processing index 708...! Done\n",
      "Processing index 709...! Done\n",
      "Processing index 710...! Done\n",
      "Processing index 711...! Done\n",
      "Processing index 712...! Done\n",
      "Processing index 713...! Done\n",
      "Processing index 714...! Done\n",
      "Processing index 715...! Done\n",
      "Processing index 716...! Done\n",
      "Processing index 717...! Done\n",
      "Processing index 718...! Done\n",
      "Processing index 719...! Done\n",
      "Processing index 720...! Done\n",
      "Processing index 721...! Done\n",
      "Processing index 722...! Done\n",
      "Processing index 723...! Done\n",
      "Processing index 724...! Done\n",
      "Processing index 725...! Done\n",
      "Processing index 726...! Done\n",
      "Processing index 727...! Done\n",
      "Processing index 728...! Done\n",
      "Processing index 729...! Done\n",
      "Processing index 730...! Done\n",
      "Processing index 731...! Done\n",
      "Processing index 732...! Done\n",
      "Processing index 733...! Done\n",
      "Processing index 734...! Done\n",
      "Processing index 735...! Done\n",
      "Processing index 736...! Done\n",
      "Processing index 737...! Done\n",
      "Processing index 738...! Done\n",
      "Processing index 739...! Done\n",
      "Processing index 740...! Done\n",
      "Processing index 741...! Done\n",
      "Processing index 742...! Done\n",
      "Processing index 743...! Done\n",
      "Processing index 744...! Done\n",
      "Processing index 745...! Done\n",
      "Processing index 746...! Done\n",
      "Processing index 747...! Done\n",
      "Processing index 748...! Done\n",
      "Processing index 749...! Done\n",
      "Processing index 750...! Done\n",
      "Processing index 751...! Done\n",
      "Processing index 752...! Done\n",
      "Processing index 753...! Done\n",
      "Processing index 754...! Done\n",
      "Processing index 755...! Done\n",
      "Processing index 756...! Done\n",
      "Processing index 757...! Done\n",
      "Processing index 758...! Done\n",
      "Processing index 759...! Done\n",
      "Processing index 760...! Done\n",
      "Processing index 761...! Done\n",
      "Processing index 762...! Done\n",
      "Processing index 763...! Done\n",
      "Processing index 764...! Done\n",
      "Processing index 765...! Done\n",
      "Processing index 766...! Done\n",
      "Processing index 767...! Done\n",
      "Processing index 768...! Done\n",
      "Processing index 769...! Done\n",
      "Processing index 770...! Done\n",
      "Processing index 771...! Done\n",
      "Processing index 772...! Done\n",
      "Processing index 773...! Done\n",
      "Processing index 774...! Done\n",
      "Processing index 775...! Done\n",
      "Processing index 776...! Done\n",
      "Processing index 777...! Done\n",
      "Processing index 778...! Done\n",
      "Processing index 779...! Done\n",
      "Processing index 780...! Done\n",
      "Processing index 781...! Done\n",
      "Processing index 782...! Done\n",
      "Processing index 783...! Done\n",
      "Processing index 784...! Done\n",
      "Processing index 785...! Done\n",
      "Processing index 786...! Done\n",
      "Processing index 787...! Done\n",
      "Processing index 788...! Done\n",
      "Processing index 789...! Done\n",
      "Processing index 790...! Done\n",
      "Processing index 791...! Done\n",
      "Processing index 792...! Done\n",
      "Processing index 793...! Done\n",
      "Processing index 794...! Done\n",
      "Processing index 795...! Done\n",
      "Processing index 796...! Done\n",
      "Processing index 797...! Done\n",
      "Processing index 798...! Done\n",
      "Processing index 799...! Done\n",
      "Processing index 800...! Done\n",
      "Processing index 801...! Done\n",
      "Processing index 802...! Done\n",
      "Processing index 803...! Done\n",
      "Processing index 804...! Done\n",
      "Processing index 805...! Done\n",
      "Processing index 806...! Done\n",
      "Processing index 807...! Done\n",
      "Processing index 808...! Done\n",
      "Processing index 809...! Done\n",
      "Processing index 810...! Done\n",
      "Processing index 811...! Done\n",
      "Processing index 812...! Done\n",
      "Processing index 813...! Done\n",
      "Processing index 814...! Done\n",
      "Processing index 815...! Done\n",
      "Processing index 816...! Done\n",
      "Processing index 817...! Done\n",
      "Processing index 818...! Done\n",
      "Processing index 819...! Done\n",
      "Processing index 820...! Done\n",
      "Processing index 821...! Done\n",
      "Processing index 822...! Done\n",
      "Processing index 823...! Done\n",
      "Processing index 824...! Done\n",
      "Processing index 825...! Done\n",
      "Processing index 826...! Done\n",
      "Processing index 827...! Done\n",
      "Processing index 828...! Done\n",
      "Processing index 829...! Done\n",
      "Processing index 830...! Done\n",
      "Processing index 831...! Done\n",
      "Processing index 832...! Done\n",
      "Processing index 833...! Done\n",
      "Processing index 834...! Done\n",
      "Processing index 835...! Done\n",
      "Processing index 836...! Done\n",
      "Processing index 837...! Done\n",
      "Processing index 838...! Done\n",
      "Processing index 839...! Done\n",
      "Processing index 840...! Done\n",
      "Processing index 841...! Done\n",
      "Processing index 842...! Done\n",
      "Processing index 843...! Done\n",
      "Processing index 844...! Done\n",
      "Processing index 845...! Done\n",
      "Processing index 846...! Done\n",
      "Processing index 847...! Done\n",
      "Processing index 848...! Done\n",
      "Processing index 849...! Done\n",
      "Processing index 850...! Done\n",
      "Processing index 851...! Done\n",
      "Processing index 852...! Done\n",
      "Processing index 853...! Done\n",
      "Processing index 854...! Done\n",
      "Processing index 855...! Done\n",
      "Processing index 856...! Done\n",
      "Processing index 857...! Done\n",
      "Processing index 858...! Done\n",
      "Processing index 859...! Done\n",
      "Processing index 860...! Done\n",
      "Processing index 861...! Done\n",
      "Processing index 862...! Done\n",
      "Processing index 863...! Done\n",
      "Processing index 864...! Done\n",
      "Processing index 865...! Done\n",
      "Processing index 866...! Done\n",
      "Processing index 867...! Done\n",
      "Processing index 868...! Done\n",
      "Processing index 869...! Done\n",
      "Processing index 870...! Done\n",
      "Processing index 871...! Done\n",
      "Processing index 872...! Done\n",
      "Processing index 873...! Done\n",
      "Processing index 874...! Done\n",
      "Processing index 875...! Done\n",
      "Processing index 876...! Done\n",
      "Processing index 877...! Done\n",
      "Processing index 878...! Done\n",
      "Processing index 879...! Done\n",
      "Processing index 880...! Done\n",
      "Processing index 881...! Done\n",
      "Processing index 882...! Done\n",
      "Processing index 883...! Done\n",
      "Processing index 884...! Done\n",
      "Processing index 885...! Done\n",
      "Processing index 886...! Done\n",
      "Processing index 887...! Done\n",
      "Processing index 888...! Done\n",
      "Processing index 889...! Done\n",
      "Processing index 890...! Done\n",
      "Processing index 891...! Done\n",
      "Processing index 892...! Done\n",
      "Processing index 893...! Done\n",
      "Processing index 894...! Done\n",
      "Processing index 895...! Done\n",
      "Processing index 896...! Done\n",
      "Processing index 897...! Done\n",
      "Processing index 898...! Done\n",
      "Processing index 899...! Done\n",
      "Processing index 900...! Done\n",
      "Processing index 901...! Done\n",
      "Processing index 902...! Done\n",
      "Processing index 903...! Done\n",
      "Processing index 904...! Done\n",
      "Processing index 905...! Done\n",
      "Processing index 906...! Done\n",
      "Processing index 907...! Done\n",
      "Processing index 908...! Done\n",
      "Processing index 909...! Done\n",
      "Processing index 910...! Done\n",
      "Processing index 911...! Done\n",
      "Processing index 912...! Done\n",
      "Processing index 913...! Done\n",
      "Processing index 914...! Done\n",
      "Processing index 915...! Done\n",
      "Processing index 916...! Done\n",
      "Processing index 917...! Done\n",
      "Processing index 918...! Done\n",
      "Processing index 919...! Done\n",
      "Processing index 920...! Done\n",
      "Processing index 921...! Done\n",
      "Processing index 922...! Done\n",
      "Processing index 923...! Done\n",
      "Processing index 924...! Done\n",
      "Processing index 925...! Done\n",
      "Processing index 926...! Done\n",
      "Processing index 927...! Done\n",
      "Processing index 928...! Done\n",
      "Processing index 929...! Done\n",
      "Processing index 930...! Done\n",
      "Processing index 931...! Done\n",
      "Processing index 932...! Done\n",
      "Processing index 933...! Done\n",
      "Processing index 934...! Done\n",
      "Processing index 935...! Done\n",
      "Processing index 936...! Done\n",
      "Processing index 937...! Done\n",
      "Processing index 938...! Done\n",
      "Processing index 939...! Done\n",
      "Processing index 940...! Done\n",
      "Processing index 941...! Done\n",
      "Processing index 942...! Done\n",
      "Processing index 943...! Done\n",
      "Processing index 944...! Done\n",
      "Processing index 945...! Done\n",
      "Processing index 946...! Done\n",
      "Processing index 947...! Done\n",
      "Processing index 948...! Done\n",
      "Processing index 949...! Done\n",
      "Processing index 950...! Done\n",
      "Processing index 951...! Done\n",
      "Processing index 952...! Done\n",
      "Processing index 953...! Done\n",
      "Processing index 954...! Done\n",
      "Processing index 955...! Done\n",
      "Processing index 956...! Done\n",
      "Processing index 957...! Done\n",
      "Processing index 958...! Done\n",
      "Processing index 959...! Done\n",
      "Processing index 960...! Done\n",
      "Processing index 961...! Done\n",
      "Processing index 962...! Done\n",
      "Processing index 963...! Done\n",
      "Processing index 964...! Done\n",
      "Processing index 965...! Done\n",
      "Processing index 966...! Done\n",
      "Processing index 967...! Done\n",
      "Processing index 968...! Done\n",
      "Processing index 969...! Done\n",
      "Processing index 970...! Done\n",
      "Processing index 971...! Done\n",
      "Processing index 972...! Done\n",
      "Processing index 973...! Done\n",
      "Processing index 974...! Done\n",
      "Processing index 975...! Done\n",
      "Processing index 976...! Done\n",
      "Processing index 977...! Done\n",
      "Processing index 978...! Done\n",
      "Processing index 979...! Done\n",
      "Processing index 980...! Done\n",
      "Processing index 981...! Done\n",
      "Processing index 982...! Done\n",
      "Processing index 983...! Done\n",
      "Processing index 984...! Done\n",
      "Processing index 985...! Done\n",
      "Processing index 986...! Done\n",
      "Processing index 987...! Done\n",
      "Processing index 988...! Done\n",
      "Processing index 989...! Done\n",
      "Processing index 990...! Done\n",
      "Processing index 991...! Done\n",
      "Processing index 992...! Done\n",
      "Processing index 993...! Done\n",
      "Processing index 994...! Done\n",
      "Processing index 995...! Done\n",
      "Processing index 996...! Done\n",
      "Processing index 997...! Done\n",
      "Processing index 998...! Done\n",
      "Processing index 999...! Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>key_4</th>\n",
       "      <th>key_5</th>\n",
       "      <th>key_6</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_1</th>\n",
       "      <th>pos_2</th>\n",
       "      <th>pos_3</th>\n",
       "      <th>pos_4</th>\n",
       "      <th>pos_5</th>\n",
       "      <th>pos_6</th>\n",
       "      <th>pos_7</th>\n",
       "      <th>pos_8</th>\n",
       "      <th>pos_9</th>\n",
       "      <th>pos_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>organisasi</td>\n",
       "      <td>fase</td>\n",
       "      <td>pengganti</td>\n",
       "      <td>tender</td>\n",
       "      <td>kandidat personil penting</td>\n",
       "      <td>kualifikasi personil penting</td>\n",
       "      <td>usulan personil penting</td>\n",
       "      <td>personil penting</td>\n",
       "      <td>pergantian personil penting</td>\n",
       "      <td>personil</td>\n",
       "      <td>...</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acuan</td>\n",
       "      <td>processing</td>\n",
       "      <td>ctr</td>\n",
       "      <td>tiung</td>\n",
       "      <td>coordination</td>\n",
       "      <td>exhibit</td>\n",
       "      <td>facilities</td>\n",
       "      <td>pengelolaan</td>\n",
       "      <td>jambaran</td>\n",
       "      <td>procedure</td>\n",
       "      <td>...</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>FW</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kantor</td>\n",
       "      <td>ruangan</td>\n",
       "      <td>usulan</td>\n",
       "      <td>scope</td>\n",
       "      <td>artikel</td>\n",
       "      <td>klarifikasi</td>\n",
       "      <td>lokasi</td>\n",
       "      <td>accommodation</td>\n",
       "      <td>diskusi</td>\n",
       "      <td>tabel</td>\n",
       "      <td>...</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>FW</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        key_1       key_2      key_3   key_4                      key_5  \\\n",
       "0  organisasi        fase  pengganti  tender  kandidat personil penting   \n",
       "1       acuan  processing        ctr   tiung               coordination   \n",
       "2      kantor     ruangan     usulan   scope                    artikel   \n",
       "\n",
       "                          key_6                    key_7             key_8  \\\n",
       "0  kualifikasi personil penting  usulan personil penting  personil penting   \n",
       "1                       exhibit               facilities       pengelolaan   \n",
       "2                   klarifikasi                   lokasi     accommodation   \n",
       "\n",
       "                         key_9     key_10  ...  pos_1  pos_2  pos_3  pos_4  \\\n",
       "0  pergantian personil penting   personil  ...     NN     NN     NN     NN   \n",
       "1                     jambaran  procedure  ...     NN     NN     NN     NN   \n",
       "2                      diskusi      tabel  ...     NN     NN     NN     FW   \n",
       "\n",
       "   pos_5  pos_6  pos_7  pos_8  pos_9  pos_10  \n",
       "0     NP     NP     NP     NP     NP      NN  \n",
       "1     FW     NN     NN     NN     NN      NN  \n",
       "2     NN     NN     NN     NN     NN      NN  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_textrank = pd.DataFrame()\n",
    "for i in df.index:\n",
    "#for i in df.loc[2:5].index:\n",
    "    print('Processing index', i, end='...! ')\n",
    "    # Processing Text\n",
    "    text = df[\"text\"][i]\n",
    "\n",
    "    # FITUR-1 Fungsi Phrase Detection\n",
    "    words = detect_all_tokens(text)\n",
    "    # Postagging\n",
    "    pos_tokens = postag_tokens(words)\n",
    "\n",
    "    # FITUR-2 dan FUNGSI KOMPUTASI TEXTRANK\n",
    "    # Building Co-occurrence Matrix\n",
    "    co_occurrence = build_co_occurrence_matrix(words)\n",
    "    # Building Graph and Computing TextRank\n",
    "    scores_w_stopwords = build_graph_and_compute_textrank(co_occurrence, w2v_model)\n",
    "    #scores_w_stopwords = F2_graph_edgeweight_and_textrank(co_occurrence, ft_model)\n",
    "\n",
    "    # clean stopword\n",
    "    scores_w_F12 = filter_stopwords(scores_w_stopwords, stopwords)\n",
    "\n",
    "    # FITUR 3_POS_filter\n",
    "    selected_pos_tags = {'NN', 'NNP', 'VB', 'NP', 'VP', 'FW'}\n",
    "    scores_w_F123 = F3_scores_w_pos_filter_no_filter(scores_w_F12, apply_filter=True, selected_pos_tags=selected_pos_tags)\n",
    "    #scores_w_F123 = F3_scores_w_pos_filter_no_filter(scores_w_F12, apply_filter=True, selected_pos_tags=None)\n",
    "\n",
    "    # FITUR 4_posisi_kata = yes, judul * 2\n",
    "    title = df[\"judul\"][1]# Replace with actual title\n",
    "    scores_w_F1234 = F4_scores_weight_for_title(scores_w_F123, title)\n",
    "\n",
    "    # FITUR-5 phrase_weight\n",
    "    scores_w_F12345 = F5_scores_weight_for_phrase(scores_w_F1234)\n",
    "\n",
    "    # FITUR-6 TFIDF_weight\n",
    "    corpus = df[\"text\"]\n",
    "    scores_w_F123456 = F6_scores_tfidf(scores_w_F12345, text, corpus)\n",
    "\n",
    "    df_keyphrases = format_keyphrases_dataframe(scores_w_F123456)\n",
    "\n",
    "    predict_textrank = pd.concat([predict_textrank, df_keyphrases], ignore_index=True)\n",
    "    print('Done')\n",
    "\n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(preds_row, targets_row):\n",
    "    result = []\n",
    "    strict_accum = 0\n",
    "    strict_prec = 0\n",
    "    flex_accum = 0\n",
    "    flex_prec = 0\n",
    "\n",
    "    for pred in preds_row:\n",
    "        pred = str(pred).lower().strip()\n",
    "        state = 'no_match'\n",
    "\n",
    "        for target in targets_row:\n",
    "            target = str(target).lower().strip()\n",
    "\n",
    "            if pred == target:\n",
    "                state = 'full_match'\n",
    "                flex_accum += 1\n",
    "                strict_accum += 1\n",
    "                break\n",
    "            elif pred in target:\n",
    "                state = 'partial_match'\n",
    "                flex_accum += 1\n",
    "                strict_accum += 0.5\n",
    "                break\n",
    "            # perlu ditambahkan jika pred ada terdiri dari bbrp kata, dan katanya berisisan dengan target, maka menjadi partial match\n",
    "            '''\n",
    "            elif target in pred:\n",
    "                state = 'partial_match'\n",
    "                flex_accum += 1\n",
    "                strict_accum += 0.5\n",
    "            '''\n",
    "        result.append(state)\n",
    "\n",
    "    len_targets_row = len(targets_row)\n",
    "    len_preds_row = len(preds_row)\n",
    "\n",
    "    if len_preds_row != 0:\n",
    "        flex_recall = flex_accum / len_targets_row\n",
    "        flex_prec = flex_accum / len_preds_row\n",
    "\n",
    "    if len_targets_row != 0:\n",
    "        strict_recall = strict_accum / len_targets_row\n",
    "        strict_prec = strict_accum / len_preds_row\n",
    "\n",
    "    result.extend([strict_recall, strict_prec, flex_recall, flex_prec])\n",
    "\n",
    "    return result\n",
    "\n",
    "def eval(predictions, targets, to_dataframe = False):\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "      result.append(check_similarity(predictions[i], targets[i]))\n",
    "    \n",
    "    if to_dataframe:\n",
    "        return pd.DataFrame(result)\n",
    "    else:\n",
    "        return result\n",
    "    \n",
    "def evaluate_prediction(predict_list, targets_list):\n",
    "    '''\n",
    "    fungsi untuk membandingkan hasil prediksi dengan gold truth\n",
    "    dengan hasil exact match =1, partial match = 1, no match = 0, utk kemudian dihitung per row\n",
    "    '''\n",
    "    evaluation = eval(predict_list, targets_list, True).round(3)\n",
    "    evaluation.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "    evaluation = evaluation[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']]\n",
    "    return evaluation\n",
    "\n",
    "def summarize_evaluation(eval_method):\n",
    "    '''\n",
    "    fungsi untuk menghitung score metric recall, precision, dan F1\n",
    "    dengan paramater flexible : exact match =1, partial match = 1, no match = 0\n",
    "    hasil dalam bentuk dataframe summary\n",
    "    '''\n",
    "    recall = eval_method['flex_recall'].mean()\n",
    "    prec = eval_method['flex_prec'].mean()\n",
    "    f1 = 2 * (prec * recall) / (prec + recall)\n",
    "\n",
    "    summary = pd.DataFrame({'metric': [recall, prec, f1]}, index=['recall', 'precision', 'F1'])\n",
    "    summary = summary.round(3)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import eval\n",
    "targets = df[[\"k1\", \"k2\", \"k3\",\"k4\", \"k5\", \"k6\",\"k7\"]].values.tolist()\n",
    "df_targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation TextRank top 10\n",
    "predict_textrank_list_10 = predict_textrank[['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10']].values.tolist()\n",
    "eval_textrank_10 = eval(predict_textrank_list_10, targets, True).round(3)\n",
    "eval_textrank_10.columns = ['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_10 = eval_textrank_10[['key_1','key_2','key_3', 'key_4','key_5','key_6', 'key_7','key_8','key_9', 'key_10', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_10.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_10 = eval_textrank_10['flex_recall'].mean()\n",
    "textrank_prec_10 = eval_textrank_10['flex_prec'].mean()\n",
    "textrank_f1_10 = 2 * (textrank_prec_10 * textrank_recall_10) / (textrank_prec_10 + textrank_recall_10)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_10 = pd.DataFrame({'textrank': [textrank_recall_10, textrank_prec_10, textrank_f1_10]}, index=['recall', 'precision', 'F1'])\n",
    "summary_10 = summary_10.round(3)\n",
    "#summary_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation TextRank top 5\n",
    "predict_textrank_list_5 = predict_textrank[['key_1','key_2','key_3', 'key_4','key_5']].values.tolist()\n",
    "eval_textrank_5 = eval(predict_textrank_list_5, targets, True).round(3)\n",
    "eval_textrank_5.columns = ['key_1','key_2','key_3', 'key_4','key_5','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_5 = eval_textrank_5[['key_1','key_2','key_3', 'key_4','key_5', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_5.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_5 = eval_textrank_5['flex_recall'].mean()\n",
    "textrank_prec_5 = eval_textrank_5['flex_prec'].mean()\n",
    "textrank_f1_5 = 2 * (textrank_prec_5 * textrank_recall_5) / (textrank_prec_5 + textrank_recall_5)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_5 = pd.DataFrame({'textrank': [textrank_recall_5, textrank_prec_5, textrank_f1_5]}, index=['recall', 'precision', 'F1'])\n",
    "summary_5 = summary_5.round(3)\n",
    "#summary_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textrank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textrank\n",
       "recall        0.130\n",
       "precision     0.302\n",
       "F1            0.181"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation TextRank top 3\n",
    "predict_textrank_list_3 = predict_textrank[['key_1','key_2','key_3']].values.tolist()\n",
    "eval_textrank_3 = eval(predict_textrank_list_3, targets, True).round(3)\n",
    "eval_textrank_3.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank_3 = eval_textrank_3[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "#eval_textrank_3.head(3)\n",
    "\n",
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall_3 = eval_textrank_3['flex_recall'].mean()\n",
    "textrank_prec_3 = eval_textrank_3['flex_prec'].mean()\n",
    "textrank_f1_3 = 2 * (textrank_prec_3 * textrank_recall_3) / (textrank_prec_3 + textrank_recall_3)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary_3 = pd.DataFrame({'textrank': [textrank_recall_3, textrank_prec_3, textrank_f1_3]}, index=['recall', 'precision', 'F1'])\n",
    "summary_3 = summary_3.round(3)\n",
    "summary_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>key_4</th>\n",
       "      <th>key_5</th>\n",
       "      <th>key_6</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>...</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>organisasi</td>\n",
       "      <td>fase</td>\n",
       "      <td>pengganti</td>\n",
       "      <td>tender</td>\n",
       "      <td>kandidat personil penting</td>\n",
       "      <td>kualifikasi personil penting</td>\n",
       "      <td>usulan personil penting</td>\n",
       "      <td>personil penting</td>\n",
       "      <td>pergantian personil penting</td>\n",
       "      <td>personil</td>\n",
       "      <td>...</td>\n",
       "      <td>usulan</td>\n",
       "      <td>pengganti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acuan</td>\n",
       "      <td>processing</td>\n",
       "      <td>ctr</td>\n",
       "      <td>tiung</td>\n",
       "      <td>coordination</td>\n",
       "      <td>exhibit</td>\n",
       "      <td>facilities</td>\n",
       "      <td>pengelolaan</td>\n",
       "      <td>jambaran</td>\n",
       "      <td>procedure</td>\n",
       "      <td>...</td>\n",
       "      <td>acuan</td>\n",
       "      <td>pengelolaan</td>\n",
       "      <td>dokumen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>full_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kantor</td>\n",
       "      <td>ruangan</td>\n",
       "      <td>usulan</td>\n",
       "      <td>scope</td>\n",
       "      <td>artikel</td>\n",
       "      <td>klarifikasi</td>\n",
       "      <td>lokasi</td>\n",
       "      <td>accommodation</td>\n",
       "      <td>diskusi</td>\n",
       "      <td>tabel</td>\n",
       "      <td>...</td>\n",
       "      <td>lingkup kerja</td>\n",
       "      <td>akomodasi</td>\n",
       "      <td>services for company</td>\n",
       "      <td>exhibit a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        key_1       key_2      key_3   key_4                      key_5  \\\n",
       "0  organisasi        fase  pengganti  tender  kandidat personil penting   \n",
       "1       acuan  processing        ctr   tiung               coordination   \n",
       "2      kantor     ruangan     usulan   scope                    artikel   \n",
       "\n",
       "                          key_6                    key_7             key_8  \\\n",
       "0  kualifikasi personil penting  usulan personil penting  personil penting   \n",
       "1                       exhibit               facilities       pengelolaan   \n",
       "2                   klarifikasi                   lokasi     accommodation   \n",
       "\n",
       "                         key_9     key_10  ...              2            3  \\\n",
       "0  pergantian personil penting   personil  ...         usulan    pengganti   \n",
       "1                     jambaran  procedure  ...          acuan  pengelolaan   \n",
       "2                      diskusi      tabel  ...  lingkup kerja    akomodasi   \n",
       "\n",
       "                      4          5    6          key_1     key_2       key_3  \\\n",
       "0                   NaN        NaN  NaN       no_match  no_match  full_match   \n",
       "1               dokumen        NaN  NaN     full_match  no_match    no_match   \n",
       "2  services for company  exhibit a  NaN  partial_match  no_match    no_match   \n",
       "\n",
       "   flex_recall  flex_prec  \n",
       "0        0.143      0.333  \n",
       "1        0.143      0.333  \n",
       "2        0.143      0.333  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_10 = pd.concat([predict_textrank, df_targets, eval_textrank_10], axis=1)\n",
    "#predict_textrank_10.head(3)\n",
    "\n",
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_5 = pd.concat([predict_textrank, df_targets, eval_textrank_5], axis=1)\n",
    "#predict_textrank_5.head(3)\n",
    "\n",
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank_3 = pd.concat([predict_textrank, df_targets, eval_textrank_3], axis=1)\n",
    "predict_textrank_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def write_excel(df, sheet_name, filename):\n",
    "    \"\"\"\n",
    "    Writes the given dataframe to an excel file with the given filename and sheet name.\n",
    "    If the sheet already exists in the file, the data in the sheet will be overwritten.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to load the existing workbook\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl', mode='a') as writer:\n",
    "            if sheet_name in writer.book.sheetnames:\n",
    "                # If sheet already exists, remove it\n",
    "                sheet = writer.book[sheet_name]\n",
    "                writer.book.remove(sheet)\n",
    "\n",
    "            # Write the dataframe to the excel file\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If the file doesn't exist, create a new workbook\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl', mode='w') as writer:\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to excel file\n",
    "#from utils import write_excel\n",
    "\n",
    "sheet_name_10 = 'SE22_wv2_adjusted_textrank_10'\n",
    "sheet_name_5 = 'SE22_wv2_adjusted_textrank_5'\n",
    "sheet_name_3 = 'SE22_wv2_adjusted_textrank_3'\n",
    "\n",
    "output_file = 'SE22_wv2_adjusted_textrank.xlsx'\n",
    "write_excel(predict_textrank_10, sheet_name_10, output_file)\n",
    "write_excel(predict_textrank_5, sheet_name_5, output_file)\n",
    "write_excel(predict_textrank_3, sheet_name_3, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDITIONAL UTILITIES \n",
    "pilihan: \n",
    "- menunjukan gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "'''\n",
    "def get_unique_tokens_pos(all_tokens, pos_model_path):\n",
    "    \"\"\"\n",
    "    Get unique POS tags for tokens.\n",
    "    \"\"\"\n",
    "    pos_model_path = os.path.join(repo_root, \"notebooks/nlp-id_retraining/train_tuned.pkl\")\n",
    "    postagger = PosTag(pos_model_path)\n",
    "    pos_tokens = []\n",
    "    seen_tokens = set()\n",
    "    \n",
    "    for token in all_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            tokens_pos = postagger.get_phrase_tag(token)\n",
    "            pos_tokens.append(tokens_pos)\n",
    "    return pos_tokens\n",
    "'''\n",
    "'''\n",
    "# Fungsi Visualisasi\n",
    "def visualize_graph(G, labels):\n",
    "    # Remove self-loops (edges that connect a node to itself)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    #nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\", node_size=5000, node_color='skyblue')\n",
    "    nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\")\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=12)\n",
    "    plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def generate_ngrams(words, n=2):\n",
    "    \"\"\"Generate ngrams from a list of words.\"\"\"\n",
    "    return [\" \".join(gram) for gram in ngrams(words, n)]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
