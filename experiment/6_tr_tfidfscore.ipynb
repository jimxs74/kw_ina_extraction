{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. rutin1 import module\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. rutin2 membuat syspath ke root utk aktifkan __init__.py\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(repo_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. rutin3 Load the dataset\n",
    "#dataset_path = os.path.join(repo_root, \"notebooks/postager_nlp-id/dataset_ekstraksi_r29_pos_sm.xlsx\")\n",
    "dataset_path = os.path.join(repo_root, \"data/dataset_ekstraksi_r29_lg.xlsx\")\n",
    "df = pd.read_excel(dataset_path)\n",
    "df[\"text\"] = df[\"judul\"] +\". \"+ df[\"isi\"]\n",
    "#df_pos = df['pos_sentence_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^a-zA-Z.]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df['text'].apply(preprocess)\n",
    "df[\"judul\"] = df[\"judul\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(words, n=2):\n",
    "    \"\"\"Generate ngrams from a list of words.\"\"\"\n",
    "    return [\" \".join(gram) for gram in ngrams(words, n)]\n",
    "'''\n",
    "def get_phrase_embedding(phrase, w2v_model):\n",
    "    \"\"\"Get the averaged word embedding for a phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "'''\n",
    "\n",
    "def get_weighted_phrase_embedding(phrase, w2v_model, text_corpus):\n",
    "    \"\"\"\n",
    "    Generate a TF-IDF weighted averaged word embedding for a given phrase.\n",
    "    \"\"\"\n",
    "    # Generate TF-IDF dictionary\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text_corpus)\n",
    "    tfidf_dict = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "    \n",
    "    # Get phrase embedding\n",
    "    words = phrase.split()\n",
    "    embeddings = []\n",
    "    weights = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in w2v_model.wv.key_to_index:\n",
    "            embedding = w2v_model.wv[word]\n",
    "            tfidf_weight = tfidf_dict.get(word, 1.0)  # Default weight is 1 if word not in tfidf_dict\n",
    "            embeddings.append(embedding)\n",
    "            weights.append(tfidf_weight)\n",
    "    \n",
    "    if embeddings:\n",
    "        embeddings = np.array(embeddings)\n",
    "        weights = np.array(weights)\n",
    "        weighted_average_embedding = np.average(embeddings, axis=0, weights=weights)\n",
    "        return weighted_average_embedding\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nlp_id_local.tokenizer import PhraseTokenizer \n",
    "from nlp_id_local.postag import PosTag\n",
    "\n",
    "\n",
    "model_path = os.path.join(repo_root, \"notebooks/nlp-id_retraining/train_tuned.pkl\") #add_8\n",
    "\n",
    "def detect_bigram(text, available_tokens,):\n",
    "    \n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only bigrams whose individual words are in available_tokens\n",
    "    bigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 1 and all(word in available_tokens for word in phrase.split())]\n",
    "\n",
    "    return bigrams_only\n",
    "\n",
    "def detect_trigram(text, available_tokens):\n",
    "\n",
    "    tokenizer = PhraseTokenizer()\n",
    "    phrases = tokenizer.tokenize(text)\n",
    "    # Include only trigrams whose individual words are in available_tokens\n",
    "    trigrams_only = [phrase for phrase in phrases if phrase.count(\" \") == 2 and all(word in available_tokens for word in phrase.split())]\n",
    "\n",
    "    return trigrams_only\n",
    "\n",
    "def get_unique_tokens_pos(all_tokens, model_path):\n",
    "    \"\"\"\n",
    "    Get unique POS tags for tokens.\n",
    "    \"\"\"\n",
    "    postagger = PosTag(model_path)\n",
    "    pos_tokens = []\n",
    "    seen_tokens = set()\n",
    "    \n",
    "    for token in all_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            tokens_pos = postagger.get_phrase_tag(token)\n",
    "            pos_tokens.append(tokens_pos)\n",
    "    return pos_tokens\n",
    "\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists into a single list.\n",
    "    \"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "\n",
    "def filter_tokens_by_pos(flat_tokens, pos_filters):\n",
    "    \"\"\"\n",
    "    Filter tokens based on their POS tags and ensure they're unique.\n",
    "    \"\"\"\n",
    "    seen_tokens = set()\n",
    "    return [token[0] for token in flat_tokens if token[1] in pos_filters and not (token[0] in seen_tokens or seen_tokens.add(token[0]))]\n",
    "\n",
    "# Function to determine if a token is a unigram, bigram, or trigram\n",
    "def get_ngram_type(token):\n",
    "    return len(token.split())\n",
    "\n",
    "def extract_keyphrases_with_ngrams_graph(text, w2v_model, judul, available_tokens, n=3):\n",
    "    # Read stopwords from the file\n",
    "    #stopwords_path = os.path.join(repo_root, \"data/all_stop_words.txt\") \n",
    "    stopwords_path = os.path.join(repo_root, \"notebooks/stopwords_tuning/all_stop_words.txt\")\n",
    "    with open(stopwords_path, 'r') as file:\n",
    "        stopwords = set(file.read().strip().splitlines())\n",
    "\n",
    "    # Tokenize the text into unigrams\n",
    "    #unigrams = [word for word in text.split() if word not in stopwords]\n",
    "\n",
    "    # Tokenize the text into unigrams that are in available_tokens\n",
    "    unigrams = [word for word in text.split() if word not in stopwords and word in available_tokens]\n",
    "\n",
    "    # Generate bigrams and trigrams using nlp-id\n",
    "    bigrams = detect_bigram(text, available_tokens)\n",
    "    trigrams = detect_trigram(text, available_tokens)\n",
    "    \n",
    "    # Combine unigrams, filtered bigrams, and filtered trigrams\n",
    "    all_tokens = unigrams + bigrams + trigrams\n",
    "\n",
    "    # Filter tokens only for selected POS\n",
    "    pos_tokens = get_unique_tokens_pos(all_tokens, model_path)\n",
    "    flat_pos_tokens = flatten_list_of_lists(pos_tokens)\n",
    "    selected_pos = {'NN', 'NNP', 'VB', 'NP', 'VP'} # FW di exclude\n",
    "    filtered_tokens = filter_tokens_by_pos(flat_pos_tokens, selected_pos)\n",
    "\n",
    "    # Get embeddings for each token (averaging word embeddings for bigrams/trigrams)\n",
    "    token_embeddings = [get_weighted_phrase_embedding(token, w2v_model, text) for token in filtered_tokens]\n",
    "    \n",
    "    # Filter out tokens that don't have embeddings\n",
    "    tokens, embeddings = zip(*[(token, emb) for token, emb in zip(filtered_tokens, token_embeddings) if emb is not None])\n",
    "    # todo : masih ada token bahasa asing atau token aneh yg lolos. \n",
    "\n",
    "    # Compute the cosine similarity between token embeddings\n",
    "    cosine_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Create a graph and connect tokens with high similarity\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            if cosine_matrix[i][j] > 0.5:  # This threshold can be adjusted\n",
    "                G.add_edge(tokens[i], tokens[j], weight=cosine_matrix[i][j])\n",
    "    \n",
    "    # Create labels dictionary using the tokens\n",
    "    labels = {token: token for token in tokens}\n",
    "\n",
    "    # Compute the PageRank scores to rank the tokens\n",
    "    scores = nx.pagerank(G)\n",
    "\n",
    "    # Modify scores based on n-gram type\n",
    "    for token in scores:\n",
    "        ngram_type = get_ngram_type(token)\n",
    "        if ngram_type == 1:  # Unigram\n",
    "            pass  # No change to score\n",
    "        elif ngram_type == 2:  # Bigram\n",
    "            scores[token] *= 2  # Double the score\n",
    "        elif ngram_type == 3:  # Trigram\n",
    "            scores[token] *= 2  # Double the score\n",
    "\n",
    "    # Modify scores if token is in title letter\n",
    "    for token in scores:\n",
    "        if any(token in title for title in judul):\n",
    "            scores[token] *= 2\n",
    "\n",
    "    # Extract top N keyphrases along with their scores\n",
    "    ranked_tokens = sorted(((scores[token], token) for token in tokens if token in scores), reverse=True)\n",
    "    \n",
    "    keyphrases_with_scores = []\n",
    "    seen_tokens = set()  # Set to keep track of tokens that have already been added\n",
    "\n",
    "    for score, token in ranked_tokens:\n",
    "        if token not in seen_tokens:\n",
    "            keyphrases_with_scores.append((token, score))\n",
    "            seen_tokens.add(token)  # Mark the token as seen\n",
    "            if len(keyphrases_with_scores) >= n:\n",
    "                break  # Stop when the desired number of keyphrases is reached\n",
    "\n",
    "    return keyphrases_with_scores, G, labels\n",
    "\n",
    "\n",
    "def visualize_graph(G, labels):\n",
    "\n",
    "    # Remove self-loops (edges that connect a node to itself)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos=pos, with_labels=False, font_weight=\"bold\")\n",
    "    nx.draw_networkx_labels(G, pos, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = os.path.join(repo_root, \"models/w2v_200/idwiki_word2vec_200_new_lower.model\")\n",
    "\n",
    "w2v_model = Word2Vec.load(w2v_path)\n",
    "\n",
    "# Get available tokens from the Word2Vec model\n",
    "available_tokens = set(w2v_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 0...! "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#text = df_tr[i] # setelah di preprocess\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ls_judul \u001b[39m=\u001b[39m preprocess(df[\u001b[39m\"\u001b[39m\u001b[39mjudul\u001b[39m\u001b[39m\"\u001b[39m][i])\u001b[39m.\u001b[39msplit()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m keyphrases,_,_ \u001b[39m=\u001b[39m extract_keyphrases_with_ngrams_graph(text, w2v_model, ls_judul, available_tokens, \u001b[39m3\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df_keyphrases \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(keyphrases, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mKeyword\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m a \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(df_keyphrases\u001b[39m.\u001b[39mKeyword)\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m filtered_tokens \u001b[39m=\u001b[39m filter_tokens_by_pos(flat_pos_tokens, selected_pos)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39m# Get embeddings for each token (averaging word embeddings for bigrams/trigrams)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m token_embeddings \u001b[39m=\u001b[39m [get_weighted_phrase_embedding(token, w2v_model, text) \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m filtered_tokens]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m# Filter out tokens that don't have embeddings\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m tokens, embeddings \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[(token, emb) \u001b[39mfor\u001b[39;00m token, emb \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(filtered_tokens, token_embeddings) \u001b[39mif\u001b[39;00m emb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n",
      "\u001b[1;32m/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m filtered_tokens \u001b[39m=\u001b[39m filter_tokens_by_pos(flat_pos_tokens, selected_pos)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39m# Get embeddings for each token (averaging word embeddings for bigrams/trigrams)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m token_embeddings \u001b[39m=\u001b[39m [get_weighted_phrase_embedding(token, w2v_model, text) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m filtered_tokens]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m# Filter out tokens that don't have embeddings\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m tokens, embeddings \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[(token, emb) \u001b[39mfor\u001b[39;00m token, emb \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(filtered_tokens, token_embeddings) \u001b[39mif\u001b[39;00m emb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n",
      "\u001b[1;32m/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Generate TF-IDF dictionary\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(text_corpus)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m tfidf_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(vectorizer\u001b[39m.\u001b[39mget_feature_names_out(), vectorizer\u001b[39m.\u001b[39midf_))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jim/Documents/GitHub/kw_ina_extraction/experiment/6_tr_tfidfscore.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Get phrase embedding\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kw_ina/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2128\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2129\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2130\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2131\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2132\u001b[0m )\n\u001b[0;32m-> 2133\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2135\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kw_ina/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1365\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m \u001b[39m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m \u001b[39m# TfidfVectorizer.\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1366\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m     )\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_ngram_range()\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "predict_textrank = pd.DataFrame()\n",
    "for i in df.index:\n",
    "    print('Processing index', i, end='...! ')\n",
    "    text = df[\"text\"][i] # sblm di preprocess\n",
    "    #text = df_tr[i] # setelah di preprocess\n",
    "    ls_judul = preprocess(df[\"judul\"][i]).split()\n",
    "    keyphrases,_,_ = extract_keyphrases_with_ngrams_graph(text, w2v_model, ls_judul, available_tokens, 3)\n",
    "    df_keyphrases = pd.DataFrame(keyphrases, columns=['Keyword', 'Score'])\n",
    "    a = pd.DataFrame(df_keyphrases.Keyword).T.reset_index(drop=True)\n",
    "    b = pd.DataFrame(df_keyphrases.Score).round(3).T.reset_index(drop=True)\n",
    "    df_keyphrases = pd.concat([a, b], axis=1)\n",
    "\n",
    "    # Check if there are missing columns and add them with zero values\n",
    "    missing_columns = 6 - df_keyphrases.shape[1]\n",
    "    for _ in range(missing_columns):\n",
    "        df_keyphrases[df_keyphrases.shape[1]] = 0\n",
    "\n",
    "    df_keyphrases.columns = ['key_1', 'key_2','key_3','score_1', 'score_2','score_3']\n",
    "    predict_textrank = pd.concat([predict_textrank, df_keyphrases], ignore_index=True)\n",
    "    print('Done')\n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eval\n",
    "\n",
    "targets = df[[\"k1\", \"k2\", \"k3\",\"k4\", \"k5\", \"k6\",\"k7\"]].values.tolist()\n",
    "df_targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partial_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key_1       key_2     key_3  flex_recall  flex_prec\n",
       "0       no_match    no_match  no_match        0.000      0.000\n",
       "1  partial_match  full_match  no_match        0.286      0.667\n",
       "2  partial_match    no_match  no_match        0.143      0.333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation TextRank\n",
    "predict_textrank_list = predict_textrank[['key_1','key_2','key_3']].values.tolist()\n",
    "eval_textrank = eval(predict_textrank_list, targets, True).round(3)\n",
    "eval_textrank.columns = ['key_1', 'key_2','key_3','strict_recall', 'strict_prec', 'flex_recall','flex_prec']\n",
    "eval_textrank = eval_textrank[['key_1', 'key_2','key_3', 'flex_recall','flex_prec']] # untuk menyederhanakan hasil evaluasi\n",
    "eval_textrank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textrank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textrank\n",
       "recall        0.101\n",
       "precision     0.235\n",
       "F1            0.141"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TextRank Score, using flexible score : exact maatch =1, partial match = 1, no match = 0\n",
    "textrank_recall = eval_textrank['flex_recall'].mean()\n",
    "textrank_prec = eval_textrank['flex_prec'].mean()\n",
    "textrank_f1 = 2 * (textrank_prec * textrank_recall) / (textrank_prec + textrank_recall)\n",
    "\n",
    "# Create a DataFrame with the scores\n",
    "summary = pd.DataFrame({'textrank': [textrank_recall, textrank_prec, textrank_f1]}, index=['recall', 'precision', 'F1'])\n",
    "summary = summary.round(3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>flex_recall</th>\n",
       "      <th>flex_prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usulan personil penting</td>\n",
       "      <td>personil penting</td>\n",
       "      <td>kandidat personil penting</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>persetujuan tertulis</td>\n",
       "      <td>prosedur</td>\n",
       "      <td>usulan</td>\n",
       "      <td>pengganti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>template document</td>\n",
       "      <td>project</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.154</td>\n",
       "      <td>template document</td>\n",
       "      <td>exhibit c</td>\n",
       "      <td>acuan</td>\n",
       "      <td>pengelolaan</td>\n",
       "      <td>dokumen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>full_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ruang</td>\n",
       "      <td>usulan</td>\n",
       "      <td>ruangan</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.039</td>\n",
       "      <td>ruang kantor</td>\n",
       "      <td>change inquiry</td>\n",
       "      <td>lingkup kerja</td>\n",
       "      <td>akomodasi</td>\n",
       "      <td>services for company</td>\n",
       "      <td>exhibit a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>partial_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>no_match</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     key_1              key_2                      key_3  \\\n",
       "0  usulan personil penting   personil penting  kandidat personil penting   \n",
       "1                 document  template document                    project   \n",
       "2                    ruang             usulan                    ruangan   \n",
       "\n",
       "   score_1  score_2  score_3                     0               1  \\\n",
       "0    0.049    0.045    0.045  persetujuan tertulis        prosedur   \n",
       "1    0.174    0.174    0.154     template document       exhibit c   \n",
       "2    0.047    0.047    0.039          ruang kantor  change inquiry   \n",
       "\n",
       "               2            3                     4          5    6  \\\n",
       "0         usulan    pengganti                   NaN        NaN  NaN   \n",
       "1          acuan  pengelolaan               dokumen        NaN  NaN   \n",
       "2  lingkup kerja    akomodasi  services for company  exhibit a  NaN   \n",
       "\n",
       "           key_1       key_2     key_3  flex_recall  flex_prec  \n",
       "0       no_match    no_match  no_match        0.000      0.000  \n",
       "1  partial_match  full_match  no_match        0.286      0.667  \n",
       "2  partial_match    no_match  no_match        0.143      0.333  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine dataframe predict_textrank, df_targets and eval_textrank\n",
    "predict_textrank = pd.concat([predict_textrank, df_targets, eval_textrank], axis=1)\n",
    "predict_textrank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jim/Documents/GitHub/kw_ina_extraction/utils/ia_file_operation.py:16: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n",
      "/Users/jim/Documents/GitHub/kw_ina_extraction/utils/ia_file_operation.py:25: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "# Write predictions to excel file\n",
    "from utils import write_excel\n",
    "\n",
    "sheet_name = '5_tr_weightscorephrases'\n",
    "output_file = '5_tr_weightscorephrases.xlsx'\n",
    "write_excel(predict_textrank, sheet_name, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_ina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
